[
{"module": ["干货教程"], "note": ["背景\n作为一个开放领域的知识社交平台，知乎为大家提供了「友善」、「理性」、「专业」的讨论氛围，吸引了大量用户参与，产生了很多优质内容。但同时也吸引了一些垃圾制造者，在知乎上生产了..."], "title": ["知乎技术日志：基于 AC 自动机和贝叶斯方法的垃圾内容识别"], "content": ["作为一个开放领域的知识社交平台，知乎为大家提供了「友善」、「理性」、「专业」的讨论氛围，吸引了大量用户参与，产生了很多优质内容。但同时也吸引了一些垃圾制造者，在知乎上生产了不少的垃圾内容，如「违法」、「广告」、「淫秽色情」、「人身攻击」等，严重影响了知乎用户的正常讨论交流，极大地影响了用户体验，同时也对社区管理造成了较大的干扰。", "我们先来看看都有哪些真实的垃圾：", "，比如：1）传播、寻求或买卖情色，如「xxx 宾馆酒店小姐」、「xxx 一夜情小姐」、「xxx 全套小姐」、「xxx 红灯区小姐」；2）传播赌博，如 「澳*新*天地 xxx xxx 博人真钱赌 连赢 100 万」、「*博 xxx xx 玩法多」、「xxx xx zzz」等等。这类内容违反了现行法律，危及了国家安全和社会和谐。", "，比如：1）辱骂他人，如「你这个智障」、「艹你妈」；2）用侮辱、夸张的手法嘲讽他人，如「脑残」、「智商欠费」等等。这类内容表现为不尊重他人，用恶毒的言语刺激对方，使得讨论无法正常有效进行。", "还有一些垃圾广告，如微商、代开发票。", "这些垃圾内容严重影响了知乎用户的正常交流。此前我们的工程师们也尝试了一些方法去识别处理它们。如文本分类模型，准确率达到了 96%, 每天识别 300+ 条；利用 DFA 根据关键词大量召回。这些尝试虽然都取得了一定的效果，但是召回不够、或召回过多非垃圾内容、或者存在不少的误伤。为此我们引入人工审核，但不能快速处理，容易造成内容堆积，而且对管理员也是很大压力，平均每周要消耗 1 个人力。", "前期的尝试虽然效果不是很理想，但积累了比较多的数据。对这些数据的分析，我们发现这些垃圾内容是有套路的。基于此，我们利用 Aho-Corasick 自动机实现多模匹配，在其基础上增加了过滤机制，实现了第一版的垃圾内容分析系统，取得了不错的效果。", "AC 自动机算法于 1975 年产生于贝尔实验室。该算法巧妙地将多模式串建成一个确定性有限状态机 (DFA)，以待匹配字符串作为该 DFA 的输入，使状态机进行状态转移，当到达某些特定的状态时，完成模式匹配， 能 ", " 时间内完成多模式匹配（其中 n 为待匹配字符串的长度）。下面以模式串「 he / she / his / hers 」构建一个 AC 自动机举例说明（如图一所示）", "当输入一个字符串时「ushers」，该自动机从状态 0 开始进行状态转换，完整的状态转移路径如图二所示", "当遇到 AC 中的红色节点时，说明发生了模式匹配，匹配到的模式有：「he」、「she」、「hers」。", "具体可以用 Double Array Trie 实现 AC 自动机，在保持高效多模匹配的基础，进一步节省空间。", "虽然 AC 自动机能快速的从字符串中找到存在于词典中的关键词，但这仅仅能满足一小部分需求，即不顾准确率的大量召回，很显然会造成误伤，这对知友也是很不友好。肯定有知乎用户会问，直接用「贝叶斯」方法不就可以搞定吗？你看人家做 spam 邮件过滤，不也做得还不错，还用什么 AC ？对对对，你说的都是正确的。但是实验发现，单纯地用 「贝叶斯」方法直接进行过滤时，准确率和召回率都不是很理想。究其原因呀，有 1）知友们知识面广、思维发散，2）长尾，很多词语出现频次相对较低。", "考虑到上述问题，我们提出了利用 AC 自动机，根据设定的类别关键词圈定相应类别的内容，然后在每个类别里利用「贝叶斯」方法的思想准确过滤出垃圾内容。现在我们有了解决问题的思想（思想很重要），来看看我们具体是怎么利用 AC 和「贝叶斯」这两个神器，打造垃圾内容过滤的。直接上图，一图胜千言。", "图三中「主关键词」就是利用 AC 自动机按关键词圈定相应类别的内容。圈定之后，利用「可有可无」这儿所配置的策略在每个类别里进行垃圾内容过滤，策略即是利用「贝叶斯」思想总结出来的。", "下面以评论数据为例，介绍如何运用贝叶斯方法来总结策略。", "首先，分析样本数据，提取每一个词，计算每个词在正常评论和垃圾评论中出现的频率。比如，我们假定「sb」这个词，在 1000 条垃圾评论中，有 500 条包含该词，那么它的出现频率就是 ", "；而在 1000 条正常评论中，只有 2 条包含该词，那么出现频率就是 ", "。那现在对一条新的评论，发现其中包含「sb」这个词，它是垃圾评论的概率可以通过式一计算。（此处用 ", " 和 ", " 分别表示垃圾评论和正常评论, ", " 表示词 「sb」，", " 表示垃圾评论的概率，", "表示垃圾评论中 ", " 出现的频率）", "\n(式一）", "在没有更多先验知识的情况下，我们通常假设 ", "。那在前文的例子中，很容易计算出 ", "，说明「sb」词很容易区分出垃圾评论。通过这样的方式去挖掘出词语，当然也可以从正面角度考虑，比如「我」这个词，在我们的数据中能较好地区分出不是垃圾评论。此外，还可以考虑多个词语联合共现，甚至词之间的空间结构关系。这些在目前的逻辑里都是支持的。", "有了具体实现，我们来看看实际的效果，如图四所示。（对于不利于讨论的内容也会被处理）", " ", "线上效果如图五所示。", "这套逻辑已融入到了算法机器人「瓦力」的大脑中，在知乎的诸多场景下，如评论、私信、回答、提问等，以 99% 的准确率处理着每天产生的垃圾内容。", "，上线后处理了", "、上千条代为完成个人任务、上千条求医问药等违规提问，帮助知友们维护起了一个「友善」的讨论环境。", "此外，这套系统也十分方便运营人员实现一站式自助策略管理。首先通过样本制定策略，然后通过离线版本进行策略验证，评估其准确率和召回率，最后自助上线策略。整个过程均无需工程师的介入，大大提高了运营效率。", "为了让知乎用户友善地讨论交流，我们踏出了这一小步，主动识别处理了诸多垃圾内容。但还有很长的路要走，后续我们将为机器人「瓦力」打造更加完善智能的大脑，如自动归纳策略，引入深度学习等，为其建立更加科学高效的识别能力，以全自动的方式准确地识别出所有内容。", "虽以识别垃圾内容为出发点，建立了基于 AC 自动机和「贝叶斯」思想的内容识别系统，但这套系统还可以运用到其他场景，比如舆情、其他内容归类等，在这儿就不展开了。", "作者：孙先，张瑞，王璐", "Reference:", "[1] ", "[2] ", "[3] ", "[4] ", "[5] ", "via:「知乎技术日志」", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78546"]},
{"module": ["干货教程"], "note": ["广告和推荐系统是机器学习是最成熟的应用领域。那么广告和推荐系统是怎么在线上部署机器学习模型的呢？\n\n1.预测函数上线\n刚刚学习机器学习时候，我认为广告和推荐系统过程如下图所示：\n1）线..."], "title": ["广告和推荐系统部署机器学习模型的两种架构"], "content": ["广告和推荐系统是机器学习是最成熟的应用领域。那么广告和推荐系统是怎么在线上部署机器学习模型的呢？", "刚刚学习机器学习时候，我认为广告和推荐系统过程如下图所示：", "1）线下部分，从用户和广告（物品）属性抽取用户和物品特征，将抽取的特征合并进日志生成训练数据，训练机器学习模型；", "2）线上部分，来了一个请求，从用户和广告（物品）属性抽取请求中的用户和物品的特征，将这些特征合并请求生成预测实例，用线上模型得到预测结果。", "但是这个架构有两个问题：", "1）从用户和广告（物品）属性抽取特征的程序有线上线下两套，这两套程序必须保持完全一致。但由于调参的原因，特征抽取是机器学习系统中最经常发生变化的模块。经常变化的模块需要保持一致，这很困难。那么我们能不能强行地用一套程序呢？比如，我们把特征抽取和特征处理模块写成 .so 文件。这样也有问题：线下要求快速变化以方便工程师调特征，可能会使用一些训练框架（比如 Spark）；线上要求程序快速实时，要求工程师编码严谨。写成严谨的 .so 文件，能够保证线上的需求，但无法快速变化，也不能在 Spark 上使用。", "2）线上特征抽取要求非常快速，特别在线上吞吐量很大的情况。但有些重度特征不可能在短时间内抽取出来，比如广告的历史点击率（生成这个特征需要遍历一段时间的点击日志）。", "在读书期间，这两个问题困扰了我很久, 直到 2014 年我知道了神器 Redis。Redis 是一个开源内存数据库，支持集群模式、持久化和 Key-Value 数据结构。在使用时，我们可以将 Redis 看成一个巨大的哈希表。Redis 在后台开发中经常用作 cache 服务器, 后来被工程师们用于广告和推荐系统中的特征服务器。工程师将用户和广告（物品）的 ID 作为 Key，将用户和广告（物品）的特征作为 Value 存入 Redis，这样线上程序只需要用户和广告（物品）的 ID 就能知道特征。引入 Redis 之后，广告和推荐系统过程如下所示：", "1）线下部分，从用户和广告（物品）属性抽取用户和广告（物品）特征，把抽取的特征合并进日志生成训练数据用于训练机，并把抽取的特征上载到线上 Redis 服务器；", "2）线上部分，来了一个请求，从 Redis 服务器取出用户和广告（物品）特征，将特征合并进请求生成预测实例，用线上模型得到预测结果。", "这种架构还有一个变种：在线下抽取特征之后不生成训练数据而是直接送到 Redis，在线上用 Storm 实时拼接训练数据。但我对这个变种的前因后果不太了解，就不展开讨论了。这种架构将预测函数（也就是训练出来的模型）部署在线上。为了和下面的架构区分开来，我们将这种架构称为预测函数上线架构。", "了解预测函数上线架构之后，我将之作为广告和推荐系统线上部署模型的 “正统”。 因此当 2014 年我接触到另一种架构时，我内心是拒绝的。这种架构的要点在于把预测结果上线，具体过程如下所示：", "1）在线上，从用户和广告（物品）属性抽取用户和物品特征，将抽取的特征合并进日志生成训练数据，训练机器学习模型；将几乎所有可能的请求合并特征，进而生成预测实例，用模型得到预测结果；", "2）线上就很简单了，接入线下传过来的预测结果。这里稍微难理解的是 “穷尽几乎所有可能的请求”，疑惑那么多可能的请求怎么可能穷尽呢？微博广告系统（虚构的）所有可能的请求貌似很多，但每个用户只需要匹配若干个广告就行了。因此微博广告系统的预测结果 “userid,adid1,adid2…,adidn” 上载到线上，一旦线上传一个 userid 请求展示广告，线上模块就按照一定的逻辑返回预测结果中这个用户对应的广告。这种架构是将预测结果部署到线上，我们将之称为预测结果上线架构。", "慢慢地我也开始明白预测结果上线的好处了。预测结果上线架构将机器学习全过程和绝大部分控制逻辑都搬到线下，规避了线上的各种隐患。这样不那么厉害的工程师用不那么厉害的机器也能搞定线上模块了，毕竟线上模块只需要实现少量的控制逻辑和展示。这大大降低了建立一个广告系统或者推荐系统的难度。", "我正式工作之后，组里支持运营活动的推荐系统采用了预测结果上线的架构。我发现有不少时间浪费在重跑数据上，原因在于有时需要临时增加或者删除物品。一旦增加或者删除物品，预测结果上线的推荐系统就需要重新生成预测数据（因此之前跑的数据要么没有要加的物品，要么有要删的数据）。另外一个问题就是预测结果上线架构有延时性：今天线上展示的是昨天准备的预测结果，今天准备的预测结果要等明天才能展示，这会导致节奏慢一些。最后还有一个问题，预测结果上线架构只适用于几乎所有可能的请求能够穷尽的场景。比如，预测结果上线架构不适用于搜索广告系统，因为搜索广告系统不能穷尽所有可能的请求。", "预测函数上线架构能够覆盖预测结果上线架构的适用场景，但是预测结果上线架构不能够覆盖预测函数上线架构的适用场景。同时预测函数上线架构更具灵活性。预测函数上线架构不愧为部署机器学习模型的 “堂堂正正” 之法。", "预测结果上线架构的好处就是难度比较低。预测结果上线架构将机器学习全过程和绝大部分控制逻辑，规避了线上的各种隐患。在机器、时间和人力等各种条件不充足的情况，预测结果上线架构不失为一个好的选择。预测结果上线架构是 “剑走偏锋” 的机器学习模型部署之法。兵法有云：以正合以奇胜，选择哪一种架构还是需要仔细的分析和权衡。", "via:AlgorithmDog", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78539"]},
{"module": ["干货教程"], "note": ["\n这几年来，机器学习和数据挖掘非常火热，它们逐渐为世界带来实际价值。与此同时，越来越多的机器学习算法从学术界走向工业界，而在这个过程中会有很多困难。数据不平衡问题虽然不是最难的，..."], "title": ["如何解决机器学习中数据不平衡问题"], "content": ["这几年来，机器学习和数据挖掘非常火热，它们逐渐为世界带来实际价值。与此同时，越来越多的机器学习算法从学术界走向工业界，而在这个过程中会有很多困难。数据不平衡问题虽然不是最难的，但绝对是最重要的问题之一。", "在学术研究与教学中，很多算法都有一个基本假设，那就是数据分布是均匀的。当我们把这些算法直接应用于实际数据时，大多数情况下都无法取得理想的结果。因为实际数据往往分布得很不均匀，都会存在“长尾现象”，也就是所谓的“二八原理”。下图是新浪微博交互分布情况：", " ", "可以看到大部分微博的总互动数（被转发、评论与点赞数量）在0-5之间，交互数多的微博（多于100）非常之少。如果我们去预测一条微博交互数所在档位，预测器只需要把所有微博预测为第一档（0-5）就能获得非常高的准确率，而这样的预测器没有任何价值。那如何来解决机器学习中数据不平衡问题呢？这便是这篇文章要讨论的主要内容。", "严格地讲，任何数据集上都有数据不平衡现象，这往往由问题本身决定的，但我们只关注那些分布差别比较悬殊的；另外，虽然很多数据集都包含多个类别，但这里着重考虑二分类，因为解决了二分类中的数据不平衡问题后，推而广之就能得到多分类情况下的解决方案。综上，这篇文章主要讨论如何解决二分类中正负样本差两个及以上数量级情况下的数据不平衡问题。", "不平衡程度相同（即正负样本比例类似）的两个问题，解决的难易程度也可能不同，因为问题难易程度还取决于我们所拥有数据有多大。比如在预测微博互动数的问题中，虽然数据不平衡，但每个档位的数据量都很大——最少的类别也有几万个样本，这样的问题通常比较容易解决；而在癌症诊断的场景中，因为患癌症的人本来就很少，所以数据不但不平衡，样本数还非常少，这样的问题就非常棘手。综上，可以把问题根据难度从小到大排个序：大数据+分布均衡<大数据+分布不均衡<小数据+数据均衡<小数据+数据不均衡。对于需要解决的问题，拿到数据后，首先统计可用训练数据有多大，然后再观察数据分布情况。经验表明，训练数据中每个类别有5000个以上样本，数据量是足够的，正负样本差一个数量级以内是可以接受的，不太需要考虑数据不平衡问题（完全是经验，没有理论依据，仅供参考）。", "解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类”。", "采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。", "采样分为上采样（Oversampling）和下采样（Undersampling），上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。", "随机采样最大的优点是简单，但缺点也很明显。上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；而下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。", "上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。", "因为下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大，感兴趣的可以参考“Learning from Imbalanced Data”这篇综述的3.2.1节。", "数据合成方法是利用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例，比如医学图像分析等。", "其中最常见的一种方法叫做SMOTE，它利用小众样本在特征空间的相似性来生成新样本。对于小众样本", "从它属于小众类的K近邻中随机选取一个样本点", "生成一个新的小众样本", "其中", "是随机数。", "上图是SMOTE方法在", "近邻下的示意图，黑色方格是生成的新样本。", "SMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。为了解决这个问题，出现两种方法：Borderline-SMOTE与ADASYN。", "Borderline-SMOTE的解决思路是寻找那些应该为之合成新样本的小众样本。即为每个小众样本计算K近邻，只为那些K近邻中有一半以上大众样本的小众样本生成新样本。直观地讲，只为那些周围大部分是大众样本的小众样本生成新样本，因为这些样本往往是边界样本。确定了为哪些小众样本生成新样本后再利用SMOTE生成新样本。", "ADASYN的解决思路是根据数据分布情况为不同小众样本生成不同数量的新样本。首先根据最终的平衡程度设定总共需要生成的新小众样本数量", "确定个数后再利用SMOTE生成新样本。", "除了采样和生成新数据等方法，我们还可以通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同，如下图：", "横向是真实分类情况，纵向是预测分类情况，C(i,j)是把真实类别为j的样本预测为i时的损失，我们需要根据实际情况来设定它的值。", "这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。", "对于正负样本极不平衡的场景，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。", "解决数据不平衡问题的方法有很多，上面只是一些最常用的方法，而最常用的方法也有这么多种，如何根据实际问题选择合适的方法呢？接下来谈谈一些我的经验。", "在正负样本都非常之少的情况下，应该采用数据合成的方式；在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。", "采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许上采样往往要比加权好一些。", "另外，虽然上采样和下采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，我的经验是如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。", "更多细节与更多方法可以参考TKDE上的这篇综述：“Learning from Imbalanced Data”。", "作者：无影随想", "\n时间：2016年1月。", "\n", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78557"]},
{"module": ["干货教程"], "note": ["\n很多行业为什么能赚钱？因为它促进了资源之间的流动效率。\n促进物品之间的流动效率，叫物流业；促进信息之间的流动效率，叫互联网行业；促进金钱之间的流动效率，叫金融业。\n今天这里要谈的..."], "title": ["闷声发大财，关于支付行业的秘密"], "content": ["很多行业为什么能赚钱？因为它促进了资源之间的", "。", "促进物品之间的流动效率，叫", "；促进信息之间的流动效率，叫", "；促进金钱之间的流动效率，叫", "。", "今天这里要谈的是金融业的一个分支，", "这个行业怎么赚走你的钱？", "你的每一次", "，除了现金支付，都会产生相应的手续费，落到一家公司的口袋，这家公司叫", "把银联拆开看，可以看做", "，于2002年3月份成立于上海。", "为什么成立这样一个联盟呢？废话当然是赚钱！但赚钱之外还有什么用呢？", "2002年以前，各家银行都是自吸门前血，莫管他人瓦上霜，你把钱存工行，只能去工行的ATM上取，你拿着建行信用卡，只能去建行的POS机上刷，想跨行？没门，银行间的系统都没打通。", "于是，在央行的怂恿下，", "，将各家银行的系统和数据打通，从此往后，你拿着工行的卡去建行的ATM取钱，再也不会被人骂SB啦，撒花！", "所以，银联起的是", "的作用，将", "，此路是我开，此树是我栽，收点过路费很正常！", "银行辣么多，银联只有一个，慢慢地，银联成了一个", "，暴发户身边除了小蜜还有什么？小弟！银联养活了一群小弟，这些小弟是", "想象一下，银行和银联都是官老爷，高高在上，他们才不会主动去谈生意，如果顾客去买东西都用现金支付，那么银行和银联是赚不到钱的，所以必须让顾客", "并不是每一家商户都愿意装POS机，因为装机需要付钱！安装也要付钱！装了以后顾客付钱还要收我手续费！我又不是傻，很贵的好么。", "于是银行和银联找到 ", "央行凝神一想，想出了 “", " 这个东西，于是大手一挥，找来一堆点头哈腰的民营企业，跟他们说，弟兄们，一起赚钱的时候到了，你们花点钱贿赂一下我，哦不，申请一下这个 ", "，我就准你们上门给商家推广POS机。", "所以这里的 ", " 指的是，持有 “", " 的具有很强线下拓展能力的", "，比如拉", "等公司。", "比如你去餐厅吃饭，用工行卡在卡友的POS机上刷卡支付", "，餐厅需要支付", "的手续费，也就是", "，这1.25元按照", "的比例分配给", "。", "正当银联和小蜜、小弟一起酒池肉林的时候，突然遭到两记重拳，一下被打倒在地，这两记重拳来自", "2003年前，我们在马云的淘宝买东西，采取的支付方式大部分是线下的", "，也就是一手交钱一手交货，货款两清，但这样子一是不方便，二是不安全，于是马云跑去找银联谈，说大哥咱们合作吧，我有用户你有技术，珠联璧合！可惜郎有情妾无意，银联对马云爱搭不理。", "马云怒了，我可是未来的中国首富，你敢看不起我？于是绕过银联，直接跟各家银行谈，没想到给他谈成了，各家银行纷纷给马云开通 “", "” 的接口。", "2003年10月，淘宝推出 ", "，从此在淘宝上的跨行支付变成这样", "也就是你去淘宝买一件100元的商品，是通过支付宝绑定工行卡支付的，这100元实际是付给支付宝的", "，然后支付宝再从", "中转给商家的建行账户100元。", "可以看到，中间没有", "参与，这是银联挨的 ", "，这记重拳让银联掉了一颗牙齿，这颗牙齿叫 ", "。", "银联掉的第二颗牙齿叫 ", "。", "2013年8月，微信推出 “", "” 功能，依靠庞大用户量，迅速把扫二维码这个动作，变成生活中习以为常的一个场景。", "二维码其实是一种信息的转码，把微信的用户信息变成二维码，扫描即互相关注，而把商户的银行账户信息变成二维码，扫描即", "。", "于是，二维码的应用，迅速衍伸到了支付领域。", "2014年3月，微信开放 “", "” 功能，扫码支付正式成了替代", " 的一种方式。", "让我们来看一下", " 和", " 分别产生的手续费：", "刷卡支付，商户需要缴纳", "，也就是", " 的手续费 （不同行业，手续费比例不一样）。", "而扫码支付，商户仅支付", "，也就是", " 的手续费，足足省了一半。", "不仅如此，微信和支付宝还砸钱补贴商户还有顾客，2014年12月12日，支付宝正式向银联宣战，通过每单最高20元的补贴，高调进军线下支付业务。", "，二维码迅速攻占了洼地，面对二维码支付的咄咄逼人，银联选择的是", "2013年，银联的NFC手机支付产品正式投入商用。", "NFC支付是什么？是使用无线通讯技术，使手机和POS机或自动售货机等设备在没有互联网的条件下进行连接，从而实现支付的一种手段。", "银联为什么信任NFC？因为ApplePay在国外已经证实了其可行性，而且NFC可以放在SIM卡里面实现，更符合运", "。", "但NFC还是被二维码支付打的满地找牙，因为", "截止2016年底，使用", "，是", "%，", "第三方调查机构易观的数据显示，从", "看，2016年第二季度", "，在支付宝和腾讯的财付通之后", "银联在国内的垄断一去不复返，而", "，银联也遭到支付宝的蚕食，支付宝目前已实现", "的覆盖，并与法国巴黎银行、英国巴克莱银行、意大利联合信贷等展开合作，而2016-2017年，支付宝更是动作频频", "年收入 130亿 人民币，净利润 20亿 人民币的 银联，在年收入超过 1000亿 人民币，净利润超过 700亿 人民币的 阿里巴巴 面前，上演了一场 郭敬明打姚明", "看到这里，你可能会拍手叫好，毕竟", "而支付宝的母公司 ", "，也计划在2017年上市，估值 ", "，很好，Happy ending.", "但你如果经常看欧美电影，你就知道电影不是这么演的。", " 和 ", " 合并之后，一家独大带来的是打车费用的坐地涨价；", "在撤离中国后，留下的是 百度 的竞价排名和莆田系。", "且看中国支付行业的未来。", "via：挖数", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78891"]},
{"module": ["干货教程"], "note": ["本文来源于斗鱼数据平台部吴瑞诚先生在光谷猫友会的分享。\n我是吴瑞诚，现在负责斗鱼数据平台部，今天给大家分享一下斗鱼大数据这块的玩法。我先做个自我介绍，我是11年初华科通信硕士毕业就..."], "title": ["斗鱼大数据的玩法"], "content": ["本文来源于斗鱼数据平台部吴瑞诚先生在光谷猫友会的分享。", "我是吴瑞诚，现在负责斗鱼数据平台部，今天给大家分享一下斗鱼大数据这块的玩法。我先做个自我介绍，我是11年初华科通信硕士毕业就进入淘宝，主要做HBase相关开发，后来回武汉后在1号店转向应用架构方向。", "我是14年9月加入斗鱼，当时斗鱼研发是30人的规模，从0开始搭建斗鱼大数据平台，单枪匹马一个人，大概干了三个月，招不到大数据开发，哪怕只是基本了解Hadoop的都很招不到，干的很苦。所以团队第一批组员是组内培养的，会一些Java开发基础的应届生，生拉硬拽凑到大数据团队。其中一位武汉理工15年的本科应届生，现在已经成长为我们部门BI/DW团队的Leader。到现在，加上即将入职的应届生，数据平台部团队规模接近60人，分为四个组：大数据处理组、BI/DW组、数据挖掘组、基础架构组。", "在武汉招开发难，招大数据开发更难，这也是我、柴楹和猫友会发起这个收费群的主要目的。让整个武汉互联网圈看着起势的时候，会有一个大数据的圈子。大家多沟通，多交流，把整个圈子的气氛带动起来。这样对于我们研发人员本身和武汉本地的公司都是极大的利好。", "先简单介绍一下斗鱼：1.国内最大游戏直播平台，从16年开始向泛娱乐化方向发展；2.日活用户 2000万，最高同时在线人数约500万；3.主播日活 40,000人，每天产生原创视频数万小时。；4.ALEXA排名：全球约200名、国内约20名（近期略下滑，今天中午看在30多名，比优酷、Bilibili等站要高）；", "这是一个典型的斗鱼直播间，是斗鱼最主要的内容形态：左边是视频区，上面飘过的文字就是弹（dàn）幕，直播网站上用户和主播互动的最主要形式。弹幕服务是性能压力最大的服务之一，相当于是需要百万人群聊时的消息推送量，后续有机会和大家分享；视频区下面是礼物赠送区；右侧是弹幕区，Tab页上是排行榜，用户对该直播间的贡献值，土豪喜闻乐见；", "接下来我会跟大家分享一下斗鱼大数据现在的玩法和下一步的规划。主要内容分为四方面：", "先来看看斗鱼大数据平台的整体架构，上一张镇楼的图：", "14年加入斗鱼，当时负责的第一个业务是用shell脚本，从主站服务器上拉Nginx log到Hive集群中，统计产生报表。随着网站数据量的增长和配备约来越完备，从最开始的小系统越做越大，做到现在这个架构。", "这个架构差不多是在16年初的时候成型的，主要包括：数据源层，用户行为打点、服务日志、Nginx/PHP日志（离线和实时两条数据流）、线上MySQL库、MongoDB、Redis存储数据接入层，包括了Kafka，负责接入打点上报（最大吞吐量约200w TPS）、Canal/Sqoop（主要解决数据库数据接入问题）、Flume/Logstash/rsyslog等实时日志；数据预处理层，简单的清洗和聚合计算；存储层，典型的系统HDFS（文件存储）、HBase（KV存储）、Kafka（消息缓存）；计算层，主要包含了典型的所有计算模型的计算引擎，包含了MR、Hive、Storm、Spark以及深度学习平台比如Tensorflow等等再往上就是数据服务层，主要提供对外数据服务。这一块现在主要是基于Docker实现的一整套微服务环境来支撑；", "下面先简单介绍一下各个组件的玩法，首先是大数据管理平台，承载所有的元数据管理、统一监控系统、报表展示、任务调度、发布系统等所有配套功能。", "可以看到，清晰的区别了online 和offline 集群", "实时离线集群分离，避免资源竞争造成业务处理抖动。集群分离后也可以更安全的对集群进行运维操作；", "使用Kafka作为MQ，更确切说是消息通道；", "Impala之前，我们有使用Presto的经验，后来因为运维上的问题，暂时下线了。Kylin15年很早的时候，在北京和kylingence官方有过一次深入沟通，仔细评估后发现斗鱼的场景不能发挥他的优势。最近Kylin重大版本之后，准备再评估一次。这方面应该有不少同学都有相关的经验，最后讨论阶段，大家可以一起聊一下。", "可能对大数据组件不太熟悉的同学，可能对OLAP概念不太熟悉，主要是针对秒级大数据量查询场景。对应的OLTP 是针对事务处理，我们比较常见的MySQL和Oracle属于这类。这样解释大家可能接受起来会简单一些。", "使用Impala作为SQL查询引擎，相比Hive整体提升5倍速度；使用了完全区别于MapReduce的一套数据处理模式。", "我们主要做了一点：由于Impala是C++编写，为了提升性能，将部分高查询密度的UDF（如：JSON解析）替换为C++实现。使用HA Proxy作为Impala负载均衡器，均衡JDBC连接。避免 Impala Daemon 故障、重启时影响正常业务。这样，可以覆盖我们现在大部分的秒级大数据量查询（10亿级别）", "Spark在斗鱼大数据生态系统有着举足轻重的作用。涉及到：", "部署：Spark on Yarn ，单节点部署，依赖Yarn环境", "发布：基于自研发布系统，可配置一件部署", "调度：基于自研发布系统，定时调度与依赖调度两种模式可选", "告警：监控与告警系统支持任务执行监控及其结果校验监控", "由于正在经历Spark大版本升级（1.6.xà2.1.x）进程之中，斗鱼数据平台部线上存在两个Spark集群来支撑目前的业务", "Spark生态中，我们主要用到的两个组件：", "1.Zeppelin主要提供Spark SQL交互界面查询，数据可视化支持。后续将支持Python、R语言的建模，支持用户Notebook的提交和调度。", "2.Alluxio基于内存的分布式文件系统，它是架构在底层分布式文件系统和上层分布式计算框架之间的一个中间件，主要职责是以文件形式在内存或其它存储设施中提供数据的存取服务。", "底层的存储层和计算层的基础集群大概是这些。", "现在我们把es集群按业务场景划分为多个小集群，这样的可以避免不同业务出现抢占资源的情况。", "多个ES集群，50+物理节点、每日15T+日志量，多实例部署，接入全站所有服务器日志；", "年后，完成全部升级至ElasticSearch 5.X，性能提升明显；", "废弃FlumeAgent，使用Firebeat、Rsyslog，小巧稳定、资源占用低；", "为部分业务独立开发日志解析器，提高性能，有Java版和Spark版；", "我们应用ELK的场景中，最难的就是在agent 资源占用和 agent抽取吞吐之间做权衡，想马儿快，又想马儿不吃草，会针对不同的语言栈，使用不同的agent是实现。", "推荐日志使用JSON格式，降低解析压力、增减字段灵活；", "我们agent会把日志灌入kafka，然后从kafka出口的日志流就需要稳定、格式化的结构往ES集群小batch灌，这样就出现了使用hangout和自研数据管道两种实现方式，hangout是java版的Logstash，效率仍然满足不了我们的要求，所以，有了基于spark 的自研日志消费管道。", "对ELK的使用，算是踩坑无数。讲一些ES主要优化点：", "1.索引按小时切分，当索引无数据写入时进行ForceMerge提升查询性能；", "2.由于日志收集写请求远远大于读请求，保证每个节点有分片分摊写压力（每个节点1~2个分片），节点磁盘独立；", "3.每台服务器（128G内存），1Master+2Data，预留大约一半内存作为System Cache提升查询性能；记住，一定要留足cache 。。", "4.cluster.routing.allocation.same_shard.host禁止主从分片被分到同一台服务器上（不同节点），保证服务器宕机时索引可用；", "ELK的整体优化思路是为了能抗住低延迟和大日志量的问题；现在我们已经能稳定在每日15T+日志量级，基于这一套统一日志监控系统，做了很多业务、服务的监控和告警。", "我们现在吞吐量最大的实时数据流，100w+TPS，全站各个端所有用户行为的实时监控，可以实时看到整体打点水位、各个客户端、各个版本播放器的健康状况", "这是推荐接口的实时Dashboard监控效果图：左上是每次请求的最大耗时、右上50分位和99分位耗时统计；左下是推荐接口的整体水位统计、右下是各个Tomcat实例的水位统计；这样可以对实时推荐接口的整体访问量、每个实例的请求量、性能水位、超时请求一目了然，监控神器。基于此可以做阀值的监控，解放双手。", "现在每个核心接口分给不同同学来负责，人手配置一个类似的Dashboard。加上告警，就可以腾出手来了。", "再分享最前面有一个大的架构图，最上层是数据应用层，我们会对外提供多种服务，包括个性推荐、实时监控、广告系统、风控系统、搜索引擎（基于ES）、后台数据应用等服务都是由我们自己来完成整个服务的部署，用以对外提供访问；这样，除了数据层面的处理，也对我们工程实现上提出了高要求，要能直接提供对外服务。我们现在的玩法是基于Docker生态构建完备的微服务体系来实现，直接上图", "前后端完全分离，使用Nginx作为网关，代理后端服务。功能较单一，正在预研其他网关方案；", "1.使用内嵌的Web容器（Tomcat、Jetty、Undertow），由应用自己掌控整个生命周期，形成闭环；", "2.容器化改造更平滑，提高应用弹性（方便统一底层系统环境、方便扩缩容）；", "3.内置的Metrics 接口使应用监控更方便。", "1.使用K8S作为容器编排引擎，进行容器调度、运行、滚动升级、灰度发布；", "2.暴露服务状态接口（isReady，isHeahthy），更好的利用K8S探针进行自我健康检查；", "3.容器网络使用Flannel（VxLan模式）；", "4.使用Nginx作为网关代理外部请求（非容器网络内的请求）;", "5.进行容器化改造的过程中，对于需要暴露接口的服务，初期可以将服务容器网络设置为宿主机网络，避免接口无法访问的问题（尤其是在服务发现场景，注册的是容器IP，外部无法访问），后期进行宿主机端口感知、注册。", "主要挑了核心组件来做简单分享，抛个砖；当然为了支撑Devops开发模式（每位同学从需求分析开始，一直负责设计、开发、测试、发布、运维、告警、优化等服务全周期），需要有配套的任务调度系统（基于ZK自研）、发布系统（基于Jenkins自研）、监控系统（自研）等系统限于篇幅不做展开；", "斗鱼数据仓库最开始就是一个Hive default库，导入的表是Nginx/PHP log。慢慢需要对注册用户进行统计分析，逐渐导入了注册表、礼物流水、充值表等等关系表。逐渐增加到近200张表粗放的放在default库，碰到权限、误删除等问题后，数据仓库分层刻不容缓；", "斗鱼数据仓库分层主要分为以下几层：ods：ods层主要用来存储从业务数据库表，如MySQL，MongoDB同步过来的原始数据，以及线上用户行为等原始数据external：external层主要用来存放线上写到HBase数据建立的外部表，以及日志数据的外部表dim：dim层主要用来存放维度表信息，如直播间维度信息等dwd：dwd层主要用来存放从ods层以及dim，external处理清洗过后的数据，方便接下来的计算dw：dw层主要用来对用户或主播等进行一些轻维度的汇总ads：ads层主要是应用层的一些数据，如对外的报表数据archive：archive层主要是归档历史数据", "数据仓库中还会根据业务以及数据类型做域的划分：", "业务域：ad-广告，game-游戏数据域： log-日志域，pay-交易域", "斗鱼数据仓库规范实施数据仓库基本的建设完成后，就会有一些用户操作数据的规范，如：SQL规范，数据时间周期的规范SQL规范主要有以下几个方面类型统一：double，string，bigint数据倾斜：group，count distinct，开窗函数表级&字段级注释条件语句类型必须一致case when语句必须要有else表查询带上库名称", "\n", "\n开始同步数据时使用了开源的sqoop作为同步工具，由于对多数据源的支持不够丰富以及对内部系统的兼容性问题，逐渐放弃使用；", "自研了Data-Porter工具作为目前主要的多种数据源与数据仓库间的同步工具。Data-Porter基于Spark Pipeline，将数据从DB读取为Spark RDD，再转为Spark DateFrame，最后使用Spark SQL将数据写入Hive，这种通过API由低阶到高阶的应用，实现了无附加操作的数据同步。", "目前支持MySQL的数据库分库分表的同步，MongoDB集群同步以及数据源去重，但是也有一些不足的地方，比如对数据源目前还只支持了MySQL和MongoDB，后续会规划开发关于HBase等目前主要使用的数据源的同步支持。", "先看看斗鱼个性推荐的主要栏目位：", "– App首页几乎都是个性推荐的栏目位，每个用户看到的都是依据自己口味推荐的直播间，千人千面；", "– 直播列表中有特定的两个位置；", "– 直播间页面中，有“超管推荐”，每个直播间看到的都不一样，百人百面。", "其中App首页是完全千人千面，不同的分区、不同的房间，都是根据用户历史行为，预测的用户偏好进行推荐。可以看下斗鱼个性推荐的场景，很多，而且会越来越多。是公司的总体战略目标。", "斗鱼个性推荐是从15年开始预研，线下做推荐方案对比、模型效果对比、实际数据试跑，到16年6月份才正式上线。从Web端的列表、直播间内的推荐位，同步APP端上线。现在APP首页大部分都是个性推荐位，做到千人千面的实时推荐；", "个性推荐的算法模型选择是一方面，推荐服务本身工程实现的性能也是很大的考验，要支持500w的用户同时在线量，接口处理请求峰值近1w +QPS。得益于服务化，核心功能都配备完备的监控告警。正在上线的Docker容器化，可以大大提升服务的弹性，方便服务规模的扩缩。", "个性推荐的效果好坏拼的就是用户画像，对自己用户越熟悉，推荐效果才可能越好；斗鱼的用户画像现在是这么玩的：", " ", "从上面的图可以看到，目前我们的推荐系统主要分为三个模块：推荐服务层、数据维护层、监控层。", "1)推荐服务层推荐业务规则过滤，根据业务需求过滤不需要推荐的数据推荐算法配置，根据业务场景及推荐位配置不同的推荐算法ABTest分流，用于推荐算法对比及灰度上线结果数据格式化及排序", "2)数据维护层基础信息数据使用dubbo提供服务，降低服务层对数据源的依赖基于物品相似、用户标签、KPI体系等算法，离线计算与实时计算相结合推荐算法数据存储，基于不同算法数据分开存储维护数据缓存策略，提升缓存命中率，降低DB请求", "3)监控层接口可用性监控、接口性能监控数据质量监控，保证数据准确性推荐效果监控，实时关注推荐转化率", "a.推荐规则单一，点击转化率约5成，仍有提升空间", "b.推荐服务开发效率及扩展性不高", "c.推荐数据分散，复用率低", "这个图片有一个很励志的故事，是这样的：20平米房间，2万张卡，前期投入约40万，一天换一次，一个月内所有的卡都跑一圈。每月收入30万，高峰月入百万。工作室的日常：每天晚上将卡一张张取下来，更换新卡。这是黑产的一个缩影。", "那斗鱼面对的黑产风险有：", "这是前天我从淘宝上搜索斗鱼tv时的结果截图，可以作为现在斗鱼面对黑产形势的一个典型缩影。", "主要集中在直播间人气（主播价值的一个衡量依据）、鱼丸（主播可兑换的礼物）、打折鱼翅（黑卡充值—重灾区，主播可以兑换，危害更大更直接）、主播刷关注，可以从淘宝价格上一瞥黑市上各个刷量的难易和获益程度。同时，在搜索结果中，也能看到斗鱼友台出现在搜索结果中，可见，黑产风险是整个行业要面对的，不仅仅是斗鱼。在这一块，是需要行业内的整体合作的，尽管这个合作有些障碍。", "1 大力惩治的同时，要保证用户体验，尽量降低误杀；", "2 依据用户行为数据获取特征模型，提取作弊行为的特征模型；", "3 针对IP和设备画像，对IP和设备进行嫌疑级别评级打分；", "4 采用线上实时+离线分析双层识别模型为主，辅以“基于规则-rule-based”判断；", "5 提取用户行为风险模型、用户风险评分等级", "对照上图，挨个注释一下：", "建设决策中心(Drools)，识别各种作弊行为，优化反作弊算法模型。风控实时引擎，以实时弹幕、异地登录等关键行为实时流为基础，在结合账号防盗及充值消费数据，实时评估用户风险等级。", "用户行为轨迹分析系统，协助分析用户作弊行为，提炼风控规则。风控离线引擎，不定期更新离线风控规则，接入决策中心，统一配置。", "风控web管理平台，用于配置决策规则、黑白名单管理、行为轨迹查询等功能。风控数据服务化，对外提供安全访问的接口，实时查询用户风险行为，降低损失。", "我们有很多数据；围绕数据我们做了很多努力；要让数据发挥更大的价值", "我个人感觉上大数据一般都会被业务推着走，如果是特意要去玩大数据，可能会比较难推动。比如我从淘宝回武汉，在一号店干了一段时间，在某些场景，确实是不合适。。所以我建议，用集群的方式，先做一个简配版数据仓库，拿到了数据，才好想一些玩法。这个问题大概是这样。", "吴：我们现在用的cdh版本，也是基于的yarn管理集群资源。", "吴：这个问题我有经历，所以有发言权。我当时回武汉，是因为家庭原因。所以我是没有选择的情况下回的武汉。现在武汉的坑位比早两年已经多了很多，公司也多了很多。大公司的研发中心，本土起来的互联网公司都有不少。所以坑位有了，就看自身的成长，能不能满足这些坑的期望。我们的offer被好多公司都有抢过，所以薪资这块，我觉得也不是问题，呵呵。", "吴：按我的理解来回答，网站入口是DNS-lvs-nginx-服务容器，数据安全依靠权限管理（账号-角色-权限）。构建自己的VPN，这个我就不太了解的。太偏网络层面，得找IT同学帮忙。", "5、我在北京工作已有五年。先后在百度,小米做过游戏视频分发的开发工作。大数据这一块接触的比较少。年后想回武汉发展。请问斗鱼有合适的坑吗？", "吴：游戏和视频分发斗鱼现在没有自己做，但是从你的经历来看，在武汉肯定能有坑。", "-待续-", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78664"]},
{"module": ["干货教程"], "note": ["作者 | Zbigniew Baranowski\n\n\nZBigniew Baranowski是一位数据库系统专家，并且是提供和支持中央数据库和基于Hadoop服务的CERN（欧洲核子研究组织）的成员。此博客最初发表在CERN的“CERN数据..."], "title": ["不同文件格式和存储引擎在Apache Hadoop生态系统中的性能比较"], "content": ["作者 | Zbigniew Baranowski", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78890"]},
{"module": ["干货教程"], "note": ["作者|杨冬越 郭景瞻\n1 背景\n前段时间京东公开了面向第二个十二年的战略规划，表示京东将全面走向技术化，大力发展人工智能和机器人自动化技术，将过去传统方式构筑的优势全面升级。京东Y事业..."], "title": ["Spark技术在京东智能供应链预测的应用"], "content": ["作者|杨冬越 郭景瞻", "前段时间京东公开了面向第二个十二年的战略规划，表示京东将全面走向技术化，大力发展人工智能和机器人自动化技术，将过去传统方式构筑的优势全面升级。京东Y事业部顺势成立，该事业部将以服务泛零售为核心，着重智能供应能力的打造，核心使命是利用人工智能技术来驱动零售革新。", "京东一直致力于通过互联网电商建立需求侧与供给侧的精准、高效匹配，供应链管理是零售联调中的核心能力，是零售平台能力的关键体现，也是供应商与京东紧密合作的纽带，更是未来京东智能化商业体布局中的核心环节。", "目前京东在全国范围内的仓库数量已超过700个，按功能可划分为RDC、FDC、大件中心仓、大件卫星仓、图书仓和城市仓等等。RDC（Regional Distribution Center）即区域分发中心，可理解为一级仓库，向供货商采购的商品会优先送往这里，一般设置在中心城市，覆盖范围大。FDC（Forward Distribution Center）即区域运转中心，可理解为二级仓库，覆盖一些中、小型城市及边远地区，通常会根据需求将商品从RDC调配过来。", "结合人工智能、大数据等技术，京东首先从供货商那里合理采购定量的商品到RDC，再根据实际需求调配到FDC，然后运往离客户最近的配送站，最后快递员将商品带到客户手中。这只是京东供应链体系中一个普通的场景，但正因为有这样的体系，使得京东对用户的响应速度大大提高，用户体验大大提升。", "用户体验提升的同时也伴随着大量资金的投入和成本的提高，成本必须得到控制，整个体系才能发挥出最大的价值，于是对供应链的优化就显得至关重要了。", "京东自打建立供应连体系的那一天起，就不断地进行改进和优化，并且努力深入到供应链的每一个环节。优化其实是一门运筹学问题，需考虑在各种决策目标之间如何平衡以达到最大收益，在这个过程中需要考虑很多问题，把这些考虑清楚，问题就容易解决了。举几个简单的例子：", "l  ", "考虑在什么时间，给哪个RDC采购什么商品，采购量是多少？", "l  ", "考虑在什么时间，给哪个FDC调配什么商品，调配量是多少？", "l  ", "在大促来临之际，仓库和配送站要增配多少人手、多少辆货车？", "虽然看上去这些问题都很容易回答，但仔细想想却又很难给出答案，原因就在于想要做到精确不是那么容易的事情，就拿补货来说，补的太多会增加库存成本，补的太少会增加缺货成本，只有合理的补货量才能做到成本最低。", "借助机器学习、大数据等相关技术，京东在很多供应链优化问题上都已经实现系统化，由系统自动给出优化建议，并与生产系统相连接，实现全流程自动化。在这里有一项技术起着至关重要的低层支撑作用–预测技术。据粗略估算，1%的预测准确度的提升可以节约数倍的运营成本。", "怎样理解预测在供应链优化中的作用呢?拿商品补货举例，一家公司为了保证库房不缺货，可能会频繁的从供货商那里补充大量商品，这样做虽然不会缺货，但可能会造成更多卖不出去的商品积压在仓库中，从而使商品的周转率降低，库存成本增加。反之，这家公司有可能为了追求零库存而补很少的商品，但这就可能出现严重的缺货问题，从而使现货率降低，严重影响用户体验，缺货成本增加。于是问题就来了，要补多少商品才合适，什么时间补货，这就需要权衡考虑了，最终目的是要使库存成本和缺货成本达到一个平衡。", "考虑一下极端情况，等库存降到零时再去补货，这时供货商接到补货通知后将货物运往仓库。但是这么做有个问题，因为运送过程需要时间，这段时间库房就缺货了。那怎么办呢?就是利用预测技术。利用预测我们可以计算出未来商品在途的这段时间里销量大概是多少，然后我们让仓库保证这个量，低于这个量就给供货商下达补货通知，于是问题得以解决。总而言之，预测技术在这里发挥了重要的作用，成为关键的一个环。", "预测系统在整个供应链体系中处在最底层并且起到一个支撑的作用，支持上层的多个决策优化系统，而这些决策优化系统利用精准的预测数据结合运筹学技术得出最优的决策，并将结果提供给更上层的业务执行系统或是业务方直接使用。", "目前，预测系统主要支持三大业务：销量预测、单量预测和GMV预测。其中销量预测主要支持商品补货、商品调拨；单量预测主要支持仓库、站点的运营管理；GMV预测主要支持销售部门计划的定制。", "销量预测按照不同维度又可以分为RDC采购预测、FDC调拨预测、城市仓调拨预测、大建仓补货预测、全球购销量预测和图书促销预测等；单量预测又可分为库房单量预测、配送中心单量预测和配送站单量预测等（在这里“单量”并非指用户所下订单的量，而是将订单拆单后流转到仓库中的单量。例如一个用户的订单中包括3件物品，其中两个大件品和一个小件品，在京东的供应链环节中可能会将其中两个大件品组成一个单投放到大件仓中，而将那个小件单独一个单投放到小件仓中，单量指的是拆单后的量）；GMV预测支持到商品粒度。", "整体架构从上至下依次是：数据源输入层、基础数据加工层、核心业务层、数据输出层和下游系统。首先从外部数据源获取我们所需的业务数据，然后对基础数据进行加工清洗，再通过时间序列、机器学习等人工智能技术对数据进行处理分析，最后计算出预测结果并通过多种途径推送给下游系统使用。", "l  ", "京东数据仓库中存储着我们需要的大部分业务数据，例如订单信息、商品信息、库存信息等等。而对于促销计划数据则大部分来自于采销人员通过Web系统录入的信息。除此之外还有一小部分数据通过文本形式直接上传到HDFS中。", "l  ", "在这一层主要通过Hive对基础数据进行一些加工清洗，去掉不需要的字段，过滤不需要的维度并清洗有问题的数据。", "l  ", "这层是系统的的核心部分，横向看又可分为三层：特征构建、预测算法和预测结果加工。纵向看是由多条业务线组成，彼此之间不发生任何交集。", "Ø  ", "将之前清洗过的基础数据通过近一步的处理转化成标准格式的特征数据，提供给后续算法模型使用。", "Ø  ", "利用时间序列分析、机器学习等人工智能技术进行销量、单量的预测，是预测系统中最为核心的部分。", "Ø  ", "预测结果可能在格式和一些特殊性要求上不能满足下游系统，所以还需要根据实际情况对其进行加工处理，比如增加标准差、促销标识等额外信息。", "l  ", "将最终预测结果同步回京东数据仓库、MySql、HBase或制作成JSF接口供其他系统远程调用。", "l  ", "包括下游任务流程、下游Web系统和其他系统。", "预测系统核心层技术主要分为四层：基础层、框架层、工具层和算法层", "HDFS用来做数据存储，Yarn用来做资源调度，BDP（Big Data Platform）是京东自己研发的大数据平台，我们主要用它来做任务调度。", "以Spark RDD、Spark SQL、Hive为主， MapReduce程序占一小部分，是原先遗留下来的，目前正逐步替换成Spark RDD。选择Spark除了对性能的考虑外，还考虑了Spark程序开发的高效率、多语言特性以及对机器学习算法的支持。在Spark开发语言上我们选择了Python，原因有以下三点：", "l  Python有很多不错的机器学习算法包可以使用，比起Spark的MLlib，算法的准确度更高。我们用GBDT做过对比，发现xgboost比MLlib里面提供的提升树模型预测准确度高出大概5%~10%。虽然直接使用Spark自带的机器学习框架会节省我们的开发成本，但预测准确度对于我们来说至关重要，每提升1%的准确度，就可能会带来成本的成倍降低。", "l  我们的团队中包括开发工程师和算法工程师，对于算法工程师而言他们更擅长使用Python进行数据分析，使用Java或Scala会有不小的学习成本。", "l  对比其他语言，我们发现使用Python的开发效率是最高的，并且对于一个新人，学习Python比学习其他语言更加容易。", "一方面我们会结合自身业务有针对性的开发一些算法，另一方面我们会直接使用业界比较成熟的算法和模型，这些算法都封装在第三方Python包中。我们比较常用的包有xgboost、numpy、pandas、sklearn、scipy和hyperopt等", "Xgboost：它是Gradient Boosting Machine的一个C++实现，xgboost最大的特点在于，它能够自动利用CPU的多线程进行并行，同时在算法上加以改进提高了精度。", "numpy：是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表结构要高效的多（该结构也可以用来表示矩阵）。", "pandas：是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。", "sklearn：是Python重要的机器学习库，支持包括分类、回归、降维和聚类四大机器学习算法。还包含了特征提取、数据处理和模型评估三大模块。", "scipy：是在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理和稀疏矩阵等等。", "我们用到的算法模型非常多，原因是京东的商品品类齐全、业务复杂，需要根据不同的情况采用不同的算法模型。我们有一个独立的系统来为算法模型与商品之间建立匹配关系，有些比较复杂的预测业务还需要使用多个模型。我们使用的算法总体上可以分为三类：时间序列、机器学习和结合业务开发的一些独有的算法。", "是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。我们用它来预测高销量，但历史规律不明显的商品。", "这种网络的内部状态可以展示动态时序行为。不同于前馈神经网络的是，RNN可以利用它内部的记忆来处理任意时序的输入序列，这让它可以更容易处理如时序预测、语音识别等。", "：该方法是一种压缩估计。它通过构造一个罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。用来预测低销量，历史数据平稳的商品效果较好。", "全称为自回归积分滑动平均模型，于70年代初提出的一个著名时间序列预测方法，我们用它来主要预测类似库房单量这种平稳的序列。", "又称三次指数平滑算法，也是一个经典的时间序列算法，我们用它来预测季节性和趋势都很明显的商品。", "库存决策树模型，用来预测受库存状态影响较大的商品。", "相似品模型，使用指定的同类品数据来预测某商品未来销量。", "新品模型，顾名思义就是用来预测新品的销量。", "预测核心流程主要包括两类：以机器学习算法为主的流程和以时间序列分析为主的流程。", "通过数据分析、模型试验确定主要特征，通过一系列任务生成标准格式的特征数据。", "不同的商品有不同的特性，所以首先会根据商品的销量高低、新品旧品、假节日敏感性等因素分配不同的算法模型。", "对一批特征进行筛选过滤不需要的特征，不同类型的商品特征不同。", "对训练数据进行分组，分成多组样本，真正训练时针对每组样本生成一个模型文件。一般是同类型商品被分成一组，比如按品类维度分组，这样做是考虑并行化以及模型的准确性。", "选择最优的模型参数，合适的参数将提高模型的准确度，因为需要对不同的参数组合分别进行模型训练和预测，所以这一步是非常耗费资源。", "待特征、模型、样本都确定好后就可以进行模型训练，训练往往会耗费很长时间，训练后会生成模型文件，存储在HDFS中。", "读取模型文件进行预测执行。", "为了提高预测准确度，我们可能会使用多个算法模型，当每个模型的预测结果输出后系统会通过一些规则来选择一个最优的预测结果。", "我们发现越是复杂且不易解释的算法越容易出现极个别预测值异常偏高的情况，这种预测偏高无法结合历史数据进行解释，因此我们会通过一些规则将这些异常值拦截下来，并且用一个更加保守的数值代替。", "计算预测准确度，我们通常用使用mapd来作为评价指标。", "通过分析预测准确度得出一个误差在不同维度上的分布，以便给算法优化提供参考依据。", "将历史销量、价格、库存等数据按照规定格式生成时序数据。", "计算节假日与销量之间的关系，用来平滑节假日对销量影响。", "计算周一到周日这7天与销量的关系，用来平滑周日对销量的影响。", "计算促销与销量之间的关系，用来平滑促销对销量的影响。", "历史销量是不稳定的，会受到节假日、促销等影响，在这种情况下进行预测有很大难度，所以需要利用之前计算的各类因子对历史数据进行平滑处理。", "在一个相对平稳的销量数据上通过算法进行预测。", "结合未来节假日、促销计划等因素对预测结果进行调整。", "我们使用Spark SQL和Spark RDD相结合的方式来编写程序，对于一般的数据处理，我们使用Spark的方式与其他无异，但是对于模型训练、预测这些需要调用算法接口的逻辑就需要考虑一下并行化的问题了。我们平均一个训练任务在一天处理的数据量大约在500G左右，虽然数据规模不是特别的庞大，但是Python算法包提供的算法都是单进程执行。我们计算过，如果使用一台机器训练全部品类数据需要一个星期的时间，这是无法接收的，所以我们需要借助Spark这种分布式并行计算框架来将计算分摊到多个节点上实现并行化处理。", "我们实现的方法很简单，首先需要在集群的每个节点上安装所需的全部Python包，然后在编写Spark程序时考虑通过某种规则将数据分区，比如按品类维度，通过groupByKey操作将数据重新分区，每一个分区是一个样本集合并进行独立的训练，以此达到并行化。流程如下图所示：", "伪码如下：", "repartitionBy方法即设置一个重分区的逻辑返回(K,V)结构RDD，train方法是训练数据，在train方法里面会调用Python算法包接口。saveAsPickleFile是Spark Python独有的一个Action操作，支持将RDD保存成序列化后的sequnceFile格式的文件，在序列化过程中会以10个一批的方式进行处理，保存模型文件非常适合。", "虽然原理简单，但存在着一个难点，即以什么样的规则进行分区，key应该如何设置。为了解决这个问题我们需要考虑几个方面，第一就是哪些数据应该被聚合到一起进行训练，第二就是如何避免数据倾斜。", "针对第一个问题我们做了如下几点考虑：", "l  被分在一个分区的数据要有一定的相似性，这样训练的效果才会更好，比如按品类分区就是个典型例子。", "l  分析商品的特性，根据特性的不同选择不同的模型，例如高销商品和低销商品的预测模型是不一样的，即使是同一模型使用的特征也可能不同，比如对促销敏感的商品就需要更多与促销相关特征，相同模型相同特征的商品应倾向于分在一个分区中。", "针对第二个问题我们采用了如下的方式解决：", "l  对于数据量过大的分区进行随机抽样选取。", "l  对于数据量过大的分区还可以做二次拆分，比如图书小说这个品类数据量明显大于其他品类，于是就可以分析小说品类下的子品类数据量分布情况，并将子品类合并成新的几个分区。", "l  对于数据量过小这种情况则需要考虑进行几个分区数据的合并处理。", "总之对于后两种处理方式可以单独通过一个Spark任务定期运行，并将这种分区规则保存。", "《图解Spark：核心技术与案例实战》一书以Spark2.0版本为基础进行编写，系统介绍了Spark核心及其生态圈组件技术。其内容包括Spark生态圈、实战环境搭建和编程模型等，重点介绍了作业调度、容错执行、监控管理、存储管理以及运行架构，同时还介绍了Spark生态圈相关组件，包括了Spark SQL的即席查询、Spark Streaming的实时流处理、MLlib的机器学习、GraphX的图处理和Alluxio的分布式内存文件系统等。下面介绍京东预测系统如何进行资源调度，并描述如何使用Spark存储相关知识进行系统优化。", "在图解Spark书的第六章描述了Spark运行架构，介绍了Spark集群资源调度一般分为粗粒度调度和细粒度调度两种模式。粗粒度包括了独立运行模式和Mesos粗粒度运行模式，在这种情况下以整个机器作为分配单元执行作业，该模式优点是由于资源长期持有减少了资源调度的时间开销，缺点是该模式中无法感知资源使用的变化，易造成系统资源的闲置，从而造成了资源浪费。而细粒度包括了Yarn运行模式和Mesos细粒度运行模式,该模式的优点是系统资源能够得到充分利用，缺点是该模式中每个任务都需要从管理器获取资源，调度延迟较大、开销较大。", "由于京东Spark集群属于基础平台，在公司内部共享这些资源，所以集群采用的是Yarn运行模式，在这种模式下可以根据不同系统所需要的资源进行灵活的管理。在YARN-Cluster模式中，当用户向YARN集群中提交一个应用程序后，YARN集群将分两个阶段运行该应用程序：第一个阶段是把Spark的SparkContext作为Application Master在YARN集群中先启动；第二个阶段是由Application Master创建应用程序，然后为它向Resource Manager申请资源，并启动Executor来运行任务集，同时监控它的整个运行过程，直到运行完成。下图为Yarn-Cluster运行模式执行过程：", "我们都知道大数据处理的瓶颈在IO。我们借助Spark可以把迭代过程中的数据放在内存中，相比MapReduce写到磁盘速度提高近两个数量级；另外对于数据处理过程尽可能避免Shuffle，如果不能避免则Shuffle前尽可能过滤数据，减少Shuffle数据量；最后，就是使用高效的序列化和压缩算法。在京东预测系统主要就是围绕这些环节展开优化，相关Spark存储原理知识可以参见图解Spark书第五章的详细描述。", "由于资源限制，分配给预测系统的Spark集群规模并不是很大,在有限的资源下运行Spark应用程序确实是一个考验，因为在这种情况下经常会出现诸如程序计算时间太长、找不到Executor等错误。我们通过调整参数、修改设计和修改程序逻辑三个方面进行优化：", "l  减少num-executors，调大executor-memory，这样的目的是希望Executor有足够的内存可以使用。", "l  查看日志发现没有足够的空间存储广播变量，分析是由于Cache到内存里的数据太多耗尽了内存，于是我们将Cache的级别适当调成MEMORY_ONLY_SER和DISK_ONLY。", "l  针对某些任务关闭了推测机制，因为有些任务会出现暂时无法解决的数据倾斜问题，并非节点出现问题。", "l  调整内存分配，对于一个Shuffle很多的任务，我们就把Cache的内存分配比例调低，同时调高Shuffle的内存比例。", "参数的调整虽然容易做，但往往效果不好，这时候需要考虑从设计的角度去优化：", "l  原先在训练数据之前会先读取历史的几个月甚至几年的数据，对这些数据进行合并、转换等一系列复杂的处理，最终生成特征数据。由于数据量庞大，任务有时会报错。经过调整后当天只处理当天数据，并将结果保存到当日分区下，训练时按天数需要读取多个分区的数据做union操作即可。", "l  将“模型训练”从每天执行调整到每周执行，将“模型参数选取”从每周执行调整到每月执行。因为这两个任务都十分消耗资源，并且属于不需要频繁运行，这么做虽然准确度会略微降低，但都在可接受范围内。", "l  通过拆分任务也可以很好的解决资源不够用的问题。可以横向拆分，比如原先是将100个品类数据放在一个任务中进行训练，调整后改成每10个品类提交一次Spark作业进行训练。这样虽然整体执行时间变长，但是避免了程序异常退出，保证任务可以执行成功。除了横向还可以纵向拆分，即将一个包含10个Stage的Spark任务拆分成两个任务，每个任务包含5个Stage，中间数据保存到HDFS中。", "为了进一步提高程序的运行效率，通过修改程序的逻辑来提高性能，主要是在如下方面进行了改进：避免过多的Shuffle、减少Shuffle时需要传输的数据和处理数据倾斜问题等。", "1 避免过多的Shuffle", "l  Spark提供了丰富的转换操作，可以使我们完成各类复杂的数据处理工作，但是也正因为如此我们在写Spark程序的时候可能会遇到一个陷阱，那就是为了使代码变的简洁过分依赖RDD的转换操作，使本来仅需一次Shuffle的过程变为了执行多次。我们就曾经犯过这样一个错误,本来可以通过一次groupByKey完成的操作却使用了两回。业务逻辑是这样的：我们有三张表分别是销量（s）、价格（p）、库存（v），每张表有3个字段：商品id（sku_id）、品类id（category）和历史时序数据（data），现在需要按sku_id将s、p、v数据合并，然后再按category再合并一次，最终的数据格式是：[category，[[sku_id, s , p, v], [sku_id, s , p, v], […]，[…]]]。一开始我们先按照sku_id + category作为key进行一次groupByKey，将数据格式转换成[sku_id, category , [s，p, v]]，然后按category作为key再groupByKey一次。后来我们修改为按照category作为key只进行一次groupByKey，因为一个sku_id只会属于一个category，所以后续的map转换里面只需要写一些代码将相同sku_id的s、p、v数据group到一起就可以了。", "两次groupByKey的情况：", "修改后变为一次", "的情况：", "多表join时，如果key值相同，则可以使用union+groupByKey+flatMapValues形式进行。比如：需要将销量、库存、价格、促销计划和商品信息通过商品编码连接到一起，一开始使用的是join转换操作，将几个RDD彼此join在一起。后来发现这样做运行速度非常慢，于是换成union+groypByKey+flatMapValue形式，这样做只需进行一次Shuffle，这样修改后运行速度比以前快多了。", "实例代码如下：", "如果两个", "需要在", "后进行", "操作，可以使用", "转换操作代替。比如， 将历史销量数据按品类进行合并，然后再与模型文件进行", "操作，流程如下：", "使用cogroup后，经过一次Shuffle就可完成了两步操作，性能大幅提升。", "l  在Shuffle操作前尽量将不需要的数据过滤掉。", "l  使用comebineyeByKey可以高效率的实现任何复杂的聚合逻辑。", "comebineyeByKey属于聚合类操作，由于它支持map端的聚合所以比groupByKey性能好，又由于它的map端与reduce端可以设置成不一样的逻辑，所以它支持的场景比reduceByKey多，它的定义如下 ：", "和", "内部实际是调用了", "我们之前有很多复杂的无法用reduceByKey来实现的聚合逻辑都通过groupByKey来完成的，后来全部替换为comebineyeByKey后性能提升了不少。", "有些时候经过一系列转换操作之后数据变得十分倾斜，在这样情况下后续的RDD计算效率会非常的糟糕，严重时程序报错。遇到这种情况通常会使用repartition这个转换操作对RDD进行重新分区，重新分区后数据会均匀分布在不同的分区中，避免了数据倾斜。如果是减少分区使用coalesce也可以达到效果，但比起repartition不足的是分配不是那么均匀。", "虽然京东的预测系统已经稳定运行了很长一段时间，但是我们也看到系统本身还存在着很多待改进的地方，接下来我们会在预测准确度的提高、系统性能的优化、多业务支持的便捷性上进行改进。未来，随着大数据、人工智能技术在京东供应链管理中的使用越来越多，预测系统也将发挥出更大作用，对于京东预测系统的研发工作也将是充满着挑战与乐趣。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78623"]},
{"module": ["干货教程"], "note": ["\n文|数加大数据团队 (任何不表明作者和来源36大数据的转载均为侵权。）\n昨天晚上老公突然说到，要是excel每天能自动生成自己想要的所有数据就好了。\n我楞了一下，老公居然要做报表，我是遇到..."], "title": ["我是数据分析师（一）：假老公提Excel数据自动更新的需求"], "content": ["文|数加大数据团队 (任何不表明作者和来源36大数据的转载均为侵权。）", "昨天晚上老公突然说到，要是excel每天能自动生成自己想要的所有数据就好了。", "我楞了一下，老公居然要做报表，我是遇到了个假老公么！不过，他说得也挺有道理的，在与excel打交道的过程中免不了会遇到这些问题：", "好不容易把excel的数据分析好做成了报表，过几天，同样的数据更新了，而报表数据却不会自动更新，又得基于新数据重新来一遍；", "如不想手工重复操作，就得在Excel里写VB宏定义，再要么就直接写代码了，拼不过代码啊。", "好吧，回想了一下，之前了解过Quick BI，貌似可以实现这样的功能，于是就开始进行了尝试。（如果有人想要了解可以去他们的官网自行了解哦）", "首先和老公商量，拟定了一个他经常会遇到的场景。场景如下：有一张每周都需要更新的excel销售订单表：company_sales_record。此表主要为销售主管提供通过时间查询每天的订单量及销售金额，并且看每个省份的销售和利润情况。", "当每周销售订单表数据更新时，报表的数据也需要同步更新。", "接下来就是如何用qbi来实现这个功能。", "我现在有一张每周更新的excel销售订单表，于是我把excel另存为UTF-8编码方式的.csv文件", "，上传到qbi中。", "2.1 选择本地的csv文件，上传成功后，会自动创建数据集，", "本实例中，我需要新建计算字段，以及将省份等字符型转化为地理维度，以便于可以在地图上展示，于是我先点击【编辑】进行数据建模界面。", "注意：如无需做上述处理，可直接点击【新建仪表板】会【分析】进行报表制作界面。", "2.2 新建计算字段", "我需要对csv文件里的字段做些简单的计算，比如我需要算平均每单利润_元，于是我点击上图中的“编辑”，进入到数据建模中，右键选择【新建计算度量】", "2.3 将省份，城市等转化为地理纬度，如下图：2.4 点击确认后，保存数据集", "\n", "3.1 在数据集建模界面的上方菜单中，点击【新建仪表板】，则可直接转到仪表板界面", "3.2 制作每日订单销售趋势图表", "1）选择刚才保存的数据集：company_sales_record，配置数据源及显示的维度和度量，", "2）选择订单数量和订单金额，并在右侧样式中，选择双Y轴展示，", "3）为方便按日期范围查询，拖入查询控件，并将日期控件设置为时间区间，图表展示见下图：", "3.3 各省订单量及利润分布及各省销售明细数据查看", "1）从左侧组件栏中拖入气泡地图和交叉表", "2）两个图例均选择数据来源：company_sales_record，并配置需要显示的字段", "3）设置两个图例的联动", "选中 气泡地图 图例——右侧配置的“高级”——多图关联，分别选择两个图例中对应显示的字段。如下图", "注意：如果想让上面的日期控制范围能覆盖到各省的销售及明细数据，可以选中日期控件，并在右侧配置框中，配置日期的作用范围即可。", "4）保存仪表板。", "效果展示如下：本csv文件中的订单数据是截止到2016.12.24.", "4、仪表板制作完成", "点击预览，查看仪表板的效果，确认没问题后，可以选择public发布，实现免登。", "5、还有最后一个问题，如果一周后我有更新了的销售订单表呢，那以上的操作是否要重新做一遍呢？", "答案是：肯定不需要啊", "当然需要保证如下：", "1）更新后的销售订单表：company_sales_record 采用同一表名", "2）表字段信息保持不变", "那怎么操作呢？", "1）上传更新后的订单表company_sales_record 文件到探索空间中，系统会提示是否要覆盖原有的数据，点击覆盖", "\n2）刷新之前做好的仪表板：每日订单销售情况汇总， 可以看到数据已经自动变化了，更新后的数据已自动展示。", "Yeah！搞定，还能根据日期查询，实现联动。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78076"]},
{"module": ["干货教程"], "note": ["\n\n\n作者|李晓春 中兴开发者社区\n1.1 分布式数据管理之痛点\n\n\n\n为了确保微服务之间松耦合，每个服务都有自己的数据库, 有的是关系型数据库（SQL），有的是非关系型数据库（NoSQL）。\n开发企业..."], "title": ["微服务架构下的分布式数据管理"], "content": ["转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78156"]},
{"module": ["干货教程"], "note": ["\n作者|代清来\n2015年8月19日国务院常务会议上通过《关于促进大数据发展的行动纲要》；为贯彻落实国务院的通知，同年12月31日，根据这份纲要的精神，农业部又发布了《关于推进农业农村大数据..."], "title": ["对农业大数据资源目录体系的一点思考"], "content": ["作者|代清来", "2015年8月19日国务院常务会议上通过《关于促进大数据发展的行动纲要》；为贯彻落实国务院的通知，同年12月31日，根据这份纲要的精神，农业部又发布了《关于推进农业农村大数据发展的实施意见》；2016年10月14日，农业部又发布《农业大数据试点方案》。", "行动纲要指出了大数据的重要性，实施意见提出了农业大数据的具体方向，试点方案则明确试点的范围和区域。因此，农业大数据的重要性和紧迫性毋庸置疑，数据资源目录体系又是从大数据中发现新知识、创造新价值和提升新能力的一个必须的手段。因此基于大数据政策、农业行业特点和统计产品分类，再考虑到互联网+农业、农产品电商的现状和发展趋势，笔者对农业大数据资源的目录体系做了一些思考和探索，从多个维度和领域对农业大数据进行了初步的规划分类，并对技术实现思路进行了初步的考虑。", "依据农业部的管理体系，农业部下属与具体业务相关的司局包括：经管司、市场司、计划司、国际司、科教司、种植业司、种子局、农机化司、畜牧业司、兽医局、农垦局、农产品加工局、渔业局等。", "因此，从管理体系的维度出发，农业大数据资源可以分为如下五类：", "说明：根据实际业务范围对目录体系做了合并和简化。", "按照涉农数据的领域，分为十大领域：", "其中国际农业数据侧重了解其他国家的农业总体情况，全球遥感侧重了解农业的生产情况，质量安全侧重于了解农产品质量安全，科技教育侧重于对农民的科技教育，设施装备是侧重于农业机械化，农业要素是指什么目前还不太清楚，", "结合农产品批发市场现状和统计产品分类对农产品数据资源目录体系做了整合，并依据互联网+农业的发展趋势做了适当扩展，分为如下八类：", "由于人民对生活水平的逐步追求跨境电商的发展，肉类、奶类和水果等生鲜农产品中的进口品种越来越多。", "在农产品领域，根据农产品生命周期的六个大的环节，把数据分为“六大核心数据”。", "根据获取数据的渠道和方式的不同，分为互联网、物联网、政务网和信息采集员采集等多种方式。", "根据数据所有权的不同，分为互联网、政府、企事业单位和个人等多种方式。", "技术上主要考虑建立多级目录体系，利用关键词和标签技术实现数据的精准标识、多重标识。", "综上所述，由于大数据的4V特性和农业的复杂性，农业大数据资源目录体系建设是一项长期复杂的工作，我先做了一点微小的工作，在此抛砖引玉，先抛出个初步思路规划，期待与行业专家和技术专家共同探索研究。", "本文由 代清来 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据http://www.36dsj.com/archives/78013，任何不经同意的转载均为侵权。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78013"]},
{"module": ["干货教程"], "note": ["\nQ：有哪些数据是初学者不知道而只有数据专家知道的？\nWhat do experienced data scientists know that beginner data scientists don’t know?\n36大数据专稿， 本文由36大数据翻译，不..."], "title": ["Quora：有哪些数据科学新人不知道老手才知道的？（独译）"], "content": ["What do experienced data scientists know that beginner data scientists don’t know?", "A(I-Kang Ding,来自纽约软件公司capital one labs的数据专家)：（2016.5.25选自Jeffrey shen,yelp网站的数据专家）", "谢邀！其实在这些浩瀚无边的事务中，我算不上是一个“经验老手”，仅仅是相比几年前更有经验了一些而已（理论上应当如此）。这里是我对这个问题的一些看法。", "（别人不会注重你的过程只是关注你的结果）”：在大多数的情况下，你的商业伙伴（利益共同者）不会真的那么在意你是怎样完成这件事的(Jerrod Lowmaster在这一点上的回答是完全正确的)你真正要做的是提取出你所认识的事物内在的专业知识然后用非专业人士熟知的语言向他们解释。", "在另一方面，那些维持或者替换你的准则的人们是很在意你如何完成一件事的。通常，这些人对你的未来起着十分重要的作用。所以一定要为未来的自己留下丰富的评论和证明。", "“很多情况下，人们并不知道自己想要的是什么，直到你展示给他们看。——史蒂夫乔布斯”", "好吧，也没有乔布斯说的那么极端，但是你必须能够时常从海量的重复信息中提供精简的有用的信息给你的合作伙伴。你的利益伙伴们经常会问一些有正确的大方向的问题，但是并不是足够明确而让你可以着手去做。比如当你被要求建立一个模型时，你大概首先会想：（1）我们需要去解决的问题是什么？（2）在解决这些细节的问题时建立模型是否是最佳途径？", "不得不让人难过地承认的是，一些生意上的问题并不存在定量的数据解决方案，所以如果你能尽快去发现到是否值得追求数据科学解决办法那就是再好不过了。", "谢邀！从我经历的一些失误当中我获得了一些个人的经验总结。", "在把你的数据组进行回归分析和计算机统计之前，观察你的数据是很重要的。有些时候你也许会从一些完全不同的数据组中获得非常相似的结果。就拿“安斯库姆四重奏”作为一个例子，这四组图表有相同的平均值、变化趋势，甚至是相同的回归模型。然而，很显然他们互不相同。", "我见过很多人非常喜欢建模并且认为是最关键的环节。但是，大多数人，尤其是那些非专业的人是不会重视你有什么样的模型的，他们关注的只是从调查中获得的分析和推论。", "：这个与第二条结论有关，而且它不仅仅适用于成为数据学家 这个目的。如果你有精确的模型、准确 的分析、合理的推断甚至是完美 的成套体系，如果你缺少这种“讲故事的能力”，你仍然不能够将你的研究传达给你的上司或是顾客。在世界上的各界精英全部都是具备良好的演说能力的。", "对问题中的两个概念笼统地概括是很危险的——如今有一些非常具有洞察力的成熟的新一代数据科学家。同时，有一些事，我希望你们最好从经历过的伤疤中学到。", "想出能检验你假说的最低强度的试验方法，或者至少是一些接近你的假说的答案。时间是你最稀缺的资源，所以你必须充分利用时间来加快学习速度。", "•正如皮特•诺维格（谷歌研发主管）所说, 莫妮卡•罗加蒂补充道：", "初级数据科学家们，尤其是刚从学校毕业的那些，", "当我们看到随机噪声的时候我们能察觉到信号。当我们处理数据的过程中简单地出现了一个漏洞时，我们太着急以至于无法察觉出确定的证据。但是我也相信经验能帮助我们养成健康正确的怀疑态度。或者这些也仅仅是来自我的认知偏差。", "原文：", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78323"]},
{"module": ["干货教程"], "note": ["\n3月29日晚，浙江移动大数据中心——傅一平博士，就《运营商大数据变现实践》与大家分享数据利用的心得与经验，为大数据百人会社群带来两个多小时的精彩演讲。\n运营商拥有庞大且具有绝对话语权..."], "title": ["傅一平博士：“去电信化”  运营商变现需要从建模中发现数据的价值"], "content": ["运营商拥有庞大且具有绝对话语权的数据资源、数据储备，关于对数据利用的心得与经验，傅一平先生围绕浙江移动的平台能力、数据管理、数据能力、标签能力、产品服务这5大内容深入浅出地做出描述。", "下面是以第一人称视角根据现场演讲整理出来的笔记：", "2004年我进入浙江移动，3年前开始从事大数据相关的工作，推动了浙江移动大数据变现工作的开展，浙江移动在大数据变现过程中碰到了很多问题，我们在实践和探索中也总结出了一些经验，今天与大家分享一下。", "数据变现我不能直接去谈商务模式，因为商务模式依赖于我们平台、数据和标签能力。运营商的平台能力分为五横一纵，从数据采集、数据处理、数据分析、数据访问到数据应用，还有管理平台，这是一个标准架构。互联网公司的大数据平台架构可以与其作一定的映射，当前，浙江移动从这五个方面也初具雏形。", "浙江移动经过两期系统的建设，采用了非常多样的技术组件，底层用的是华为的BDI，中间的处理层用的是hadoop，有商用的，也有开源的版本，大家可以看到里面有3个应用集群，中间我们使用的是MPP，MPP用来做我们的报表，主要是融合、复杂以及交叉的分析，当然我们也采用了ASTER的数据挖掘库，它提供了一些现成挖掘的API，计算速度比较快，流处理的话我们使用的是IBM STREAM，海量数据处理方面很不错。往上一层我们主要是做读写分离的，有Oracle、HBASE、KV数据库，为应用层提供支撑。", "运营商如果想对外变现，它必须具备像阿里云一样方便驻户进驻的能力，它的平台能力应该是所见即可得的，浙江移动大数据平台可以在一周内实现合作伙伴的入驻并实现全方位的开放，当前外部合作伙伴已经超过30个，当然这仅仅是起步的阶段，“我方搭台、对方唱戏”的这种模式对平台来讲是必不可少的，很多公司如果没有多驻户的开通能力，变现是不大可行的。", "数据管理每个公司需要，浙江移动这些年做起来艰巨，我这里提炼了对数据管理非常核心的几点，也是在实际中碰到的：", "①二级互动，你的数据必须与源系统打通，源系统变动时，大数据平台所有相关的数据字典都应该变动，这能解决两张皮的问题。", "②因为大数据平台组件很多，你的数据管理平台必须让所有的技术组件对你开放，就比如说我们采用的华为BDI的产品，我就必须让他开放所有BDI的元数据接口，这样整个公司的数据管理体系才是一体的，否则你就断了条胳膊，你做的数据分析都无法贯通，这是大家在引入技术组件特别要注意的。", "③数据建模，强调数据标准化和可视化，你所有的建模都应该是基于规范的，", "当然数据管理也是一种追求，你一个小公司搞一个元数据产品其实没有必要，假如公司做大了，你做数据管理成本会非常高，因此必须要通过系统化、透明化的方式来管理，不做的代价是数据知识没有积累和传承。", "数据管理虽然做了很多规范，但所有的规范都应该纳入系统中，这样才能保证你所有的系统数据管理规范都能在系统中落地，不然靠人去推动，基本上你的数据管理系统是会失败的。", "，这是我们多年数据管理中深刻的体会。", "，这是我观点。而且数据管理平台非常强调运营，它需要这个平台不停地迭代，相对而言它对定制化的要求很高，同时，我们的数据管理平台在大数据时代要适配所有的技术组件，要能屏蔽技术细节，对上提供透明化，可视化的访问和开发能力。", "浙江移动的数据是非常丰富的，它提供1600种数据类型、300个融合模型.10万个客户标签以及上百个API，运营商只有把基础设施建好了才能为我们的客户提供更快更好的数据服务。", "，离开了差异化的能力，你说运营商去做变现和互联网公司竞争也不大现实，因为我们在人才、薪酬、机制、流程各个方面处于全面的落后，还是需要利用我们差异化的能力去在市场上进行探索和竞争，这也是迫不得已。", "浙江移动拥有6000万的客户，每个月手机上网的数据是1.5万亿条，通话记录每个月是100亿次，位置轨迹每个月是1500亿条，宽带记录每个月是10万亿条，现在的集群规模是1400+，集群规模与互联网公司比有一定差距，我们也在努力。", "运营商最核心的数据有八个方面通信、支出、社交、上网、身份、位置、时序、终端，当然还有外部数据，以前运营商是不太重视的，比如大众点评网数据、天猫品牌、京东商城、淘宝数据等，在对外变现的时候，你会发现爬取这类数据实际上对运营商非常重要。", "我们现在通过通话数据可以把所有的社交关系认出来，父母、亲人、朋友所有的社交人脉路径都能找出来而且非常准；另外运营商的通讯机制保障了你每次通话、上网所有的终端行为都会上传运营商网络，数据质量也很好，终端市场报告由运营商来出示非常有说服力，因为所有的数据都是活的也是及时的；还有黄页数据，任何用户比如打一个电话是黄页数据的话，你就可以知道这个用户的需求，对于精准营销非常重要；当然我们运营商还有和教育的数据，比如我们想知道成绩好孩子的家长有什么样的特性，运营商可以做这样的分析报告，我们通过校园通这类行业应用产品大概能知道所有的学校、家长、教师、学生的信息，基于这些信息可以给出分析结果，未来前景巨大。", "运营商的数据有四个特点：完整性、连续性、多维性、整合性。完整性是指比如京东的数据、淘宝的数据实际上运营商都有，也许没有那么详细，但可以拥有任何一家的互联网公司的数据；连续性是指我们每天生活在时空中，要么在线下要么在线上，运营商所有的数据都有，比如线下你所有的位置数据运营商有，线上你所有上网数据也有，基于线上线上再加用户属性，运营商可以完整连续的刻画；多维性是指运营商拥有时间、空间和用户多种属性数据，相对可以实施降维打击，通过时空交叉验证可以把模型做的非常准；整合性的意思是运营商拥有号码、IMEI、IDFA、COOKIE几乎所有的用户ID，所有ID信息整合在一起可以实现全数据的关联，这也是运营商的巨大优势。", "至于大家非常关心的HTTPS，我不太在意，HTTPS现在呈扩大的趋势跟流量劫持有一定关系，主要还是因为互联网为了保证它自身利益，从数据的角度来讲，即使HTTP最后仅有20%可开采，对运营商也足够了来构建自己的用户画像了，况且现在很多数据还没有开采，应对HTTPS的重要程度远远低于运营商对自身数据采集解析的要求，没必要杞人忧天；另外APP协议成千上万，HTTP整个流量占比也不超过30%吧，再者运营商有巨大的资源，完全可以利用资源去换取内容知情权，比如腾讯大小王卡，给你免费定向流量，用资源换取内容知情权，再通过全内容为你提供增值服务，这个套路很多公司都在做，我们运营商要做在当下。", "如果搞过搜索、爬虫、自然语言看来这个天眼用户偏好检索系统就知道它想干什么，实际上百度做的就是通过输入关键词它能找到所有的信息，运营商能做的是我输入一个关键词它能找到所有相关的用户，因为百度是没有用户的，而运营商拥有所有的用户，比如我输入足球关键词，我可以把五千万用户对足球的偏好进行排序，大家可以想象未来运营商变现的空间有多大。", "运营商虽然拥有数据，但我不能说就等于抱着金矿或原油，运营商的数据开采的代价非常大，", "，否则你数据直接去卖也卖不了几个钱，还有运营商卖原始数据实际上等同“杀鸡取卵”，安全上不可取也不可持续，", "，这也是当下变现的软肋，因为我们基础建模能力还很弱。", "运营商正在从传统通信画像向真正的客户画像转变，在事件标签中，我们可以把家人、亲戚、工作伙伴及行业通话信息串起来，我们有所有的线下事件、线上事件、专题事件，比如前段时间的云栖大会把所有事件记录下来以便对专门的用户进行专项分析，别看云栖大会这两年很热闹，我们分析过两年的会议数据，用户连续参加两年云栖大会的比例极低。", "用户社会画像看起来与运营商的数据没多大关系，但恰恰是基于运营商的数据可以做出来，包括职业、人生阶段甚至家庭等等，都能判断出来。而位置标签无论是常驻位置、工作位置、事件出行、移动轨迹，不管是连续还是静态的都是可以抓取以作用户分析，当然有人会提这涉及到用户的隐私，我们所有的信息处理时是去隐私化的，主要是用来做特征分析，大家不用担心信息安全的问题，互联网公司其实一样，他拿着用户所有的交易信息来做特征分析，但绝不会把清单型数据透露给外面，这是我们秉承的原则。上网行为中，无论是APP的使用、网页访问、使用时间序列很有价值，以此可以判断用户的偏好，为用户更好的服务，这个互联网公司做的比较多，运营商正处在起步阶段。", "现在我们的产品体系很丰富，有客流、选址、引客、APP、反欺诈和验真，我们也有广告平台“喜从天降”、终端产品“有机密”等，同时提供大量标准化的产品，这类产品要创造效益，还有很长的路要走。", "比如选址雷达，你可以根据标签选择偏好用户所在的位置，以方便商户选址；另一款叫引客雷达是你可以上传一批目标客户的号码，我通过Lookalike的方式帮你找到潜在用户在哪里；至于金融验真产品，大家不用担心用户隐私的问题，因为都是通过用户授权的，身份验证主要是通过手机、身份证和姓名进行三证合一，当然还可以做刷单行为的判断，比如滴滴刷单，你可以把司机手机号码和客户的号码交给运营商，运营商通过两个号码之间位置切换的关系来判断是不是异常。", "运营商合作的几种形式：", "比如验证，运营商提供标签，你输入用户号码，我们提供标签；这是第一种服务模式我们叫做数据云化，当然无论是模型还是标签都是一样的。这个更多的运营在验证场景，主要通过用户授权", "由于现在大数据处于起步阶段，许多企业处于观望心态。浙江移动提供了模型锤炼的服务，我开放你数据服务，当然这些是去隐私的，你可以在我们平台上进行模型淬炼，如果你觉得达到要求了，我们再正式商用、收费；", "：运营商（包括浙江移动）我们的产品研发能力是有限的，不可能在各个方面都做端对端的产品，我们希望有更多的伙伴和我们合作，分成都是可以的，这一块我们也在做许多尝试。", "：比如我刚才提到的“喜从天降”，运营商有许多渠道，比如微信公众号等等，可以将运营商在数据方面的能力与各产业合作伙伴的需求结合在一起，短信等传统渠道由于政策方面的原因，还有待观望。", "信息服务费指的是按次查询信息需要收取的费用，比如验证，一个号码与身份证的匹配关系，比方说每次收取一次一块钱或者两块钱，这都是通过用户授权以后，我们可以和银行合作收取的信息服务费。信息使用费是指你租用我们的数据之后，我们按照数据里面用户的量，比如每一千用户收取多少费用。设备租赁费和阿里云类似的方式。", "我们客流产品是如何收费的？它要收取功能费和增值功能费，这个产品涉及到位置，我们可以按照位置的扇区可以单独收费。这是对于相对成熟产品的收费方式。", "运营商是非常负责任的国企，有自己的数据安全原则：“凡是能够直接或者间接地识别自然人的任何信息，包括身份证、手机号码、IMEI、COOKIE等，都不能对外提供”，这一点许多互联网公司是没有遵循的，他们的串号、COOKIE都是可以互相交换的，但是对运营商来讲这一点很难，因为COOKIE也是代表了用户的属性，这也是存在隐私风险的，运营商做大数据难也是难在这里，国企对于稍有模糊的地带都是不敢踩的，因此我们是非常艰难的。“不涉及个人清单信息的数据分析服务和产品服务，比如行业分析报告”，这些是可以做的，因为这些是统计类的信息。“基于用户授权的各类数据服务”这些我们也是都可以做的，不论是查询标签，还是个人身份信息。", "只有确保这些前提下，运营商才能做各类变现服务，挑战难度是相当大的，这也是在考验运营商的创新能力。", "：这一类产品业务比较诚实，产品标准化程度高，我们向第三方购买也有可能。", "：运营商只有数据，没有任何产品、建模能力。这时我们希望与合作伙伴合作然后大家分成。", "：这是一种比较传统的方式，通过招标给运营商提供定制服务，然后运营商自己面对最终客户。", "：运营商开放部分脱敏数据，帮助合作伙伴进行数据产品孵化。", "浙江移动为了做大数据运营变现专门成立了大数据中心、云计算中心。由这两个中心保障大数据专业化的进行服务。没有专业的人做专业的事，做大数据变现是不可能的。", "：运营商经营分析起家，以前的数据挖掘基本是数据外包的形式做的，但是进入大数据时代以后，对建模的能力要求越来越高，运营商非常稀缺这一类资源。", "：运营商现在也在建立大数据平台，现在平台已经建立起来，但是售后服务支撑能力是非常薄弱的，因为技术组件太多了，我们在变现中发现了大量的问题，需要我们在技术方面进一步突破。", "：运营商对各个行业是不熟悉的，希望有能撮合最终客户和运营商的企业，这对我们很重要。", "这里有一部分文化因素，因为许多从业人员是从运营商的体系中转过来的，在策划、运营方面缺乏经验，我们需要擅长做运营的企业与我们合作，其实我们什么都缺。", "大数据变现对运营商的挑战巨大，", "。大家都知道做大数据十有八九是会失败的，你做了这么多产品、采集了这么多数据，有多少能用，有多少能真正成功，有多少客户愿意来买单很难说。十个里面能成功一两个已经很好了，这个时候你的速度是很重要的，如果你开发一个应用要两个月甚至半年，那基本上没法跟上节奏。", "另一个是渠道，", "比如说我们现在有大数据交易平台，但是现在在大数据交易平台在个人隐私没有定论的前提下是很难做的。我们在做大数据变现的时候传统渠道不能用，必须要创造新的渠道，这样才能让大数据承载在上面，大数据才能成为渠道的放大器。", "，如果做的产品与差异化的数据没有结合起来，十之八九就会失败。相对于互联网公司，你没有任何竞争力，唯一的核心竞争力就是数据。", "最后是运营，", "，运营商原来的机制、流程比较冗长，它的迭代速度还是比较慢的。", "另外，", "要，因为运营商进入流量经营时代后，内容运营越加迫切，传统的标签体系根本无法支撑，我们在对外变现中，发现外部客户对于标签的要求是非常高的，这驱动了我们去完善自己的内容标签体系，同时，运营商通过大数据，也可以培养和挽留住一批人才，你必须给一些人一些出口，可能运营商对外短期内无法规模变现，但它也承担着更多的使命，你不去实践，永远不懂大数据到底是什么。", "大家都在找大数据的商业模式，我觉得现在商业模式已经很多了，", "。再找到第三种已经很难了。我们现在已经尝试了一些，无论是做报告还是洞察，实际上最大规模的变现就是金融和广告。", "，需要精益求精，在某一点上进行单点突破，而不要求泛，每一方面都做一点但每一点都没办法做透。因为无论你的数据模型能力，还是运营能力、产品能力，都存在非常大的欠缺，你不可能有精力做那么多，我们希望能创造一个生态，与各界合作共创大数据的未来，服务好我们的客户，这也是我的期望。", "Q：请问客流分析产品的市场需求有多大？", "A、客流分析的产品市场相当大，但它受限于定位的精度。但是如果我们能基于精准位置定位，比如说如果我们取MR的数据来做，能够做到50米到100米的精准定位，这个是非常广阔的市场，通过1-2年的时间，这个产品肯定能做出来，这个是非常有前景的，因为他可以定位到某幢楼，我对此也是非常有信心。", "Q：MR和DPS是如何连接起来？", "A、这个和运营商的数据有关系，MR相当于测量报告，我们运营商上网数据需要SE—MME有一个信令数据，这个信令数据里面就有我们的经纬度数据，它里面的ID里面有一个标识，这个标识可以和MR里的标识结合起来，这个时候你就可以得到一个关联。", "Q：目前浙江移动在大数据广告方面有开展业务吗？", "A、广告这一块，我们之前有做过，但是由于安全的原因这一块儿暂停了，运营商的数据有它的特殊性，比如手机阅读这一块儿，我们移动有个咪咕阅读，我们每个月可以帮他新增5万的手机阅读用户。通过DPI数据是很方便找到他的竞争对手用户。", "本文由", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78816"]},
{"module": ["干货教程"], "note": ["\n作者：蹇智华 达观数据\n前言\n达观数据作为一家提供大数据服务的公司，经常会遇到客户上报数据的需求。这样的请求不需要马上返回处理结果， 而是需要后台将一系列的上报数据进行统一归档整理..."], "title": ["Kafka设计原理以及在达观产品中的应用"], "content": ["作者：蹇智华 达观数据", "达观数据作为一家提供大数据服务的公司，经常会遇到客户上报数据的需求。这样的请求不需要马上返回处理结果， 而是需要后台将一系列的上报数据进行统一归档整理挖掘， 然后将结果数据呈现给客户。这样的业务需求需要达观提供数据暂存服务，也就是说我们需要一个系统在生产者（客户上报数据）和消费者（后台数据处理）之间进行沟通，简而言之叫系统间通信消息系统，这种模型就是经典的生产者（producer）、消费者（consumer）模型。", "然而有一个消息系统正好是为了应对这种业务场景而生，它就是kafka。那么kafka到底是一个什么样的系统？有什么特点？实际吞吐表现又如何？带着这些问题，我们一起来了解一下。", "首先根据", "介绍，知道kafka是一个分布式流处理平台，一个可处理企业级发布/订阅的消息系统，并且具有高容错性和消费及时性等特点，那么它是怎么做到这一点的呢？接着往下看。", "主题(topic)和日志(log)设置是kafka一大特色，一个kafka集群可以创建多个topic， 每个topic都相当于一个消息队列，这就意味着可以将不同格式的数据发布到不同的topic中，减小消费这些数据时的逻辑难度。那么每个topic中处理的数据结构是怎样呢？我们先来看一张topic的解剖图:", "图1：topic原理解析图", "从图1中可以看到， 消息传送过来时kafka会通过负载均衡将消息最终写入到磁盘上一个特定分区（partition）。由于在同一个partition上这些消息都是顺序存储的， 所以对一个特定分区每条消息都会有一个基于起始位置的偏移量（offset）， 因此我们在后续消费时只需要指明从哪个partition中哪个offset开始消费，就能达到重复消费目的。", "1）虽然kafka可以通过增加partition方式来增加负载，但是它的数据最终是被写入到磁盘中。比如机械磁盘写入效率是很低的， 难道我们需要增大一个topic的负载给它设置更多的partition吗？", "机械磁盘驱动器吞吐量跟寻道延时是强关联，也就是说，线性读写速度远大于随机读写。例如，在67200rpm SATA RAID-5磁盘阵列中， 随机写速度大约是100k/s, 然而线性写速度可以达到600M/s，后者大约是前者的6000倍。通过图1可知， kafka采用的即是后者, 利用操作系统read-ahead和write-behind技术，极大提升磁盘访问性能；设置partition数量固然可以从磁盘读写角度增大topic负载，但是partition数量过多会导致cpu计算量增大，所以最佳办法是根据不同配置的机器， 不同的业务场景设置不同的partition数量。", "2）偏移量offset存储类型是什么， 如果消息足够大，offset的值是否会重新置0， 如果置0，后续消费是否会紊乱？", "kafka offset 是一个日志序列号（ log sequence number），不必担心offset 长度问题。那么这个日志序列号到底有多大，举个例子：如果一个partition一天接收1T日志， 这个offset至少可以使用1百万年。由于offset足够用，而且不会被置0，所以从这个角度讲消费紊乱情况是不会出现的。", "3）写入磁盘的日志会被永久保留吗？如果想删除过期消息， 需要怎么操作？", "可以通过配置文件中log.retention参数设置消息过期时间，超过过期时间的消息会被系统删除，删除的消息不可再被重新消费。", "通过前文介绍我们已经了解到kafka通过partition和顺序读写磁盘的方式达到很高吞吐量，可是单台机器吞吐量再高一旦该机发生故障宕掉就会对业务产生灾难性影响，怎么处理这个问题呢？想必你已经知道了，那就是采用集群的方式，一旦一台机器发生故障客户端可以选择链接其它机器， 保证业务稳定性。每一个partition 都会有一个服务器来作为领导者（leader）， 另外一个或者多个服务器（server）来作为跟随者（follower），leader会处理所有的读写请求，而follower则会从leader那里备份数据， 如果一个leader失败了， 其它的follower会自动选举一个成为一个新的leader， 所以对于一个server来说，他可能是某些partition下的leader， 而对于另外一些partition来说则是follower,这样设计可以将负载更好均衡。", "1）搭建kafka集群时有没有什么小细节需要值得注意的？", "已经有详细的搭建过程，在此不赘述。建议正式项目中不要采用伪集群（多个broker运行在同一台物理机上）的搭建方式，而且zookeeper集群和kafka集群最好不要出现在同一台实体机上，这样会影响kafka顺序读写效率。", "2）在kafka集群中如果一个server失败， 怎样保证数据完整性？", "在kafka配置文件中有一个复制因子控制参数，如果将该参数设置为N，则表示一份数据会被保存N次，而这些数据被备份到不同server中，所以当设置复制因子为N时即使有N-1台server失败，也会保证数据完整性。", "上面讲了那么多，无非是要实现一个队列的数据结构。对于队列这种数据结构我们一点也不陌生，由此可以想到对于kafka的一个topic 队列来说，生产消费逻辑应该是这样：有很多生产者向topic中写入数据，另外一端则有许多消费者消费数据。（见图2）", "图2：生产者消费者原理解析图", "然而实际上kafka生产者消费者模式有它的特殊性，那么kafka这个队列是怎样实现入队和出队的？接下来我们一起来看看kafka生产者消费者模式。", "生产者（producer）顾名思义，就是向kafka队列中发布消息的，即入队操作者。生产者功能是在topic中选择一个partion 然后向这个partition中发送数据。选择partition的过程就是一个负载均衡的方式， 比如可以采用轮询或者自己设定partition选择函数来实现负载均衡。当然如果使用封装的api比如（https://github.com/dpkp/kafka-python）就大可不必关心负载均衡问题。会有默认的负载均衡函数来实现这一功能。", " 消费者（consumer）功能是从队列中读取数据并进行相应逻辑处理，但是kafka消费者有特殊之处。kafka增加了一个组（group）的概念，一个topic可以有多个group， 当多个consumer从属于一个组时，一条消息将被发往所有组，但是在组内，这条消息只会被一个consumer消费。由此说来一个group才是一个真正“逻辑消费者（logic consumer）”。相关逻辑如图3所示。", "：通过图3我们知道消息的消费情况，那么一个消息流消费情况会是怎样的？其实在高等级api中由于指定了负载均衡规则，同一个生产者发布两条不同消息数据时会根据相应规则发送到一个特定partition中，在消费时会按照同样规则从partition中取出数据，这样就能保证两条数据消费的先后顺序，从而保证了消息顺序性。", "1）对于一个具有多个consumer的topic，我要实现一条消息被多个consumer消费和一条消息只被一个consumer消费，那我需要怎么设置group？", "将多个consumer设置为同一个组可以实现一条消息只被多个consumer消费， 将所有的consumer都设置为不同组，一条消息将会被所有consumer消费。", "2）如果有一批数据消费时必须严格按照入队先后顺序来消费，需要怎样设置生产者和消费者。", "如果数据量小，可以将topic设置为一个partition；如果数据量较大，可以将一个生产者写死负载均衡函数，将数据发送到一个特定partition上，消费数据时指定消费者消费的partition，和offset来顺序消费数据。", "图3：多个消费者组时消息流向原理图", "kafka是跨语言消息队列系统，github上提供了Java， Python等多种语言客户端，为了简单起见，我们这里采用kafka-python（https://github.com/dpkp/kafka-python）作为客户端来链接kafka集群做测试。", "测试环境：", "1， broker 数量：3", "\n2， 备份因子数：2", "\n3， 磁盘信息：200G普通机械硬盘", "\n4， cpu参数：8核8线程", "\n5， 语言： Python2.7", "\n6, 客户端： kafka-python", "\n7, partition 数量： 5", "单进程producer 发送10条消息测试（如图4）：", "图4：一个生产者发送消息延时结果图", "统计上图数据可知平均延时：0.004488707，也就是说qps可以达到2000，这样的成绩无疑是惊人的。那么在多进程情况下kafka表现还会好吗？我们设置10个进程，看看kafka在10个进程下的延时会有较大的变化吗？如图5（打印消息过多，截取部分结果图）：", "图5：多个生产者发送消息延时结果图（部分）", "由图5可知10 个进程每个进程发送10条消息，平均延时为0.00050380466秒, qps接近200000，由于kafka支持数千个客户端同时读写，所以kafka吞吐能力是惊人的，更多测试欢迎大家去完成。", "1，在垂直搜索中的应用：", "我们知道搜索引擎需要定时对文档进行更新， 如果我们把需要更新内容暂存到 kafka，这样索引更新时,只需要从对应 partition 中从上一次取过的 offset 处继续取数据，就能达到增量更新目的，而过期数据会被自动清理， 减少了操作冗余性和复杂性。", "2，在用户画像以及相关推荐中的应用：", "和用户画像之前上报的用户点击行为数据不同，相关推荐之前的海量 item 数据上报对数据准确性要求更高，试想如果一条 item 数据因为处理失败而没有正确入库，那么相关推荐时就永远不会出现这条 item， 所以这就对“可回滚”提出了更加严格要求。然而在 kafka 中，也只需要将消费的 offset 重新置为消费失败时的 offset，修复入库问题重新消费即可。", "当然 kafka 还有更加广泛的应用，这里就不一一讨论，根据官网的介绍，kafka 在网站行为追踪（Website Activity Tracking）、数据监控， 流处理等众多方面有特长，如果你对 kafka 原理有研究或者有实际应用方面有心得，欢迎来讨论，谢谢！", "达观数据专注于企业大数据技术服务，以独创的多层智能挖掘算法，实现对海量用户行为和文本数据的深入分析和挖掘，为企业提供智能文本分析、精准用户行为建模、个性化推荐、智能搜索等尖端数据挖掘功能。", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78301"]},
{"module": ["干货教程"], "note": ["本文涉及阿里云分布式调度团队在分布式调度系统的设计、实现、优化等方面的实践以及由此总结的分布式系统设计的一般性原则，具体包括分布式调度的任务调度、资源调度、容错机制、规模挑战、..."], "title": ["解析阿里云分布式调度系统伏羲"], "content": ["本文涉及阿里云分布式调度团队在分布式调度系统的设计、实现、优化等方面的实践以及由此总结的分布式系统设计的一般性原则，具体包括分布式调度的任务调度、资源调度、容错机制、规模挑战、安全与性能隔离以及未来发展方向六部分。", "云计算并不是无中生有的概念，它将普通的单台PC计算能力通过分布式调度软件连接起来。其最核心的问题是如何把一百台、一千台、一万台机器高效地组织起来，灵活进行任务调度和管理，从而可以像使用台式机一样使用云计算。在云计算中，最核心的模块是分布式调度，它好比云计算的中央处理器。目前，业界已存在多种分布式调度实现方案，如伏羲、Hadoop MapReduce、YARN、Mesos等系统。", "伏羲系统在前人的基础上进行了一系列改造，首先与YARN和Mesos系统类似，将资源的调度和任务调度分离，形成两层架构，使其具备以下优势：", "：两层架构易于横向扩展，资源管理和调度模块仅负责资源的整体分配，不负责具体任务调度，可以轻松扩展集群节点规模；", "：当某个任务运行失败不会影响其他任务的执行；同时资源调度失败也不影响任务调度；", "：不同的计算任务可以采用不同的参数配置和调度策略，同时支持资源抢占；", "：计算framework决定资源的生命周期，可以复用资源，提高资源交互效率。", "这套系统目前已经在阿里集团进行了大范围的应用，能支持单集群5000节点、并发运行10000作业、30分钟完成100T数据terasort，性能是Yahoo在Sort Benchmark的世界纪录的两倍。", "伏羲的系统架构如图1所示，整个集群包括一台Fuxi Master以及多台Tubo。其中Fuxi Master是集群的中控角色，负责资源的管理和调度；Tubo是每台机器上都有的一个Agent，负责管理本台机器上的用户进程；同时集群中还有一个叫Package Manager的角色，因为用户的可执行程序以及一些配置需要事先打成一个压缩包并上传到Package Manager上，Package Manager专门负责集群中包的分发。 ", "图1 伏羲的系统架构", "集群部署完后，用户通过Client端的工具向Fuxi Master提交计算任务；Fuxi Master接收到任务后首先通知某一个Tubo启动这个计算任务所对应的APP Master；APP Master启动之后，它获知了自己的计算任务，包括数据分布在哪里、有多少的任务需要计算等等信息；接着APP Master会向Fuxi Master提交资源申请，表明它需要多少计算资源；Fuxi Master经过资源调度以后，将资源的分配结果下发给APP Master；APP Master在这个资源的基础之上进行它的任务调度，来决定哪些机器上运行哪些计算任务，并且将这个计算任务发送给对应机器上的Tubo进程；Tubo接受到命令之后就会从Package Manager中下载对应的可执行程序并解压；然后启动用户的可执行程序，加载用户的配置（图1中的APP Worker）；APP Worker根据配置中的信息读取文件存储系统中的数据，然后进行计算并且将计算结果发往下一个APP Worker。其中，数据的切片称之为Instance或者叫计算实例。", "Fuxi Master与Tubo这套结构解决了分布式调度中的资源调度，每个计算任务的APP Master以及一组APP Worker组合起来解决任务调度的问题。", "伏羲在进行任务调度时，主要涉及两个角色：计算框架所需的APP Master以及若干个APP Worker。", "图2 伏羲在任务调度时涉及的主要角色", "APP Master首先向Fuxi Master申请/释放资源；拿到Fuxi Master分配的资源以后会调度相应的APP Worker到集群中的节点上，并分配Instance（数据切片）到APP Worker；APP Master同时还要负责APP Worker之间的数据传递以及最终汇总生成Job Status；同时为了达到容错效果，APP Master还要负责管理APP Worker的生命周期，例如当发生故障之后它要负责重启APP Worker。", "而APP Worker的职责相对比较简单，首先它需要接收App Master发来的Instance，并执行用户计算逻辑；其次它需要不断地向APP Master报告它的执行进度等运行状态；其最为主要的任务是负责读取输入数据，将计算结果写到输出文件；此处的Instance是指输入数据的切片。伏羲任务调度系统的技术要点主要包括数据的Locality、数据的Shuffle以及Instance重试和Backup Instance三点。", "数据Locality是指调度时要考虑数据的亲近性，也就是说APP Worker在处理数据时，尽量从本地的磁盘读取数据，输出也尽量写到本地磁盘，避免远程的读写。要实现这一目标，在任务调度时，尽量让Instance（数据分片）数据最多的节点上的AppWorker来处理该Instance。", "数据Shuffle指的是APP Worker之间的数据传递。在实际运行中，APP Worker之间是有多种传递形态的，如一对一、一对N、M对N等模式。如果用户去处理不同形态的传输模式，势必会带来较大的代价。伏羲分布式调度系统将数据传递的过程封装成streamline lib，用户无需关心数据传递的细节。首先Map进行运算，将结果直接交给streamline，streamline底层会根据不同的配置将数据传给下游计算任务的streamline；然后streamline将接到的数据交给上层的计算任务。", "在Instance的运行过程中可能有多种原因导致Instance失败，比如APP Worker进程重启或运行时机器、磁盘发生故障，种种原因都可能导致一个Instance在运行时最终失败；另外APP Master还会监控Instance的运行速度，如果发现Instance运行非常慢（容易造成长尾），会在另外的APP Worker上同时运行该Instance，也就是同时有两个APP Worker处理同一份数据，APP Master会选取最先结束的结果为最终结果。判断一个Instance运行缓慢的依据有：", "目前已完成的Instance比例，防止在整体任务运行初期发生误判。", "资源调度要考虑几个目标：一是集群资源利用率最大化；二是每个任务的资源等待时间最小化；三是能分组控制资源配额；四是能支持临时紧急任务。在飞天分布式系统中，Fuxi Master与Tubo两者配合完成资源调度。", "图3 飞天分布式系统中的资源调度", "在飞天分布式系统中，Fuxi Master与Tubo两者配合完成资源调度。Tubo是每个节点都有的，用于收集每个机器的硬件资源（CPU、Memory、Disk、Net），并发送给FuxiMaster；FuxiMaster是中控节点，负责整个集群的资源调度。当启动计算任务时，会生成APP Master，它根据自己的需要向Fuxi Master申请资源，当计算完成不再需要时，归还该资源。", "飞天分布式调度常用的分配资源策略包括优先级和抢占、公平调度、配额。在实际应用场景中，不同策略可配合起来使用。", "每个Job在提交时会带一个priority值（整数值），该值越小优先级越高；相同优先级按提交时间，先提交的优先级高；FuxiMaster在调度时，资源优先分配给高优先级的Job，剩余的资源继续分配给次高优先级Job。", "如果临时有高优先级的紧急任务加入，FuxiMaster会从当前正在运行的任务中，从最低优先级任务开始强制收回资源，以分配给紧急任务，此过程称为“抢占”。抢占递归进行，直到被抢任务优先级不高于紧急任务，也就是不能抢占比自己优先级高的任务。", "公平调度策略是指当有资源时Fuxi Master依次轮询地将部分资源分配给各个Job，它避免了较大Job抢占全部资源导致其他Job饿死现象发生。公平调度首先按优先级分组，同一优先级组内的平均分配，如果有剩余资源再去下一个优先级组进行分配，依此类推。", "配额是资源分配时的第三个策略，通常是按照不同的业务进行区分，多个任务组成一个组，例如淘宝、支付宝等；集群管理员会设立每一个组的资源上限，意味着这个组最多能使用这么多CPU、Memory、磁盘等，该上限值称为Quota；每个组的Job所分配的资源总和不会超过该组内的Quota，当然如果每一个组内没有用完的Quota是可以分享给其他组的，会按照Quota的比例进行均分。", "在大规模进程集群中故障是常态，这些常态会来自硬件，比如主板、电源、内存条；也可能来自软件，比如进程有Bug导致进程Crash，机器故障导致性能慢。因此，分布式调度必须具有容错机制，以保证正在运行的任务不受影响，并对用户透明，能够从故障中恢复过来，保障系统的高可用。下面将从任务调度的Failover和资源调度的Failover两个方面介绍。", "AppMaster进程重启后的任务调度Failover", "每个计算任务有自己的APP Master，如果APP Master进程发生了重启，那其重启之后的任务调度如何进行Failover呢？这里采用了Snapshot机制，它将Instance的运行进度保存下来，当APP Master重启之后会自动加载Snapshot以获取之前每个Instance的执行进度，然后继续运行Instance；当APP Master进程重启之后，从APP Worker汇报的状态中重建出之前的调度结果，继续运行Instance。", "[b]FuxiMaster进程重启后的资源调度Failover[/b]", "另一种情况是Fuxi Master发生了Failover。Fuxi Master Failover起来之后需要重建内部状态，该状态通常分为两种：一是Hard State，主要是之前提交的Application配置信息，如不同的Job配置参数等，它们来自于Fuxi Master写的Snapshot；另一类是Soft State，Fuxi Master会收集来自各个Tubo以及APP Master的信息重建出自己的状态，这些信息包括机器列表、每个APP Master的资源请求以及之前的资源分配结果。", "Fuxi Master进程重启之后的资源调度过程如图4所示，首先会从Checkpoint中读取出所有Job的配置信息；同时会收集所有的Tubo以及APP Master上报上来的关于资源分配的结果，如CPU多少、Memory多少等等。", "图4 Fuxi Master进程重启之后的资源调度过程", "规模挑战", "分布式系统设计主要目标之一就是横向扩展（scale-out），目前阿里云飞天在2013年时已支撑单个集群5000个节点、并发1万个任务。在做横向扩展设计时，需要注意两个要点：一是多线程异步；二是增量的资源调度。", "多线程异步", "多线程异步是编写分布式程序一个非常重要而且常用的技术手段。在网络通信模块中，每个APP Master都需要跟Fuxi Master进行资源通信，同时也需要跟多个Tubo进行通信以启动它们的APP Worker。APP Master处理网络通信的过程称之为RPC，RPC通信时必须采用线程池来处理。如图5中采用四个线程池来处理这些消息。由于Fuxi Master是一个中控节点，而Tubo的数量非常众多，如果将这些消息都在同一个线程池中处理，则Fuxi Master的消息有可能会被大量的Tubo消息阻塞（对头阻塞问题）。为了解决该问题，在伏羲系统当中设立了一个独立的线程池来处理Fuxi Master的消息；另外一个线程池来处理Tubo的消息，将线程池进行分开，也称之为泳道；独立的泳道能有效解决Fuxi Master的消息被对头阻塞的问题。", "图5 RPC通信时采用的四个线程池", "增量的资源调度", "伏羲解决规模问题的另一个技术点是增量。目前，伏羲采用增量的消息通信和资源调度，下面通过具体例子，来介绍伏羲所采用的增量资源调度的协议。", "图6 伏羲所采用的增量资源调度的协议示例", "图6左侧是中控节点Fuxi Master；右边为某一个APP Master，如果说APP Master需要1000份资源，最直接的一种实现方式是将“我要1000个资源”这样的消息直接发送给Fuxi Master；Fuxi Master在接到消息之后可能当前的剩余资源只有200份，它将会“我分配给你200”这样的消息发送给APP Master；那APP Master还会继续发送消息“我还要剩余的800”，Fuxi Master回复“此时没有资源，我分配0个给你”；则APP Master在下一次通信的时候需要继续发送“我还要剩余的800”……依此类推，可能某一个时刻Fuxi Master还能分一点资源下来。这就是最直观的全量消息通信，每一次APP Master提出请求时都要指明它总共需要多少。", "而在伏羲的实现当中为了减小通信量和不必要的开销，采用了增量的语义。首先APP Master发送一个请求“我要1000个资源”，Fuxi Master收到之后将当时空闲的200个资源返回给APP Master；之后APP Master无需再提交请求说我还需要800，因为Fuxi Master会将这1000个请求记录下来等到某一时刻又有更多的资源，比如150个资源释放，它直接将150个分配结果发送给APP Master即可。这期间APP Master无需再发多余的网络通信。", "在分布式系统当中通常有多个用户在执行自己的计算任务，多个任务之间需要互相隔离、互相不影响。飞天伏羲实现了全链路的访问控制，采用了两种访问控制进行安全的验证，一种是Capability，指通信双方基于私钥进行解密并验证的一种方式；还有一种称为Token的方式，这种方式需要通信的双方临时生成基于私钥加密的口令，在通信时进行验证。", "两种方式最大区别在于口令生成的时机，Capability方式是在通信之前就已经加密好；而Token是需要在通信时临时生成。", "\n", "图7 访问控制的两种安全验证方式", "两种方式使用于不同的场景，如图7所示FuxiMaster与Tubo通信采用的是Capability方式，因为这两个角色在集群部署时就已启动，可以事先进行加密生成好Capability；FuxiMaster与APP之间是采用Token的方式，这是因为APP与FuxiMaster进行通信时，当每个任务执行完计算之后会退出；在进程与进程之间，伏羲采用了沙箱的方式将不同的进程进行隔离开、互不干扰。", "除了安全的隔离之外，还需要考虑性能的隔离。目前伏羲采用的几种技术手段：Cgroup（Linux LXC）、Docker container、VM等。这几种技术的隔离性、资源配额/度量、移动性、安全性的比较如图8所示，不再一一叙述。", "图8 性能隔离的技术手段对比表", "伏羲目前采用的隔离技术是基于Docker和LXC混合部署的方式，之所以抛弃虚拟机的方式，是因为其性能损耗太多。当运行计算任务时，如果完全放在虚拟机当中，它的IO以及CPU时间片会受到很大的影响，会降低任务的执行效率。在目前阿里的生产环境中，实践发现基于Docker和LXC的隔离技术已经可以很好地满足需求。", "随着计算能力和数据量的持续增长，分布式调度未来可能朝向以下几个方向发展：", "在线服务与离线任务混跑。云计算最终的目的是降低IT成本，最大限度地利用单台PC的CPU处理能力，所以未来的趋势一定是在线服务与离线任务能够在同一物理集群上运行从而实现削峰填谷效果、最大化提高集群利用率。但是由于两种任务的特点不同，在线运用对于响应时间要求很高，而离线运用则对调度的吞吐率要求比较高，因此混跑会带来性能隔离与资源利用率之间的矛盾。", "实时计算的发展，Map Reduce是一个很伟大的框架，但其是为数据量一定的批处理而设计的。随着云计算越来越普及，很多计算形态需要实时拿到计算结果，并且其输入数据可能是不间断的。目前，伏羲也已经开发出了实时的计算框架——OnlineJob，它可以提供更快的执行速度。", "更大的规模，目前已能够支撑5000台的节点，随着计算量越来越大，客户的需求越来越多，需要进一步优化伏羲系统，能够支撑起1万、5万、10万等更大规模单集群，同时能够支撑更多的并发任务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78065"]},
{"module": ["干货教程"], "note": ["\nby 崔维福\n情感分析是学术领域研究多年的课题，用google学术搜索可以找到很多paper，基本的方法上有基于词典规则的方法、语言文法的方法，此外还有分类器以及近几年比较火的深度学习的方法(..."], "title": ["大数据舆情情感分析，如何提取情感并使用什么样的工具？（贴情感标签）"], "content": ["by 崔维福", "情感分析是学术领域研究多年的课题，用google学术搜索可以找到很多paper，基本的方法上有基于词典规则的方法、语言文法的方法，此外还有分类器以及近几年比较火的深度学习的方法(稍后有详细介绍)。", "各类paper是有一定的借鉴意义的，不过这主要是学术界在单个问题上的细化，要真正从研究领域落地到大数据的处理还有很多工作要做。", "工程上的处理流程具体包括以下几个方面：", "在进行情感分析任务的界定时，要弄清楚工程的需求到底是什么；要分析文本的哪个层面上的情感，比如篇章、段落、句子、短语、词等粒度；是不是要分析所有的文本还是分析其中的部分文本；准许的错误误差是在个什么范围内等。", "在实际的企业应用中往往要根据行业的特点来制定一些情感分析的标准，甚至要从客户的立场中去建立标准。根据国双实际接触客户的经验，在行业上建立标准后,还需要再具体跟客户做一些适度调整。", "有了上一步的工作， 接下来进行加工语料或者字典的总结。这一步中不同的方法要做的工作不同，基本上是铺人力的工作，难点是让各个语料加工人员能协调一致，执行统一的标准 (通常会在这个过程中还会反作用到第二步情感分析标准的制定，因为看到实际数据后会发现标准总会有一些模糊地带)", "工程中的方法并不是单一的方法，想用一个方法或者模型来解决各类数据源上的问题是不可能的。想要做出好的效果一定是采用分而治之的思想，比如，能用规则精准过的就不需要用分类器。", "当应用在实际产品时，最好能结合产品的垂直特点，充分利用垂直行业的特性，比如在金融行业、汽车行业，它们一定有自己的行话，这些行话具有非常明显的规则或者特征。", "情感分析对象的粒度最小是词汇，但是表达一个情感的最基本的单位则是句子，词汇虽然能描述情感的基本信息，但是单一的词汇缺少对象，缺少关联程度，并且不同的词汇组合在一起所得到的情感程度不同甚至情感倾向都相反。所以以句子为最基本的情感分析粒度是较为合理的。篇章或者段落的情感也可以通过句子的情感来计算。", "现阶段关于情感分析方法主要有两类：", "基于词典的方法主要通过制定一系列的情感词典和规则，对文本进行拆句、分析及匹配词典（一般有词性分析，句法依存分析），计算情感值，最后通过情感值来作为文本的情感倾向判断的依据。", "做法：", "基于词典的情感分析大致步骤如下：", "如果是对篇章或者段落级别的情感分析任务，按照具体的情况，可以以对每个句子进行单一情感分析并融合的形式进行，也可以先抽取情感主题句后进行句子情感分析，得到最终情感分析结果。", "1. 常见英文情感词库：GI（The General Inquirer）、sentiWordNet等；", "2. 常见中文情感词库：知网、台湾大学的情感极性词典；", "3. 几种情感词典构建方法：基于bootstrapping方法的Predicting the semantic orientation of adjectives及Determining the sentiment of opinions两种最为经典的词典构建方法。", "情感词典准确率高，但存在召回率比较低的情况。对于不同的领域，构建情感词典的难度是不一样的，精准构建成本较高。另外一种解决情感分析的思路是使用机器学习的方法，将情感分析作为一个有监督的分类问题。对于情感极性的判断，将目标情感分为三类：正、中、负。对训练文本进行人工标注，然后进行有监督的机器学习过程，并对测试数据用模型来预测结果。", "基于机器学习的情感分析思路是将情感分析作为一个分类问题来处理，具体的流程如下：", "文本的预处理过程是使用机器学习作用于文本分类的基础操作。由于文本是非结构化数据及其特殊性，计算机并不能直接理解，所以需要一系列的预处理操作后，转换为计算机可以处理的结构化数据。在实际分析中，文本更为复杂，书写规范也更为随意，且很有可能掺杂部分噪声数据。整体上来说，文本预处理模块包括去噪、特征提取、文本结构化表示等。", "中文最小语素是字，但是往往词语才具有更明确的语义信息，但是随着分词，可能出现词语关系丢失的情况。n-元文法正好解决了这个问题，它也是传统机器学习分类任务中最常用的方法。", "：对抽取出来的特征，向量化是一个很重要的过程，是实现由人可以理解的文本转换为计算机可以处理数据的重要一步。这一步最常用到的就是词袋模型（bag-of-words ）以及最近新出的连续分布词向量模型（word Embedding）。词袋模型长度为整个词表的长度，词语对应维度置为词频，文档的表示往往比较稀疏且维度较高。Embedding的表示方式，能够有效的解决数据稀疏且降维到固定维度，更好的表示语义信息。对于文档表示，词袋模型可以直接叠加，而Embedding的方法可以使用深度学习的方法，通过pooling得到最终表示。", "：在机器学习分类算法的使用过程中，特征好坏直接影响机器的准确率及召回率。选择有利于分类的特征，可以有效的减少训练开支及防止模型过拟合，尤其是数据量较大的情况下，这一部分工作的重要性更加明显。其选择方法为，将所有的训练语料输入，通过一定的方法，选择最有效的特征，主要的方法有卡方，信息熵，dp深层感知器等等。", "目前也有一些方法，从比句子粒度更细的层次去识别情感，如基于方面的情感分析（Aspect based Sentiment Analysis），他们从产品的评价属性等更细粒度的方面对评价主体进行情感倾向性分析。", "文本转换为机器可处理的结构后，接下来便要选择进行机器学习的分类算法。目前，使用率比较高的是深度学习（CNN，RNN）和支持向量机（SVM）。深度学习的方法，运算量大，准确率有一定的提高，所以都在做这方面的尝试。而支持向量机则是比较传统的方法，其准确率及数据处理能力也比较出色，很多人都在用它来做分类任务。", "1. svm分类 libsvm", "2. python 机器学习工具scikit-learn", "3. 深度学习框架：Tensorflow、Theano", "本文选自国双商业市场在知乎的回答。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78341"]},
{"module": ["干货教程"], "note": ["\n文 | 小红\n前言\n本篇内容来源于网络，因为工作需要，所以就去网上查找资料，顺便整理一下分享给大家，小红自己也是在学习阶段， 做这个公众号的目的也是为了输出自己学习的内容，一方面是为..."], "title": ["用户画像是什么鬼，你懂吗?"], "content": ["文 | 小红", "本篇内容来源于网络，因为工作需要，所以就去网上查找资料，顺便整理一下分享给大家，小红自己也是在学习阶段， 做这个公众号的目的也是为了输出自己学习的内容，一方面是为了自己更好的学习，另一方面希望能够帮助一些和我一样刚进入互联网行业的新人。", "本篇分为用户画像的概念、如何搭建用户画像以及用户画像的步骤三部分。", "本文框架", "用户画像，即用户信息标签化，就是企业通过收集与分析消费者社会属性、生活习惯、消费行为等主要信息的数据之后，描绘出一个真实用户的虚拟物。", "用户画像的焦点工作就是为用户打“标签”，而一个标签通常是人为规定的高度精炼的特征标识，如年龄、性别、地域、用户偏好等，最后将用户的所有标签综合来看，就可以勾勒出该用户的立体“画像”了。", "公司应搭建一个用户画像平台，将含有大量用户数据的数据平台和可视化数据工具平台连接起来，根据不同的用户交互场景，应用挖掘数据平台的价值，让研发生产，用户研究，市场营销等人员能够根据需要，随时自主地分析不同产品用户特征，快速洞察用户需求。该平台需要回答的核心问题是：用户是谁? 用户需求是什么? 用户在哪里?", "企业选择构建用户画像平台，可以实现不同的战略目的。", "1、完善产品运营：通过用户调研，进行产品迭代，制作用户喜欢的APP，提升用户体验。", "2、对外服务，提升盈利：根据产品特点，找到目标用户，根据用户的画像进行个性化运营，促成购买，实现精准运营和营销。", "对用户画像进行数据建模，结合客户实际的需求，找出相关的数据。完善的用户画像平台需要考虑周全的模型体系。通常来讲，构建用户画像平台所需的数据分成用户、商品、渠道三类实体。", "1、用户：数据维度包括自然特征、兴趣特征、社会特征、消费特征。从数据特点上看，又可分为基本属性和衍生标签，基本属性包括年龄、性别、地域、收入等客观事实数据，衍生标签属于基本属性为依据，通过模型规则生成的附加判断数据。", "2、商品：数据维度包括商品定位和商品属性。商品属性即商品的功能、颜色、能耗、价格等事实数据，商品定位即商品的风格和定位人群，需要和用户标签进行匹配。", "3、渠道：渠道分为信息渠道和购买渠道。用户在信息渠道上获得资讯，在购买渠道上进行商品采购。不同类型的用户对渠道有不同的偏好，精准的选择对应的渠道才能提高效率和收益。", "以用户、商品、渠道三类数据实体为中心，进行数据维度分解和列举。", "自然特征：性别，年龄，地域，教育水平，出生日期，职业，星座。", "兴趣特征：兴趣爱好，使用APP/网站,浏览/收藏内容，互动内容，品牌偏好，产品偏好。", "社会特征:婚姻状况，家庭情况，社交/信息渠道偏好。", "消费特征：收入状况,购买力水平，已购商品，购买渠道偏好，最后购买时间，购买频次。", "手机：品牌，颜色，尺寸，电池容量，内存，摄像头，CPU，材质，散热，价格区间。", "笔记本：品牌，屏幕尺寸，配置，颜色，风格，薄厚，价格区间。", "智能手表：品牌，功能，材质，电池容量，颜色，风格，价格区间。", "信息渠道：微信，微博，论坛，SNS，贴吧，新闻网站，咨询App。", "购买渠道：电商平台，微店，官网，实体店，卖场。", "针对不同角色人员的需求(如市场、销售、研发等)，设计各角色人员在用户画像工具中的使用功能和应用/操作流程。下面以两个应用场景为例，让大家明白如何利用用户画像。", "场景一，按需设计：改变原有的先设计、再销售的传统模式，在研发新产品前，先基于产品期望定位，在用户画像平台中分析该用户群体的偏好，有针对性的设计产品，从而改变原先新产品高失败率的窘境，增强销售表现。比如，某公司想研发一款智能手表，面向28-35岁的年轻男性，通过在平台中进行分析，发现材质=“金属”、风格=“硬朗”、颜色=“黑色”/”深灰色”、价格区间=“中等”的偏好比重最大，那么就给新产品的设计提供了非常客观有效的决策依据。", "场景二，精准营销：针对已有产品，寻找所偏好的精准人群分类，以及这些人群在信息渠道和购买渠道上的分布比例，来决定广告投放和活动开展的位置、内容等，实现精准营销。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77679"]},
{"module": ["干货教程"], "note": ["\n文 | blogchong\n近期部门在做人员招聘，所以一直在坚持看简历，包括也面了部分相关岗位的候选者，有些感触还是蛮大的。\n最想吐槽的一个点就是，混了好几年工作了，真的该好好学学怎么写一份..."], "title": ["作为大数据从业人员，如何写好一份可堪入目的简历?"], "content": ["文 | blogchong", "近期部门在做人员招聘，所以一直在坚持看简历，包括也面了部分相关岗位的候选者，有些感触还是蛮大的。", "最想吐槽的一个点就是，混了好几年工作了，真的该好好学学怎么写一份让人看得爽点的简历，不让自己难看，也不让面试官蛋疼。", "恰巧数据虫巢读者私密群中也有童鞋需要去找工作了，而且年后恰是换坑好时节，趁此机会，为这个话题开个单篇吧。", "我先不说专门如何去构造一份简历，单纯的讲述一下作为一个大数据相关岗位的面试官，我是如何去翻阅简历的。", "意味着你写上去的东西都将会面临考验，而如果你面试的是大数据相关的岗位，与大数据相关度不大的相关经历，就不要多费笔墨了。", "因为这些额外的项目经历并不会为你额外加分，只会影响面试官查看简历的心情，一笔带过就好了。", "如上所说，既然简历上的相关信息面试官都会关注，那么，什么东西该往上面放呢?", "最基本的要求就是，你能hold住面试官盘问的东西，每一行信息你都需要确认，这些东西我都很熟悉，面试官变着法儿问，我也能变着法儿回答。", "所以，简历上的东西并不是多多益善，而是在于在于精，并且在于经得住考验，这是简历书写的第一要点。", "我一般看大数据相关的简历，这几部分的信息肯定是会关注的：", "1 毕业学校以及学历，对应的专业或者研究方向。", "2 工作年限。", "3 工作经历，即跳槽史。", "4 项目经历。", "5 对于自己擅长东西的总结汇总。", "我一般不太关注的东西：", "1 自我评价部分，尤其是那种自我感觉良好的书写，更加负分。", "2 学校获奖或者各种发表论文的部分，讲真，国内论文的水有多深，我真的挺清楚的，当然，你要是能发表个国际公认论文水准的期刊，分分钟给你加分。", "3 其他诸如什么籍贯啊、居住地乱七八糟的基础信息，真的不关心(基础信息我只关注男女，学历相关的信息，工作年份)，也没必要过多写。", "1 除了常规以外的相关优点，当然，前提是你这个优点对应应聘岗位有特别明显的作用，不然就是显摆，负分滚粗。", "这点可以举个简单例子，如果招聘的爬虫工程师，应聘者说自己平时没事就逛逛微博、知乎，当然是用程序，那些地方就跟自己后花园似的，从那些地方获取的数据一坨又一坨，就只差拿出来卖了。", "这种自己爱折腾的爬虫工程师，单凭这条技能，分分钟可以给他的整体评分上扬个30%。", "针对上面这些情况，哪些该详细写，哪些该屏蔽应该差不多了吧。", "比如，对于我现在要招的数据挖掘工程师，我会着重关注其毕业学校以及最高学历，并且这项将会占据整体面试评分中不低的评分。", "而对于普通的大数据开发工程师或者爬虫工程师，这块要求相对就可以放低了，因为大学学的东西对后面所掌握的技能知识影响并没有想象中大。", "其次，对于公司经历相对较好的童鞋，公司背景也是小小的加分项，毕竟好公司还是有一定的背书能力的。", "那么，对于那些公司经历相对不是那么耀眼的童鞋，重心需要适当放在项目经验上，试图在这块弥补公司经历的不足。", "1 对口项目细写，把相关的技能都很好的展现出来，当然，前提是不管是项目也好，涉及的技能也好，需要经得起盘问。", "2 尽量的平实的描绘自己在其中的作用，不需要你一个人逆天，但需要你有体现价值的地方。", "3 项目经历不需要事事都写上，轻重缓急分清，对口的、参与程度高的适当体现，不重要的，跟你应聘岗位没啥卵关系的，一笔带过，这样会让面试官更容易找到重点，换言之，也可以把面试官的注意力圈定在你所擅长的地方，所以，别犯傻。", "4 项目经历一定要有以下几部分内容，项目目的、项目流程、时间周期、人员分工、自己的核心重点，所涉及的技术、架构、相关技能，能够把这些逻辑理清也是个技术活儿。", "对于项目这块，面试官最喜欢的是找漏子，包括我也是，因为我想看到项目中的一些不足之处，然后想看看你到底会如何处理。", "所以，在准备项目经历时，对于淌过的坑一定要梳理梳理，此外就是自己查找出相关的漏子，试图去举一反三。", "没有哪个面试官会喜欢不动脑子的候选者，只对做过的东西熟悉，对没有做过的东西连想法都没有的人是可怕的。", "除了项目经验这块，对于自己所擅长的东西，一定要做一个总结汇总，这是对自己的肯定，也是对面试的尊重，总不能让面试官去猜你擅长哪些技能吧。", "我一般除了重点会照顾项目经历之外，对于汇总处的技能点，也会挨个盘查，这种盘查是脱离项目之外的，单纯的底层知识积累的考核。", "这块的汇总同样的，没必要把你十八般武艺都放上去，记得放在前头的一定是对口的、并且是你所熟悉的。", "1 我招大数据相关岗位的，你把java相关技能放在前头的还能忍，把什么前端技能写一大坨，还放第一排的，直接负分滚粗。", "2 别是个技能都写上精通，你需要确认你是否真的精通，别写着精通的，我问一个你不懂一个，直接负分滚粗，但你要写着“了解”级别的，我会认为你不懂很正常。", "所以整个简历的书写，总结起来其实很简单，别一股脑儿平铺陈述，重要的是对口，分得清主次，最后，经得住考验。", "1 你不懂没关系，没有人什么都懂，懂该懂的东西就够了。", "2 但是，如果你说你懂，结果问你你又不懂，那就尴尬了。", "3 如果你真的懂，你要告诉我你懂，你不告诉我你懂，难道让我去猜，这就尴尬了。", "此外，不建议简历造假，在老司机面前很容易出漏子的，出了漏子更是大写的尴尬。", "当然，如果说你能hold主，你来你上，把我搞定了我就认为它是真的了(假能圆成这样，说明还是有本事的 哈哈，招了也不亏)!", "搞技术的把该说的说清楚完事，别净扯些没用的，这点特别适合刚毕业的应届生朋友。", "黄崇远，花名博客虫，“数据虫巢”个人品牌所有者（官网www.mite8.com、公众号、简书、博客），近6年的大数据行业经验，线下组织过大数据技术沙龙，线上授过大数据课，目前于深圳任职于一B轮创业公司大数据主管。", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77285"]},
{"module": ["干货教程"], "note": ["\n文 | 小红\n前言\n大数据这个概念在最近这几年很火，大家也大概知道大数据到底是个什么东西，它是如何运作的。现在好多产品上面都会有“猜你喜欢”这一功能，这就是利用大数据实现的。我们每天..."], "title": ["读书笔记 | 大数据时代"], "content": ["文 | 小红", "大数据这个概念在最近这几年很火，大家也大概知道大数据到底是个什么东西，它是如何运作的。现在好多产品上面都会有“猜你喜欢”这一功能，这就是利用大数据实现的。我们每天都在利用大数据或被大数据利用，但是我们当中应该没有多少人真正知道大数据时代给我们带来什么改变。这本书主要从大数据带来的思维变革、商业变革、管理变革三个方面来写。我主要会把这本书中的思维变革和商业变革写出来(因管理变革目前我们中大部分人还用不到，所以就先不写)，本篇写思维变革、商业变革下篇连载。", "本书框架图", "在信息处理能力受限的年代，世界需要数据分析，却缺少用来分析所收集数据的工具，所以只能用随机抽样的方式进行数据分析。", "但是真正的大数据时代是指不用随机分析法这样的捷径，而采用所有数据的分析方法。通过观察所有数据，来寻找异常值进行分析。", "比如：信用卡诈骗是通过异常情况来识别的，只有掌握了所有数据才能做到这一点，在这种情况下，异常值是最有用的信息，你可以把他与正常交易情况作对比从而发现问题。", "在如今的信息时代。我们掌握的数据库越来越全面，她不再只包括我们手头那一点可怜的数据，而是包括了与这些现象相关的大量甚至全部的数据。数据量的大幅增加会造成结果的不准确，与此同时，一些错误的数据也会混进数据库。但是正因为我们掌握了几乎所有的数据，所以我们不再担心某个数据点对整套分析的不利影响。我们要做的就是要接受这些纷繁的数据并从中受益，而不是以高昂的代价消除所有的不确定性。这就是由“小数据”到“大数据”的改变。", "有时候当我们掌握了大量新型数据时，精确性就不那么重要了，我们同样可以掌握食物的发展趋势，大数据不仅让我们不再期待准确性，也让我们无法实现准确性。", "值得注意的是，错误并不是大数据本身固有的。他只是我们用来衡量、记录和交流数据的工具的一个缺陷。如果说哪一天技术完美无缺了，不精确度的问题就不复存在了。错误不是大数据固有的特性，而是一个需要我们去处理的实际问题，并且可能长期存在。", "混杂性不是竭力避免，有的时候可以为我们所用。互联网最火的产品都会表明，不精确性、混杂性要更好点。", "比如微信朋友圈：朋友的发动态时间，在一小时之内的会显示多少分钟之前，在一小时以外的就只显示几小时前。", "在微信公众号阅读量显示，为什么超过十万以后显示地是100000+，而不是具体数据，因为超过十万以后的数据，我们心中或许就没啥概念了，没有一个参考衡量的标准了，十万已经会让我们觉得这篇文章很厉害了，能达到目的，就没必要精确。", "知道是很什么就够了，没必要知道为什么。在大数据时代，我们不必非得知道现象背后的原因，而是要让数据自己发声。", "比如：知道用户对什么感兴趣即可，没必要去研究用户为什么感兴趣。", "相关关系的核心是量化两个数据值之间的数据关系。相关关系强是指当一个数据值增加时，其他数据值很有可能也会随之增加。", "相关关系是通过识别关联物来帮助我们分析某一现象，而不是揭示其内部的运作。", "注意：即使很强的相关关系也不一定能揭示每一种情况，比如两个事物看上去行为相似，很有可能只是巧合。相关关系没有绝对，只有相似。", "通过给我们找到一个现象良好的关联物，相关关系可以帮助我们捕捉现在和预测未来。", "比如：如果A和B经常一起发生，我们只需要注意到B发生了，就可以预测A也发生了。", "在小数据时代，数据分析专家会使用一些建立在理论基础上的假想来指导自己选择适当的关联物。然后收集与关联物相关的数据来进行分析，以证明假设是否正确。但是由于这是建立在假设的基础上，那么分析结果也是有受偏见影响的可能。", "在大数据时代，我们拥有如此多的数据，如此好的计算机能力，所以不再需要人工选择一个关联物或者一小部分相似数据来逐一分析。通过去探求“是什么”而不是“为什么”，相关关系帮助我们更好的了解这个世界。", "首先我们需要明确两个概念就是数字化和数据化", "数据化、是指一种把现象转变为可制表分析的量化形式的过程。", "数字化、是指把模拟数据转换成0和1表示的二进制码。", "计算机的出现带来了数字测量和存储设备，数字化带来了数据化，但是数字化无法取代数据化。", "数据化的核心是量化一切，常见的被量化的有文字、方位和沟通。", "当文字变成图书，拿电子书为例，未数据化的电子书只能够被展示出来，读者并不能通过搜索关键词被查找到，也不能被分析。", "当方位变成数据，就是将地理信息进行，比如百度地图、各种网站的获取位置都是将方位变成数据。", "当沟通变成数据，一些社交平台通过添加各种心情表情，来收集我们的心情状态，还有人们的喜好，年龄什么的都可以变成数据。", "不同于物质性的东西，数据的价值不会随着它的使用而减少。数据就像一个神奇的砖石矿，当他的首要价值被发掘后仍能不断给予。它的真实价值就像漂浮在海洋中的冰山，第一眼只能看到冰山的一角，而绝大部分隐藏在表面之下。他可以为了同一目标被多次使用，也用于其他目的。这就需要我们选择性的对数据进行创新，下面主要介绍几点数据创新", "就是数据在实现了基本用途以后的进一步利用。", "比如搜索关键词，基本用途是可以通过消费搜索关键词来定向推送广告，就是我们在淘宝里面搜索关键词以后，会收到猜你喜欢的物品提醒。", "而他的再利用：根据客户搜索关键词的流量，来判断哪款产品或哪种颜色会成为爆款。", "有的时候可能从某一组数据上看不出什么价值，我们需要把他和其他数据进行组合以后，才能利用其价值。", "比如，美国房地产网站Zillow.com将房地产信息和价格添加在美国社区地图上，同时还压缩了大量的信息，如社区近期的交易和物业价格，以此来预测区域内具体每套住宅的价值。", "促成数据再利用的方法之一是从一开始就设计它的可扩展性。收集多个数据流或每个数据流中更多数据点的额外成本往往较低，因此，收集尽可能多的数据并在一开始的时候就考虑到其各种潜在的二次用途，使其具有扩展性是非常有意义的。", "比如：超市的摄像头在一开始的时候只是为了防止小偷，但事实上还可以跟踪商店的客户流和她们停留的位置。可以根据这些信息来设计店面的最佳布局。", "随着时间的推移，可能一些比较久远的数据就会失去其原有的价值，在这种情况下，继续依赖于旧的数据不仅不能增加价值，实际上还会破坏新数据的价值。", "比如，十年前你在亚马逊上买了一本书，而现在你已经完全对他不感兴趣了，如果亚马逊继续使用这个数据来向你推荐其他书籍就会有些不合理。", "就是收集数据中的一些错误值来进行利用。", "比如：搜索引擎的输入法，有的时候你会发现你输入的关键词时错误的，但是系统会弹出你想要的正确的结果。这就是数据废气所起的作用。搜素引擎后台会收集每天后台收到的错误关键词和用户最终查找的正确关键词的内容。这样以后一旦出现类似的错误，系统就可以推送正确的内容给用户，匹配度很高。", "根据所提供价值的不同来源，分别出现了三种大数据公司。这三种来源是指：数据本身、技能与思维。", "第一种是基于数据本身的公司。这些公司拥有大量数据或至少可以收集到大量数据，却不一定有从数据中提取价值或用数据催生创新思想的技能。", "第二种是基于技能的公司。他们通常是咨询公司、技术创新或分析公司。他们掌握了专业技能但并不一定拥有数据或提出数据创新性用途的才能。", "第三种是基于思维的公司。通过利用大数据思维提出一些创新性指导意见。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77671"]},
{"module": ["干货教程"], "note": ["\n导读：斯诺登事件造成了两个特别值得注意的结果：一是围绕美欧安全港协议的未来进行持续讨论;二是欧盟和其他地区对数据驻留实施更严格要求的发展趋势，俄罗斯和澳大利亚等国家纷纷颁布新的..."], "title": ["全球数据驻留风险管理"], "content": ["导读：斯诺登事件造成了两个特别值得注意的结果：一是围绕美欧安全港协议的未来进行持续讨论;二是欧盟和其他地区对数据驻留实施更严格要求的发展趋势，俄罗斯和澳大利亚等国家纷纷颁布新的数据驻留法律法规。", " ", "2013年夏天，美国国家安全局前雇员爱德华·斯诺登(EdwardSnowden)大爆猛料，加剧了关于跨境获取和转移机密数据的法律、政治、道德和商业争论。", "这些争论对国家数据隐私法律产生了重大影响，促使政策制定者们开始评估国际数据转移的现有法律保护是否完善。斯诺登事件造成了两个特别值得注意的结果：一是围绕美欧安全港协议的未来进行持续讨论;二是欧盟和其他地区对数据驻留实施更严格要求的发展趋势，俄罗斯和澳大利亚等国家纷纷颁布新的数据驻留法律法规。", "目前，个人数据的跨境转移很少(甚至于没有)受到监管机构、媒体和个体公民的监督。在这种背景下，Fieldfisher隐私与信息法律团队(在隐私从业者的帮助下)对全球重要国家的数据驻留规管现状进行了评估和分析。", "我们希望借此阐明这个受到热烈讨论但很少被透彻分析的话题。我们列出了报告的编制方法，并回答了以下五个重要问题：(1)什么是数据驻留规管?(2)有多少国家实行了数据驻留规管?(3)跨境转移个人数据的最常见法律依据是什么?(4)有约束力的企业规管(BCRs)是否为满足数据驻留规管和跨境转移个人数据的公认途径?(5)数据驻留规管的执法风险有哪些?", "2014年底，Fieldfisher分析了6个地区47个国家关于数据驻留和数据转移的国家法律法规，包括：欧洲(28个欧盟成员国外加冰岛、列支敦士登、挪威和瑞士);北美(美国和加拿大);南美(阿根廷、巴西和乌拉圭);亚洲(中国、印度、以色列、日本、马来西亚、新加坡和韩国);非洲(南非);大洋洲(澳大利亚和新西兰)。", "2015年初，我们多方参照、交叉比对了我们的分析，想要勾勒出真正的数据驻留规管全球图景。这份报告是正是这项工作的结晶。我们计划在未来几年继续更新这份报告以确保其准确性，并扩大报告覆盖的地理范围。", "“数据驻留”这个词语在大多数国家法律中都没有得到明确定义。但从广义上讲，数据驻留规管是指在不符合某些法律标准的情况下，禁止组织机构向本国境外或本地区以外转移个人数据的国家法律。", "欧盟的《数据保护条令》(Data Protection Directive)是数据驻留规管的最著名例子。该条令禁止组织机构向欧盟以外的接收者转移个人数据，除非对个人数据的保护达到“适当”的水平，比如签署“标准合同条款”(Standard Contractual Clauses)，进行美欧安全港协议自我认证，或者在开展集团内部数据转移时执行BCRs。其他例子包括加拿大、澳大利亚、以色列和韩国，它们都有类似的数据驻留规管。", "在我们分析的47个国家中，有44个施行了数据驻留规管。这说明，遵守这些要求是全球议题，对企业提出了重大挑战，尤其是对数据密集型企业而言，比如企业和个人云服务提供商、在线社交媒体公司、大型生命科学和医疗集团。", "在被分析的国家中，只有美国、南非和新西兰还没有实施数据驻留规管。美国提出了行业限制，但没有制定出全局性的转移限制规定。南非虽然制定了有关隐私权的宪法和行业法律，但还没有落实具体的数据隐私法律。2013年11月26日，南非政府把《个人信息保护法案第四版》(POPIA)作为国家数据隐私法律。然而，南非政府还没有确定POPIA的生效日期。", "新西兰的1993年《隐私法》并不禁止个人数据的跨境转移。不过，新西兰法律委员会建议引入有关个人信息跨境披露和外包的正式问责规定。南非和新西兰的事态发展似乎预示着两国将引入数据驻留规管，向其他国家看齐。", "换句话说，预计这些国家(以及更多的国家)将在可预见的未来实施数据驻留规管。", "在我们分析的47个国家中，关于个人数据转移的法律依据非常广泛。最常见的法律依据如下：", "值得指出的是，很多法律依据按照被转移的个人数据类型及其预期用途/目的而有所不同。例如，在俄罗斯，个人数据的跨境转移通常需要数据主体的书面同意。因此，在进行个人数据跨境转移时，应该始终把当地法律要求纳入考量。", "可以把BCRs看作是企业采用的内部数据治理政策框架。在这个框架下，企业承诺按照某些规定的数据隐私标准，保护其收集和处理的数据。", "BCRs通过合同、政策和纪律措施对员工产生内部约束力，同时通过集团内部协议或类似法律机制对集团公司产生外部约束力。集团公司作出的政策承诺必须通过适当的培训、供应商管理、投诉处理和审计过程加以落实。", "BCRs的概念最初由欧盟的第29条工作组(Article 29 Working Party)提出，目的是允许跨国公司、国际机构和企业集团按照欧盟数据驻留要求，进行组织内部的个人数据跨境转移。BCRs必须经过欧盟数据保护有关当局的审查和批准，这意味着那些成功达成BCRs的公司已经向监管机构证明，公司上下严格遵守了数据保护法规。", "虽然BCRs被视为从欧盟国家向非欧盟国家输出数据的合法途径，但这里存在一个疑问：其他非欧盟国家根据自己的数据驻留规管，是否接受和允许使用BCRs来输出个人数据?例如，这种接受要么通过明确的立法措辞批准BCRs的使用，要么“隐晦”承认BCRs符合当地数据驻留标准(即使当地法律没有明确指出这一点)，从而被当地监管机构所容许。", "在我们分析的47个国家中，有42个国家明确或隐晦承认BCRs是组织内部进行个人数据跨境转移的合法途径。对BCRs的这种广泛承认表明，BCRs为跨国企业在遵守国家数据驻留规管的前提下进行数据跨境转移提供了普适的共同标准。尽管源于欧盟，但BCRs显然已经在国际舞台上获得了更加广泛的承认和吸引力。", "在我们分析的47个国家中，有42个国家的数据保护监管机构拥有对数据驻留违规行为实施处罚的权力。", "在被分析的国家中，只有美国、印度、以色列、南非和新西兰的监管机构目前无法处罚数据驻留违规行为。在印度，监管机构没有处罚的权力。在以色列，数据库注册局的执法权力有点模糊不清，而且到目前为止，还没有相关的案例来检验其执行数据驻留规管的能力。上文已经讨论了南非和新西兰的例子。", "目前，对数据驻留规管的监管执法非常有限(除了俄罗斯)。不过，由于数据跨境转移的监管敏感度上升，个人数据的跨境转移很可能将受到数据保护有关当局更加严格的审查。", "或许更加值得注意的是，还没有适当解决方案来开展数据跨境转移的企业会发现，在提出数据驻留要求的地区达成交易和做生意将变得越来越困难。很多美国企业已经遇到了这种情况。在欧盟废除美欧安全港协议的背景下，美国企业越来越多地面临欧盟客户坚决要求采用其他的数据转移解决方案的情况，比如标准合同条款和BCRs。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77665"]},
{"module": ["干货教程"], "note": ["\n作者|李滔\n美团外卖经过3年的飞速发展，品类已经从单一的外卖扩展到了美食、夜宵、鲜花、商超等多个品类。用户群体也从早期的学生为主扩展到学生、白领、社区以及商旅，甚至包括在KTV等娱乐..."], "title": ["高频/场景驱动下 美团外卖O2O的用户画像实践"], "content": ["作者|李滔", "美团外卖经过3年的飞速发展，品类已经从单一的外卖扩展到了美食、夜宵、鲜花、商超等多个品类。用户群体也从早期的学生为主扩展到学生、白领、社区以及商旅，甚至包括在KTV等娱乐场所消费的人群。随着供给和消费人群的多样化，如何在供给和用户之间做一个对接，就是用户画像的一个基础工作。所谓千人千面，画像需要刻画不同人群的消费习惯和消费偏好。", "这意味很多用户对外卖的认知较少，对平台上的新品类缺乏了解，对自身的需求也没有充分意识。平台需要去发现用户的消费意愿，以便对用户的消费进行引导。", "外卖是个典型的高频O2O应用。一方面消费频次高，用户生命周期相对好判定；另一方面消费单价较低，用户决策时间短、随意性大。", "：场景是特定的时间、地点和人物的组合下的特定的消费意图。不同的时间、地点，不同类型的用户的消费意图会有差异。例如白领在写字楼中午的订单一般是工作餐，通常在营养、品质上有一定的要求，且单价不能太高；而到了周末晚上的订单大多是夜宵，追求口味且价格弹性较大。场景辨识越细致，越能了解用户的消费意图，运营效果就越好。", "，结合地理位置判断用户的消费意图是外卖的一个特点。", "如下图所示，我们大致可以把一个产品的运营分为用户获取和用户拓展两个阶段。在用户获取阶段，用户因为自然原因或一些营销事件（例如广告、社交媒体传播）产生对外卖的注意，进而产生了兴趣，并在合适的时机下完成首购，从而成为外卖新客。在这一阶段，运营的重点是提高效率，通过一些个性化的营销和广告手段，吸引到真正有潜在需求的用户，并刺激其转化。在用户完成转化后，接下来的运营重点是拓展用户价值。这里有两个问题：第一是提升用户价值，具体而言就是提升用户的单均价和消费频次，从而提升用户的LTV（life-time value)。基本手段包括交叉销售（新品类的推荐）、向上销售（优质高价供给的推荐）以及重复购买（优惠、红包刺激重复下单以及优质供给的推荐带来下单频次的提升）；第二个问题是用户的留存，通过提升用户总体体验以及在用户有流失倾向时通过促销和优惠将用户留在外卖平台。", "用户体验之旅", "所以用户所处的体验阶段不同，运营的侧重点也需要有所不同。而用户画像作为运营的支撑技术，需要提供相应的用户刻画以满足运营需求。根据上图的营销链条，从支撑运营的角度，除去提供常规的用户基础属性（例如年龄、性别、职业、婚育状况等）以及用户偏好之外，还需要考虑这么几个问题：", "后面“外卖O2O的用户画像实践”一节中，我们会介绍针对这三个问题的一些实践。", "下图是我们画像服务的架构：数据源包括基础日志、商家数据和订单数据。数据完成处理后存放在一系列主题表中，再导入kv存储，给下游业务端提供在线服务。同时我们会对整个业务流程实施监控。主要分为两部分，第一部分是对数据处理流程的监控，利用用内部自研的数据治理平台，监控每天各主题表产生的时间、数据量以及数据分布是否有异常。第二部分是对服务的监控。目前画像系统支持的下游服务包括：广告、排序、运营等系统。", "画像系统架构", "新客运营主要需要回答下列三个问题：", "1）新客在哪里？", "2）新客的偏好如何？", "3）新客的消费力如何？", "回答这三个问题是比较困难的，因为相对于老客而言，新客的行为记录非常少或者几乎没有。这就需要我们通过一些技术手段作出推断。例如：新客的潜在转化概率，受到新客的人口属性（职业、年龄等）、所处地域（需求的因素）、周围人群（同样反映需求）以及是否有充足供给等因素的影响；而对于新客的偏好和消费力，从新客在到店场景下的消费行为可以做出推测。另外用户的工作和居住地点也能反映他的消费能力。", "对新客的预测大量依赖他在到店场景下的行为，而用户的到店行为对于外卖是比较稀疏的，大多数的用户是在少数几个类别上有过一些消费行为。这就意味着我们需要考虑选择什么样的统计量描述：是消费单价，总消费价格，消费品类等等。然后通过大量的试验来验证特征的显著性。另外由于数据比较稀疏，需要考虑合适的平滑处理。", "我们在做高潜新客挖掘时，融入了多方特征，通过特征的组合最终作出一个效果比较好的预测模型。我们能够找到一些高转化率的用户，其转化率比普通用户高若干倍。通过对高潜用户有针对性的营销，可以极大提高营销效率。", "新客来了之后，接下来需要把他留在这个平台上，尽量延长生命周期。营销领域关于用户留存的两个基本观点是（引自菲利普.科特勒 《营销管理》）：", "用户流失的原因通常包括：竞对的吸引；体验问题；需求变化。我们借助机器学习的方法，构建用户的描述特征，并借助这些特征来预测用户未来流失的概率。这里有两种做法: 第一种是预测用户未来若干天是否会下单这一事件发生的概率。这是典型的概率回归问题，可以选择逻辑回归、决策树等算法拟合给定观测下事件发生的概率；第二种是借助于生存模型，例如COX-PH模型，做流失的风险预测。下图左边是概率回归的模型，用户未来T天内是否有下单做为类别标记y，然后估计在观察到特征X的情况下y的后验概率P(y|X)。右边是用COX模型的例子，我们会根据用户在未来T天是否下单给样本一个类别，即观测时长记为T。假设用户的下单的距今时长t<T，将t作为生存时长t’；否则将生存时长t’记为T。这样一个样本由三部分构成：样本的类别(flag)，生存时长(t’)以及特征列表。通过生存模型虽然无法显式得到P(t’|X)的概率，但其协变量部分实际反映了用户流失的风险大小。", "生存模型中，βTx反映了用户流失的风险，同时也和用户下次订单的时间间隔成正相关。下面的箱线图中，横轴为βTx，纵轴为用户下单时间的间隔。", "\nCOX MODEL", "我们做了COX模型和概率回归模型的对比。在预测用户XX天内是否会下单上面，两者有相近的性能。", "美团外卖通过使用了用户流失预警模型，显著降低了用户留存的运营成本。", "拓展用户的体验，最重要的一点是要理解用户下单的场景。了解用户的订餐场景有助于基于场景的用户运营。对于场景运营而言，通常需要经过如下三个步骤：", "场景", "场景可以从时间、地点、订单三个维度描述。比如说工作日的下午茶，周末的家庭聚餐，夜里在家点夜宵等等。其中重要的一点是用户订单地址的分析。通过区分用户的订单地址是写字楼、学校或是社区，再结合订单时间、订单内容，可以对用户的下单场景做到大致的了解。", "地址处理", "上图是我们订单地址分析的流程。根据订单系统中的用户订单地址文本，基于自然语言处理技术对地址文本分析，可以得到地址的主干名称（指去掉了楼宇、门牌号的地址主干部分）和地址的类型（写字楼、住宅小区等）。在此基础上通过一些地图数据辅助从而判断出最终的地址类型。", "另外我们还做了合并订单的识别，即识别一个订单是一个人下单还是拼单。把拼单信息、地址分析以及时间结合在一起，我们可以预测用户的消费场景，进而基于场景做交叉销售和向上销售。", "外卖的营销特征，跟其他行业的主要区别在于：", "外卖是一个高频的业务。由于用户的消费频次高，用户生命周期的特征体现较显著。运营可以基于用户所处生命周期的阶段制定营销目标，例如用户完成首购后的频次提升、成熟用户的价值提升、衰退用户的挽留以及流失用户的召回等。因此用户的生命周期是一个基础画像，配合用户基本属性、偏好、消费能力、流失预测等其他画像，通过精准的产品推荐或者价格策略实现运营目标。", "用户的消费受到时间、地点等场景因素驱动。因此需要对用户在不同的时间、地点下消费行为的差异做深入了解，归纳不同场景下用户需求的差异，针对场景制定相应的营销策略，提升用户活跃度。", "另外由于外卖是一个新鲜的事物，在用户对一些新品类和新产品缺乏认知的情况下，需要通过技术手段识别用户的潜在需求，进行精准营销。例如哪些用户可能会对小龙虾、鲜花、蛋糕这样的相对低频、高价值的产品产生购买。可以采用的技术手段包括用户分群、对已产生消费的用户做look-alike扩展、迁移学习等。", "同时我们在制作外卖的用户画像时还面临如下挑战：", "。需要用到自然语言处理技术，同时结合其他数据进行分析。", "因此需要和用户在其他场合的消费行为做融合。", "不像传统电商用户在决策前有大量的浏览行为可以用于捕捉用户单次的需求。因此更需要结合用户画像分析用户的历史兴趣、以及用户的消费场景，在消费前对用户做适当的引导、推荐。", "面临这些挑战，需要用户画像团队更细致的数据处理、融合多方数据源，同时发展出新的方法论，才能更好地支持外卖业务发展的需要。而外卖的上述挑战，又分别和一些垂直领域电商类似，经验上存在可以相互借鉴之处。因此，外卖的用户画像的实践和经验累积，必将对整个电商领域的大数据应用作出新的贡献！", "本文来自美团点评技术团队", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77851"]},
{"module": ["干货教程"], "note": ["\n一、什么是动态定价策略\n本文所指的动态定价策略仅仅限定在共享经济中的讨论，其他类型中的暂不涵盖。\n在 Uber 提出并使用动态定价策略后，经过了这段时间的市场验证，我们再回过头来看看动..."], "title": ["10分钟读懂Uber的动态定价策略"], "content": ["本文所指的动态定价策略仅仅限定在共享经济中的讨论，其他类型中的暂不涵盖。", "在 Uber 提出并使用动态定价策略后，经过了这段时间的市场验证，我们再回过头来看看动态定价策略的提出，发展，利弊以及应用。", "动态定价策略并没有一个严格的定义，但是他提现了一个经济学中的核心概念就是：供需平衡。", "因此我给动态定价策略的一个简单定义是：在一定的市场环境中，供需双方为达到平衡点而做出的价格调整。", "动态定价并不是新概念，加上了算法，智能，大数据等一堆词以后，显得有些惊艳了而已，动态定价在我们日常生活中的使用非常广泛，而且影响着每个人。", "举一个很简单的例子，过年的时候，蔬菜普遍贵了，原因就是供应少了，所以蔬菜的价格上升，只是在互联网情况下，这种变动会更加快，更加敏捷。", "任何一个方法或者技术的产生，都是为了解决某一个问题，尤其是我们在做产品的时候，遇到一个问题，要思考解决这个问题的解决办法，提出一二三，从中选取最优的方案。", "从 Uber 的角度要解决的问题是在高峰或者异常天气的情况下，Uber 的司机少，乘客打不到车的问题，为了解决这个问题，我们可以沙盘推演一下，Uber 能想到的解决办法：", "第一种情况是完全自由市场，没有任何宏观调控的时候的样子，我们认为 Uber 是一家理性的公司，显然 Uber 不会这么做", "第二种情况 Uber 几乎也不会采取，因为从 Uber 的一开始就定位于共享经济，C2C，所以一旦踏入自己购买就将是一个 B2C 和 C2C 混合的经济体，这是 Uber 不愿意考虑的方向。", "第三种情况与第二种情况也类似，都会加重公司的负担并且与初始定位违背，所以也被否决。", "这是一个很神奇的词，通过某种方法，可以让原来同样的东西产生更大的生产力，我们此处可以对第四种情况再细分成以下几个选项：", "当遇上高峰或打不到车的时候，通知打车人现在没车，请选择其他交通工具或继续等待", "当遇上高峰或打不到车的时候，通知司机师傅，现在某某地有很多人要搭车，赶紧过去拉人，去我就奖励你，不去我就惩罚你", "当遇上高峰或打不到车的时候，通知打车人现在没车，你可以加点钱，“或许”就有人来拉你了", "第一点并没有解决这个问题，虽然可以作为一种决绝方案，但只是陈述了一个事实，而且对自己用户并没有负责人，所以基本可以不采取。", "第二点相信是很多产品的选择，毕竟在威逼利诱下还是会有用户选择去做这件事，但是这里也会产生两个问题：", "如果是惩罚将会产生较差的体验和叛逆，如果是补贴奖励则会加大企业的支出，也就是开启烧钱模式的大招。", "Uber 完全有理由可以选择这一方案，但是我们看到了 Uber 选择了第三种方案，打不到车的用户请多出点钱，也就是动态定价的基本原型。", "至于这种方案是否是合理的，又有什么利弊，我们在第四点里面讨论。", "综上我们基本确定了选择动态变动价格的策略是基本可行的，正如我们前面所说的，这种策略由来已久，从经济学的角度我们来分析一下他的可行性。", "首先经济学十大原理之一：人们对激励做出反应，这应该是动态定价策略的理论基础。", "我们在一个简单的模型下面，我们来看一下整个的变动：", "黄线：表示接单人(供给)，也就司机师傅，价格越高，接单人越多", "绿线：表示发单人(需求)，也就是打车人，价格越低，打车人越多", "现在我们做以下的假设：", "1. 假设从 M1 地到 M2 地点的正常价格为 40 元，在这种情况下，刚好达到A的均衡点，所有的人都可以打到车，所有的车也刚好都拉到人", "2. 现在假设遇到了糟糕的天气，司机师傅变少了，黄色线上移变动为黄色的虚线，打车人并没有变，于是产生了：价格上涨，部分打车人打到了车，而部分打车人因为价格上涨而退出了打车的行列，这时达到了 A1 的平衡点", "3. 现在情况更复杂一些，假设遇到节假日，打车的人突然增多，而司机并没有增加，这时绿线上移，因为加价而让更多的司机加入进来，从而达到了新平衡点 A2", "4. 现在分析最复杂的情况，2，3 点同时产生，即司机减少，而打车人增加，这时两条虚线相交点，便是新的平衡点。", "即价格的上涨促使产生了两个结果：", "这是符合经济学的核心原理：供需。", "在此基础上，当我们再加入平台补贴，抽佣以及其他一些奖励的制度的时候，曲线将变得更加复杂，并且会产生无效的损耗，但是目的总是在调节供需双方达到一个合理的平衡点。", "在有了理论支持以后，我们再回过头来看看这种策略的利弊。", "基于动态定价的策略，我们来分析一下几个典型的案例和实用场景：", "目前很火的共享单车们，跟共享经济还是有很大的差异性，目前来看，多数共享单车是自己购买车辆，部分几乎很少的车辆为C端用户共享的车辆，所以这类完全是重资产的 B2C 企业，与我们讨论的内容不符，只是借用了一个共享的冠名而已。", "如人人快递，校内达和达达等，这类供需双方都是普通的C端用户，这一类的共享经济体都可以采取动态定价的策略，以校内达为例，先以一个合理定价在 3 元左右的客单价，然后基于单个学校的小范围内供需关系调整价格，从而也实现双方的平衡。", "如猪八戒，码市等，这类目前多采取的是竞价模式，其实在竞价模式的基础上，也是可以引入动态定价策略，但是这类平台考量的因素要比较多，比如团队的经验，能力等这些可能在价格方面占据的比重会更加大。", "动态定价策略是技术对经济学的体现，也是基于实际问题思考而得出的解决方案，基于这一个动态的圆形，可以不断加入完善策略，使它更加具有引导性，也使得市场更加合理。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77565"]},
{"module": ["干货教程"], "note": ["\n本文主要介绍微博商业数据挖掘的体系及方法，但并不注重模型和算法这些细节，而是阐述数据如何贴近、支持和引导业务，如何建立合理的评价体系，以及如何围绕这两点建设数据挖掘架构。\n业务..."], "title": ["微博商业数据挖掘方法"], "content": ["本文主要介绍微博商业数据挖掘的体系及方法，但并不注重模型和算法这些细节，而是阐述数据如何贴近、支持和引导业务，如何建立合理的评价体系，以及如何围绕这两点建设数据挖掘架构。", "微博广告生态的复杂程度在业界数一数二。由于微博本身的开放性，微博广告客户天生就有如下多样性：", "不同客户对微博广告产品这项营销工具的理解和应用程度相去甚远，有一部分客户已经能熟练使用不同的自助广告产品，设置不同的创意模板，撰写有针对性的创意来达到不同的营销目的，甚至经常使用时间和空间上的组合营销形式，这些客户通常效果较好，粘性也很强；但也有一部分客户还停留在传统联盟广告的时代，投放方式比较单一，对创意的生成欠缺足够思考，效果也不尽人意。客户梯度共同构成了微博广告生态，最直接的后果就是——优秀的广告与毫无吸引力的广告并存。", "由于微博的强账号属性以及由此带来的用户画像挖掘方面的潜力，客户对广告定向工具的要求非常精细。主要包括如下几类定向条件：", "微博推出了多种计算广告产品来满足多样化需求，并且还在持续迭代和改进。每一种广告产品专门抽象一大类投放需求，有不同的广告模板、计费方式、定向条件、投放平台以及专业人员配备。这是近两年微博商业化顺畅进行的主因。", "广告投放业务对数据的需求主要是流量细分及描述反馈，因此微博商业数据挖掘体系也是以流量细分，即通常说的以用户画像为核心来建设。周边辅助的数据挖掘模块主要包括：", "在长期业务实践中，我们最终将用户画像体系分为如下3个部分：", "用户数据的计算有一套完整的高复用低耦合的数据模块体系来支撑，最终成形的数据挖掘架构如图4所示。", "微博商业数据挖掘工作第一大重点是评价体系的建设。据我们了解，这是很多数据挖掘部门忽视的地方。我们建立了一个四层级的评价体系：", "1.效果级：挖掘的结果可以直接用线上广告投放效果提升来评价。这是最强的一级评价。", "示例：目前为止，只有兴趣挖掘能够使用这一级评价。", "2.Ground Truth级：Ground Truth有一个规模足够的数据集来当作标注集和交叉验证的测试集，可以使用监督学习算法来做分类。这个Ground Truth数据集被当作最终可信的评估标准，也用于交叉验证。", "示例：用户性别。微博所有用户都有自己填写的性别属性，但并非100%可信。但微博有很多实名认证的用户，这部分用户的性别是可信的，因此我们以这些用户作为标注，来修正那些没有实名认证的用户性别数据。", "3.Case级：不具备统计意义的标准数据集，即无法获得标注数据，但对于分类的结果，少部分能够通过人工到微博用户的页面上去判断是否准确。这种情况只能通过规则来挖掘。", "示例：常住城市。挖掘用户常住城市只能使用用户的IBS信息及IP地址，其余的特征对这个标签的贡献度都极其有限，因此只能使用规则来判定，然后对规则分类的结果抽样后，人工去用户微博页面上检验。只有大约5%的用户能够通过微博页面（博文、照片等信息）来人工判断他的常住城市。", "4.Logic级：当以上三个级别的评价条件都不具备，只能评价逻辑完备性。即挖掘规则逻辑是否是当前情况下最合理的。", "示例：差旅状态。用户当前位置不在常驻城市即判定为差旅状态，不做任何验证（但不做任何验证的情况极少，通常Case级和Logic级的评价很难完全分开，通常是偏Case或是偏Logic，总要同时看逻辑完备性和Case检验）。", "对于评价有如下原则：", "这是评价体系建设的重点，意味着不仅只有算法和模型工作可以不断迭代，评价方法本身也可以迭代。这项工作的重要性可能比模型的研发还要重要，如果大部分工作的评价只能停留在Case级甚至Logic级的话，整个数据挖掘体系很快就会无以为继，变得没有意义，因为这种工作的迭代余地很小，且没有方向。", "我们花大量的时间解决Ground Truth数据，方法一般有两种：", "除了兴趣标签外，能直接用效果来评价的数据并不多，而且业务层面的假设太多，我们在实践中仅用来参考。例如性别数据，对某些已知的强性别选择的广告行业（例如美妆），可以通过线上效果来间接判定数据准确率，但这种不够直接的方法很少采用，因为中间因素太多，自洽性不强。", "在这个评价体系下，数据工程师并不对兴趣标签之外挖掘结果的广告效果负责。如果用户使用了这些定向工具有好的效果，那很好，如果效果不好，数据工程师是不会就这个标签本身来进行效果优化的，因为这根本不是评价方向，这类标签在业务中的位置不处于效果的反馈环上。工程师只对兴趣标签做效果优化。", "除兴趣标签之外的数据挖掘流程如图6。", "兴趣挖掘并没有Ground Truth可以验证，因为兴趣本身就是一个非客观、难以界定的描述。在微博商业体系内，兴趣是如下定义的：", "在前一种情况下，兴趣标签是一个可预估的最优化问题，是CTR/CVR预估体系的一部分，可以做出不同粒度的兴趣标签来，而且往往不止一套。如果有N种计算广告产品，每种广告产品可以有M种预定义的转化行为，线上的兴趣标签理论上最多可以有N x M套。标签数据的评价方式直接用线上效果评价，可以持续迭代。", "在后一种情况下，兴趣标签只是一个解释性问题，在评价体系里处于最底层，实际上无法迭代。但这种兴趣标签的存在是必要的，因为并不是所有的应用场景都是广告投放，而且用特定产品的广告数据训练出的模型会比较偏，但某些场景（比如DMP的流量透视功能）需要一套不直接服务于投放效果、能完整描述用户群体的标签。因此我们根据关注和互动关系用简单统计的方法生成一版通用的兴趣标签。它只要求可解释性，所以规则越简单越好。一般禁止使用层次分析法，因为它对任何一层的评价都没有帮助。", "内容兴趣标签提供给除应用家之外的广告产品做定向工具。内容兴趣的做法如下：", "App兴趣标签是为应用家产品专门建立的。这项工作能够比较完整地表现微博商业数据挖掘中解决问题的思路。", "App兴趣标签是应用家CVR预估体系的一部分。CVR预估体系被建设成一个漏斗式的，特征的粒度从粗到细。App兴趣标签是用户-App类别粒度的，模型中较多使用交叉特征，这一层的计算结果被包装成定向工具给客户使用；中间层的粒度是用户-App，作为一个隐式定向存在；最后一层则是线上的CVR预估模型，特征粒度是用户-广告-上下文，计算结果直接参与Rank。", "在做CVR预估之前有两个数据问题。首先，应用家的功能支持广告客户指定效果目标行为：下载（推动没有安装这个App的用户下载）和唤醒（推动安装了这个App的用户重新进入该App成为当天日活）。因此至少需要知道每个用户是否安装了这些App，才能比较精准地投放。", "另一个问题是，要做CVR预估就必须获取下载数据作为训练标注。但微博无法跟踪从广告点击跳转出去的用户后续行为（尤其是iOS环境下）。", "这两个问题的解决方法如出一辙，都是先去找数据，找不到的部分再预估。预估的结果可以结合交叉验证，直接线上评价。", "应用家数据挖掘体系如图8。", "情景挖掘来源于一系列客户需求。在业务沟通中，经常接到客户类似如下的需求：", "这些需求看似零乱，实际上都属于不同于“兴趣”的另一类问题，它需要知道“用户是什么人”。因此我们建设了情景挖掘体系来整合响应这类需求的工作。", "最早建立情景引擎是为了满足某些DSP给大客户做SCRM的需求。客户需要运营社交网络上的粉丝和潜在客户，需要一些工具把消息分发给这些用户，比如：", "针对这类需求我们实现了一个情景引擎，接入微博上所有主要用户行为数据，按行为类别（谓语行为）分类存储，抽取出其中的对象（宾语个体），一个情景就定义为谓语+宾语，经过一系列中间计算后，形成“用户-情景列表”索引格式的数据，实时更新到线上缓存供定向服务使用。", "情景引擎用Storm接入实时数据，计算后分钟级别更新到线上缓存，大部分是工程问题。里面涉及到算法的地方主要有两处：", "数据清洗。接入的线上数据有垃圾流量，比如在话题区刷广告的。需要建一个反垃圾模块。", "关系扩展。计算出来的情景-用户列表通常会有极强的长尾分布，即头部的情景占据海量用户，但我们在广告投放时希望大部分情景都能有相当数量的覆盖用户。因此会丢弃掉大部分长尾数据，对分布的中间部分做基于相似性或相关性的算法扩充。", "基于情景引擎长期积累的数据，我们在上层建立了中长期情景标签体系（对外称为人生状态标签）。", "人生状态标签体系一共有20多个标签，涵盖用户的求学、旅行、车房、职业、婚恋、育儿等状态。这些标签都是各自独立挖掘，挖掘的算法完全由其评价方法而定，并没有通用方法，而评价方法完全取决于源数据情况。下面给出几个例子：", "根据发微博的内容过滤出一个准确率比较高的大学生用户集合（大学生在某些场景下发的微博会带有区分度非常高的关键词）。然后对16-25岁之间的用户建模，特征主要包括关注特征、App使用特征，IBS特征。用过滤的用户集合为正样本，随机取一个负样本集合进行训练。对所有16-25岁之间但不在样本集合中的用户进行预测，取一个预定的数量。", "当前用户的位置与用户常驻城市不符，即看做用户在差旅状态。", "根据用户行业/头衔、影响力、社交关系等信息制定过滤策略。到用户微博页上人工验证。", "用预估的方法会有一个问题，即很难保证做出来的正样本训练集是无偏的。一般来说，能够满足某种过滤条件的数据总是有偏的，通常更偏向于更好更活跃的用户。但在后期评估中发现，只要注意在模型里尽量不使用规则里的那些特征，关系并不大。另外，训练集偏向更好的用户也不算大问题，因为计算结果本来就要求优先保证更好的用户，那些不活跃的、特征缺失严重的用户对业务的影响相对不重要一些。", "人生状态标签跟兴趣标签看上去有类似的地方，但从评价方式和应用出发点来看完全不同。例如，“用户对婴儿用品感兴趣”跟“用户是婴儿父母”是两回事。从广告投放的角度出发，我们从来不把这两者混为一谈，我们对前者的效果负责，但不对后者的效果负责。", "另外，我们认为人生状态标签这样的挖掘工作并非未来的方向，而是代表着一种传统广告业的思路。过多地依赖这种人能阅读和理解的，但却高度离散化的因素并非计算广告的思维方式。但这不意味着这样的工作没有意义，在新媒体广告领域，它在相当长的时期内都是必须存在的。", "在长期实践中，我们总结出数据挖掘工作中最重要的两点是：紧贴业务，确定评价。不能做到这两点的数据挖掘团队通常会工作得比较困难，做很多无用功。", "紧贴业务意味着数据团队要从业务KPI中拆分出自己能贡献的一部分，这一部分能直接评价就不要间接评价，因此问题又回到评价上，这是数据工作的核心。", "评价体系的建设是一项容易被忽视的重要工作，它包括评价方法和流程的建立和迭代，评价数据的获取和制作。其中数据获取必须要长期进行，现在业界数据合作及打通已经变成一种趋势，大家能够通过合作来获取自己缺乏的数据，只靠自己的数据很难把工作做完整。", "微博在产品创新和商业化的道路上已经走了很久，试错和踩坑都不计其数，在利用自身优势基础上的内外部积累也开展得比较早，因此在数据挖掘领域足够接地气，足够开放，数据工作自身才能做得非常活，同时支持和引导广告业务的发展。", "。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/78715"]},
{"module": ["干货教程"], "note": ["\n文 | 数加大数据\n春节假期结束了，大家在与家人亲戚欢聚的同时，不知道有没有留意春节期间的空气质量呢?没留意也没关系，下面就由我带大家一起回顾下春节期间的空气质量变化吧。\n大屏体验地..."], "title": ["空气大数据带你回顾春节前后全国空气质量变化"], "content": ["文 | 数加大数据", "春节假期结束了，大家在与家人亲戚欢聚的同时，不知道有没有留意春节期间的空气质量呢?没留意也没关系，下面就由我带大家一起回顾下春节期间的空气质量变化吧。", "提醒下大家1月27日是春节哦，通过时序播放我们可以很明显的看到春节前后空气质量的全国分布变化，我就不在这里分析具体原因了，我这里仅仅是抛个砖。", "那么如何制作这样一块时序大屏呢?接下来为大家揭秘下制作过程。", "数据是可视化的原材料，正所谓巧妇难为无米之炊，我们首先要获取春节期间全国的空气质量数据。", "这里推荐", "，感兴趣的同学可以去上面下载数据。", "这里我下载了2017年1月1日至2017年2月2日的全国1497个监测点的数据。", "我们打开china_sites_20170101.csv看下数据文件内容：", "由上图可以看出这里横坐标是各个监测站点编号，纵坐标是一天24个时间段对应的各种空气质量指标，也就是说我们可以通过每个csv文件获取一天内每个小时的各个描述空气质量指标的值。", "这里只有监测站点的编号，没有监测站点的信息，比如我们关心的监测站点的位置信息，不要着急，上面那个网站也为我们提供了每个监测站点的经纬度信息和站点名信息", "。", "我们下载该文件打开看一下：(注意图中有条数据没有经纬度信息，这里需要补全或者过滤掉)", "这样我们已经下载好所需要的数据(稍后讲如何对数据进行处理)。", "相信大家在看天气预报时经常看到类似下面的图(这里截取中国气象网的一张图)。", "这张图就是根据地面上若干位置的气象观测站的数据利用空间插值的方法制作出来的一张等温面图。", "看下百度百科的空间插值解释：", "也就是说根据这些已知的监测站点监测出的数据去推算其他任意空间位置的数据，再根据值处在的不同区间范围去映射对应的颜色就可以得到上面这样的一张图。", "一张图读懂什么是空间插值(下面的图是使用datav制作的)：", "可以很清楚的看到空间插值就是根据离散的已知点去插值出连续的面数据的这样一个过程。", "DataV提供了一个轻分析的等值面地图组件来帮助大家来将已知的矢量点数据制作成栅格区域图，这里我们可以利用其来实时插值出全国的空气质量图，可以应对一些气象等行业的可视化需求。", "想要看到一段时间的空气质量随时序的变化，时间轴组件自然必不可少。", "时间轴控件有个重要特性就是支持回调ID,利用好这个回调ID,我们可以跟其他组件进行联动，这样当时间轴的时间变化的同时可以触发其他组件的数据更新。", "当你填写了正确的回调ID,他会在每次时间变化的时候重新触发一次数据请求，并自动在其他组件对应的API接口的参数列表加上当前回调ID及其对应的值。", "初始接口地址：http://127.0.0.1:8888/aqi", "回调触发后：http://127.0.0.1:8888/aqi?date=2017012722(这里回调ID填写的是date,2017012722对应的时间轴控件的时间序列当前时间段的date字段的值)", "同样回调ID也可以对SQL语句生效，不过这里需要使用:加上回调ID名称作为占位。", "初始SQL: select :date as value;", "回调触发后：select ‘2017022722’ as value;", "前面我们已经有了数据利器，这里我们需要打磨一下数据使其更符合我们使用。", "首先分析下我们需要什么样的数据，我们看下我们的等值面组件需要的数据格式：", "：即研究区域的边界数据，这里是全国区域，是一个geojson的格式的数据(关于geojson是一种地理交换格式，感兴趣的可以看看geojson标准)。", "：这里可以看到示例数据是一个包含经度、纬度、值的一个数组，对应我们的需求就是监测站点的经纬度和监测站点对应的某个指标的值。", "首先我们如果仅做一天的某个时段的这样一张等值面图，例如2017年1月20日的中午12点的关于AQI指标的等值面图，我们需要知道当天这个时段的每个监测站点的位置也就是经纬度信息和对应的AQI的值。", "相信到这里大家应该知道要怎么去处理这些数据了。", "写一段node脚本处理下上面下载的全国监测站点经纬度信息的csv文件。", "得到监测站点编号为key，站点信息为value的字典。", "截取一段看看：", "接下来我们再来处理下2017年1月20日的全国1497个监测点数据，也就是china_sites_20170120.csv这个文件。", "再来一段脚本处理下这个csv，这个csv包含了当天24个小时每个监测站点各个空气质量指标的信息，我们将这些信息提取出来，并根据前面获取的站点列表经纬度映射表给站点加上经纬度信息。", "这里将每天的时间段作为key，每个时间段所对应的所有监测站点的AQI信息和位置等信息的数组作为对应的值。", "看下格式示例：", "这样我们就可以方便的获取该天每个时间段的数据给等值面组件使用。", "根据前面介绍的时间轴的特性，我们如果想要时间轴变化的同时使得等值面的数据也发生变化，那么我们需要一个接口或者数据库能根据时间参数来获取不同时间段的全国各个监测站点的数据。", "这里我们可以写一个简单的接口来完成这样的一个需求。", "如下：", "：/aqi", "：GET", "：", "：date", "：string, 示例2017012722，时间格式为YYYYmmDDHH", "这里需要提前处理好上面下载的所有数据,node提供了一个glob模块可以方便的对文件夹下数据进行批量处理。", "这样就批量提取出了每天的数据。", "我们再利用glob模块对数据进行一次整合，将文件名也就是日期作为key，对应内容作为值。", "这样我们就得到了一个all.json这样一个整合后的文件。", "下面再利用node的express框架初始化一个express项目，然后按照上面的接口需求增加一个简单的接口：", "另外提醒下为了避免跨域请求的问题，需要使用cors模块，在app.js文件中增加cors模块。", "这样接口已经完成，npm start一下，测试下接口访问：", "我们已经成功处理好了数据，并写了对应的接口，接下来就是我们使用datav来制作这样一块大屏。", "，添加2D平面地图组件，删掉默认的子图层(保留底图层)，然后添加等值面子组件，将地图组件大小拖拽至合适的大小，设置地图的中心点和缩放等级", "，调整至合适的位置和大小", "我们研究的时间区域选择2017年1月22日到2017年2月2日的每天的22点。", "参照时间轴数据面板的示例数据，我们造出我们需要的数据。", "其中name事件或者时间节点名称也就是时间轴的轴点显示的内容，value是对应的时间，date是作为上面介绍的回调ID选项使用。", "将上面数据替换时间轴数据面板的静态数据，再参照下图配置(主要是回调ID和数据格式选项的填写)，我们再来看看此时的时间轴长什么样。", "这里的数据格式按照上面的数据填写%Y%m%d%H,回调ID填写date。", "由于示例的数据区域也是全国范围，这里的裁剪面的数据可以不动，有需要的同学可以根据自己需要修改这里的裁剪面数据。", "我们主要看下这个矢量插值点的数据配置：", "由于前面已经写好了对应的api，也已经测试了一下数据获取，我们修改等值面组件的插值点的数据源类型成API，然后填写前面接口测试的那个地址(这里测试http://127.0.0.1:8888/aqi?date=2017012722)。", "点击查看数据响应结果，可以看到已经得到了正确的响应结果并匹配成功。", "，由于需要轮播全国范围的数据，为了保证计算效率，我们需要适当将精度调低，也就是像元大小调大一些，这样可以保证快速得到插值结果。", "如图调整像元大小为3，分类数目为35类，颜色保持默认色使用线性渲染方式。", " ", "顺便介绍下SQL语句中回调参数的用法。", "这里连接一个postgresql数据库，然后我们修改数据源类型为数据库，选择配置好的数据库，如果没有可以点击新建按钮新建一个数据库连接，这里就不再赘述。 相关sql语句(:date在实际浏览时会传入回调ID对应的值)：", "select to_char(to_timestamp(:date,’YYYYMMDDHH24′),’YYYY年mm月DD日HH24时’)||’空气质量’ as value;", "，我们就大功告成了。", "我们再预览一下看看效果：", "这里简单利用datav的两个组件——时间轴和等值面组件制作了这样一个春节期间的空气质量回顾大屏，可以清晰的展示中国最大的节日—春节期间的空气质量。看上去是不是很酷炫，不如自己动手做一个吧!", "本文由 数加大数据 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据，任何不经同意的转载均为侵权。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76852"]},
{"module": ["干货教程"], "note": ["\n文 | 于晓松\n所有伟大的产品，都离不开用户的追随与期待。\n用户群体画像是产品用户增长的利器之一——它能够帮您探究产品指标数字背后的原因。\n通过用户群体画像，我们已经能够持续的监测产品..."], "title": ["用户群体画像功能深度解析"], "content": ["文 | 于晓松", "所有伟大的产品，都离不开用户的追随与期待。", "用户群体画像是产品用户增长的利器之一——它能够帮您探究产品指标数字背后的原因。", "通过用户群体画像，我们已经能够持续的监测产品运营状况，比如：观察产品关键指标的变化、关注用户到目标的转化趋势、分析用户的留存回访……", "除此之外，我们还可以观察到产品的每位用户，对单个用户的特征和行为进行最为细致的分析。", "但是，我们的目标是改进提升产品以实现用户增长。而数据指标并不足够直接指导产品的改进——因为，在数据指标和用户增长之间，会有很多坑，让产品和运营的改进变得步履维艰、让用户的增长变得缓慢。究其原因，是由于产品的数字指标过于宏观，而用户增长的构成是非常微观的：我们的用户是一个一个(1 by 1)被获取、激活和留存下来的。", "因此，我们需要一些有效的工具，帮我们在产品改进过程中尽早找到方向，让产品改进不再是盲目的过程。用户群体画像可以帮助我们：", "1.分析某个指标数字背后的用户，具备哪些特征——他们的人群属性、他们的行为特点?", "2.找到一些有趣的、有价值的事实，并从中发现产品有效改进提升的机会或方向。", "用户群体画像包括以下内容：", "用户价值和流失风险分析高价值用户的占比越高，高流失风险的用户占比越低，产品越健康。", "用户特点分析分析用户的性别、年龄、自定义属性、兴趣标签等。", "用户的使用环境分析分析用户的地域、渠道(来源)、应用版本、设备品牌(浏览器)等。", "用户的使用时间分析用户的首次访问时间、最后访问事件、最近30天访问时长、最近30天访问次数等。", "用户的行为特点分析用户在最近一个月内的行为分布。", "用户群体画像应如何使用?群体画像除了帮助我们查看产品用户的特点，更重要的作用是发现产品问题的背后的原因。", "因此，您可以参考下面的步骤使用群体画像：", "在开始之前，确立产品的目标，弄清当前最亟待解决的问题是至为重要的事。", "比如，一个电商类产品已经确立其目标是提升销售额。接来下，就要分析当前最主要的问题是什么?是新用户的增长不够多，还是老用户的重复购买率太低?这些问题，可以很方便的通过诸葛io分析得到。", "弄清目标和主要问题后，下一步是要找出和问题最直接相关的数据指标。", "比如，如果当前的问题是用户的重复购买率低，那么还进一步分析：用户在第一次购买多久之后的购买率会有显著的降低?哪些人群的重复购买率明显的低于或高于全部人群的平均值?", "总结起来就是：要尽可能精准的定位问题的点(时间、人群、渠道……)。", "找到较为精确的问题点及相关指标后，可以围绕这些指标做背后人群的画像分析，看能不能找到潜在的原因。", "比如，分析重复购买率明显高于均值的用户的群体画像，将其人群属性、行为特点与其他用户做对比，找到不同点，分析这些不同点与重复购买率之间的关系(需要的话，可以直接或间接联系少量的用户以做验证)。", "通过探索，您可能会发现一些可能的原因。", "比如，您可能会发现，某项功能的使用不便、或者某个地区用户习惯的不同是造成问题的可能的原因。", "在上一步，您已经分析出了一项或几项可能影响用户增长的原因。接下来，您需要做的是从可能性以及改进成本等方面评估，并对产品或运营做出改进。", "比如，改进易用性差的功能，或针对有问题地区的用户增加引导。", "改进后，对问题指标及问题相关人群进行持续的观测，验证是否达到了预期的效果。如果达到了预期的效果，则继续按照上面的步骤分析新的问题并加以解决。如果未达到预期的效果，也可以继续按照上面的步骤继续分析问题的原因，或者放弃转向其他问题。", "总结正所谓“集腋成裘、聚沙成塔”，用户增长是一件积少成多的事儿。诸葛io的用户群体画像提供了一架坚实的桥梁，可以帮助您和您的产品更加顺利的跨越鸿沟，尽早实现用户的快速增长!", "作者于晓松，诸葛io产品VP。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76989"]},
{"module": ["干货教程"], "note": ["\n文 | 秦路\n今天先聊一聊用户运营中，有关活跃的基础话题。\n对一家互联网公司来说，如果没有设置单独的数据运营岗，那么用户运营是和数据最贴近，也必须是最了解用户的。\n用户运营核心的方法..."], "title": ["一篇文章读懂活跃数据"], "content": ["文 | 秦路", "今天先聊一聊用户运营中，有关活跃的基础话题。", "对一家互联网公司来说，如果没有设置单独的数据运营岗，那么用户运营是和数据最贴近，也必须是最了解用户的。", "拉新可以作为渠道推广单独讨论，而促活和留存则相辅相成。", "非运营岗，或者其他类型的运营，通常只会注重一个活跃数据的果，而不会注意活跃数据的因。我们在这里就抽丝剥茧，教大家比较快速地了解活跃体系。", "互联网公司对活跃用户的定义大同小异，主要以用户打开APP一次记为一个活跃用户。", "按此基础可以在时间维度引申出周活跃用户，月活跃用户。即在一个自然周内打开一次APP，则本周为周活跃用户。月活跃用户同理。", "我们假设有一款新产品，这是它四个月内的活跃数据。嗯，看来不错。", "产品专注的市场领域不同，活跃用户数天差地别。一款小众的垂直领域产品和泛社交类产品，单纯看活跃用户数，你很难界定它们好坏。", "我们设定一个新指标，活跃率：某一时间段内活跃用户在总用户量的占比。", "按照时间维度引申，有日活跃率DAU，周活跃率WAU，月活跃率等MAU。", "例：月活跃，本月活跃用户在截止月末的总注册用户中占比。", "一般而言：活跃用户数，看的是产品的市场体量。活跃率，看的是产品的健康度。", "实际得承认，不同产品，用户需求(高频或低频)不同，活跃率也有差异。用户运营更多的职责是监控活跃率的变化，并且提升它。", "看，我们的活跃用户数上升，活跃率下降，这对新产品来说很正常。你不能要求每一个用户都使用我们产品不是?", "别急，我还没补刀呢。", "我们统计了注册用户数，那么我们也可以统计出本月新增用户数，很简单，两个月相减。", "是不是看出来什么了?", "要知道，按照活跃的定义，新注册的用户肯定是打开APP的用户，他也一定是活跃的用户。", "所以，我们拿每月的注册总用户数减去新增用户数，计算老用户数。并且将新老用户的活跃率独立出来。", "指标拆分后，我们发现老用户的活跃率比预期低。", "A产品打算在五月份做大投入，在APP上进行活动，希望用户大力参与，同时在B渠道进行推广。在常规的统计指标中，发现活跃数据上升。事后分析发现活跃为新增活跃，老用户的活跃数据没有显著上升。配合其他活动数据，证实活动效果较差。", "C产品获得投资后，通过大规模的烧钱推广，获得一个正向的活跃数据反馈。此时活跃有不小可能是由新增用户撑起的。产品自身的打磨若不好，老用户活跃率不会提高，这也是我们常说的留存概念。导致钱白白浪费不少。", "产品进入稳定期后，有了一定用户规模，新增活跃一般对活跃数据就不会有大的影响了。", "那么以新老用户区分活跃统计就够了?我们简单定义三个场景：", "用户包含各种类型，反应了不同群体的特征和想法。在使用整个产品的周期中，我们应定义更全面的指标：", "：有一段时间没有再打开产品，那么我们就视为流失用户，根据产品的属性，可以按30天，60天，90天等划分。", "：有一段时间没有打开产品，为了和流失区分开来，需要选择无交集的时间范围。比如流失用户是60天以上没打开产品，那么不活跃则是0～60天没打开。", "：有一段时间没用产品，之后突然回来再次使用，则称为回流用户。回流用户是活跃用户，且是由流失用户或不活跃用户唤回而来。", "：一段时间内打开过产品。", "：也可以叫超级活跃用户，长期持续使用产品，比如连续四周，或者一个月内15天等。", "现在我们发现，不论是活跃用户还是不活跃用户的维度，都一下子丰富了起来。", "通俗的理解一下用户活跃的变化", "上文ABC的三位用户活跃路径为：", "A：新增—活跃—忠诚", "B：新增-不活跃-回流-活跃-忠诚", "C：新增-不活跃-流失", "回到一开始那款产品的数据，我们将分解后的新指标统计出来。(定义忠诚用户一个月内有15天活跃;流失用户为两个月没打开过)", "(以上数据以月末当天的统计为准)", "你看，指标开始变得复杂了。产品有长期使用的忠实用户，也有流失用户。有用户回来继续使用，也有用户不怎么爱用产品。", "数据是为了方便讲解随手编的。实际的情况可能会更复杂，可以根据情况灵活应对。", "用户活跃可以简化为一个最简单的公式：新增用户的数量要大于流失用户的增加量。可以想成一个水池，运营会一直往里灌水，但是水池也会漏水，如果漏水速度太大，那么水池就干了。一款产品可能因为市场竞争、拉新乏力导致新增用户数下降，也可能因为产品改动，运营策略失误造成后续流失用户变多。", "将数据制作图表：", "(活跃用户和不活跃用户可以拆分出来，周活跃同理)", "如果是一个好的用户运营，他会继续思考：每天有多少活跃用户变得不活跃?有多少忠诚用户变得不活跃?又有多少流失用户被我们唤回来等，并且分别是什么原因引起的。", "怎么样更详细的监控活跃数据的变化呢?我们引入桑基(Sankey)图的概念。", "这时，活跃数据比单纯的表格清晰多了，而且我们也能够显著观察到不同活跃层的变化。万千变化，存乎一图。", "以上种种，皆是用户运营需要考虑，也是要和各部门协同解决，贯彻整个产品一生的运营方向。", "活跃类指标有一个显著特点需要明白，它们都是后见性的指标，也就是事情发生后我们才能观察到。比如我们发现某一段时间流失数据(假定两个月没打开APP为流失)上升，往前倒推两个月，发现当时刚好展开一次活动，那么我们有理由相信活动造成了一批用户卸载，可惜运营此时已经无能无力。先见性预防比后见性观察对运营更重要。", "根据不同的用户活跃状态，依据产品的特性能采取很多运营手段。这是精准化运营的第一步。接下来则是划分用户层次等，进行更精准的运营，不过那是另外的话题了。", "用户运营路漫漫修远兮，用我偶然得之的一句话做结尾吧。", "别低头，活跃会掉，别流泪，报表会笑。", "1.以上主要针对移动产品，对网站，或者网站手机端混合，乃至微信端。可能要采用另外一套统计逻辑，这里不展开了。", "2.市面有许多第三方应用或SDK能达到类似的数据统计。不过我建议，若只能统计，不能拿到数据，为了后续运营还是辛苦点做一套活跃统计系统写入数据库吧。", "3.下一期，我们可以引申出用户运营的其他框架，或者怎么用有趣的桑基图，做更性感的分析。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76791"]},
{"module": ["干货教程"], "note": ["\n文 | 傅一平 博士 浙江大学毕业 当前就职于浙江移动\n任何企业的大数据变现都需要充分利用自己的优势，去创造适合自己的商业模式，基于运营商的现状，其大数据变现要获得突破，前期必然是以..."], "title": ["论运营商当下的大数据变现服务模式"], "content": ["文 | 傅一平 博士 浙江大学毕业 当前就职于浙江移动", "任何企业的大数据变现都需要充分利用自己的优势，去创造适合自己的商业模式，基于运营商的现状，其大数据变现要获得突破，前期必然是以资源驱动为核心的，这里笔者就谈谈当前关于运营商大数据合作变现服务模式的看法，涉及六类资源驱动的合作模式。", "所谓资源驱动就是要最大化运营商在数据，平台上的资源优势，尽量抵消在体制上的劣势，从而走出一条自己的变现道路。", "运营商的一些基础数据是能直接创造价值的，比如号码、姓名、身份证等，同时运营商还有IMEI、IDFA、COOKIE、MAC等一些隐性的ID信息，随着黑市交易的猖狂、数据的非唯一性以及一次使用的价值限制，这些数据贬值的会比较厉害，变现前景并不看好。", "反倒是运营商的一些通信属性数据，比如号码开户时间、通信费用、停机情况、欠费情况、充值情况等，由于其独有的特性，在很多领域都具有价值。", "但基于客户隐私的考虑，这些数据是不能直接变现的，必须要通过用户的授权，因此当前只能在诸如金融验真等场景使用。", "运营商也在持续加大O域(网络)数据采集规模和粒度，诸如移固上网DPI、 CS及PS域的信令(如实时位置)、MR(测量报告)等数据采集都在变为现实，仅一个浙江地区的移动DPI记录就超过每天700亿条，一天的位置信令打点数据就上百亿，但这些基础数据价值密度低，隐私保护要求更高，实际也是很难直接变现的。", "因此，运营商的原始数据虽然有巨大潜力，但并不是直接能用的黄金或石油，在当前安全的限制下，实际对外变现的能力有限，当前的一些实践证明了这一点。", "既然纯粹的裸数据没前途，那么再来看看基于运营商的上网等数据打磨出的标签体系或者做的分析报告是否有希望，但基于笔者的判断，这些数据加工服务面临两个现实挑战：", "诸如用户上网偏好等的客户标签，只有进行对外投放才能产生价值，无论是COOKIE、IMEI亦或表征个人或设备的其它ID，必然涉及对外的输出和对接，这类标识ID当前处于客户隐私判定的模糊地带，至今业界没有明确的说法，互联网公司也许可以说交换售卖 COOKIE 是行业惯例，但对很多企业来说这是过不去的坎。", "姑且不说运营商做的标签体系对外是否有价值，就说做分析报告吧，在相当长的时间内，如果没有找到一种规模化拓展的模式，你说运营商要靠自己现有的能力去做垂直行业的分析报告赚钱，也是赔本赚吆喝的买卖。", "可以认为，运营商即使当下能打造出对外适用的标签体系，也无法规避清单输出变现的安全风险，分析报告如果仅仅做成一单单买卖，规模化的挑战也是巨大的。", "运营商通信实际做的是管道，然后源源不断的收租管道的钱，数据在大数据时代将变成一种基础资源，运营商理论上可以基于管道中承载着的海量数据，成为出租管道数据的服务商。", "如果认同管道的地位，运营商大数据变现发力的点就不应是一个个应用，而是打造一个强大的数据舞台，比如打造一套基础标签，提供一套API，让所有的应用方都能在上面更好的跳舞。", "这类服务也叫做“搭台唱戏”，现在业界提的太多了，什么开放平台，能力共享等等，反正就是玩生态，运营商通过这几年的积累，却实际具备了突破的可能。", "在当前阶段，运营商应以开放的心态进行招商引资，策略上可以宽进严出，大幅降低合作门槛，让更多的合作伙伴参与进来，只要你愿意进来，我就给你所有的数据，你想怎么玩就怎么玩，但要把成果带走，需要确保信息安全及支付一定的费用。", "BAT 虽然也在打造开放平台，但无论说得多好，其只可能是开放平台而不会是开放数据，这是由商业本质决定的，运营商反倒可能比它更为开放，这是仅有的但却是最大的优势。", "当前一些运营商在这方面也进行了尝试，效果难说理想，既有合作伙伴观望的因素，也有运营商自身的安全考虑，更有模式、数据、平台及资费等还不成熟的原因，比如当下合作伙伴自主加工输出标签清单安全上并不允许，业界专注于大数据咨询、决策服务的合作伙伴太少等等。", "诸如浙江移动等运营商当前已经具备实际条件，在平台能力，数据能力，租户能力，安全管控，流程机制，资费定价等各个方面积累了一定的经验，笔者认为还是很有希望的，因为它的确是当下最有可能兼顾到各方能力和利益的一种规模化变现的手段。", "运营商拥有强大的传统渠道资源，姑且不提营业厅等渠道，即使如短信这个渠道，如果政策上允许，那精准广告投放的变现收入也是极为可观的，但行业对于运营商主动营销类的服务具有严格的限制，更别提异业经营了，因此这条路现在基本也堵死了。", "在互联网新型渠道上，运营商无论是网厅还是手机APP，当前其服务的主要目的还是分流传统渠道的流量，其产生的内部效益是可观的，可以显著降低传统业务的渠道办理成本。", "正是由于协助运营商主业的目的，这些线上渠道在相当长的时间内重点不会是基于这些流量进行大数据的对外创新变现，因为两者的根本目的不同，两者在运营的组织、政策、流程、系统等各个方面要求也不一样，短期内鱼和熊掌可能很难兼顾。", "但是这些线上渠道的流量特点跟BAT也是不同的，大家过来主要是办理业务，用户粘性不会很高，虽然也有上亿用户，但PV的差距是很大的。", "运营商在线渠道对外运营一种可行的方式也许是再造一个全新的线上渠道，运营的模式是完全互联网化的在线广告模式，将运营商的促销、补贴作为一种投放资源产品，解决冷启动问题，同步引入异业资源和广告商，以大数据能力为核心进行精准投放，大幅吸聚流量，一方面可以实现对外变现，另一方面也反辅了主业，形成一种新的带有运营商特点的自有渠道广告生态。", "以前运营商补贴是为了发展和维系自己的用户，但这类促销补贴却做在线下，没有形成自己的在线流量非常可惜，其实其一旦成势也是可以对外变现的，由于使用的是自己的渠道，大数据精准投放不会有安全风险。", "很多年前某些运营商也提出过N+1的大胆的设想，认为省级运营商除了有N个地市公司，还应该建立一个独立的在线公司，将线上渠道和大数据合二为一运营，两者是绝配。", "互联网+现在提的很多了，但运营商当前的在线运营更多还处在+互联网阶段，两者还是有本质区别的，运营商还需要加油。", "其实理念跟淬炼环境服务类似，只是最终的形态是产品，合作伙伴可以基于运营商的数据能力打造有竞争力的产品，然后双方分成，但也面临很大的挑战，一方面是当前应用类的大数据产品真正能变现的很少，除了广告和金融，还有谁呢?另一方面很多合作方对于预先投入产品研发资源是很犹豫的，其对于运营商数据理解的有限性也极大阻碍了这种模式的拓展，而运营商自主研发还有很长的路要走。", "位置、终端等数据产品可能算是能看得到前途的几个方向，但也仅是起步吧，同时运营商本身自己数据质量的问题，也降低了这类大数据产品的品质，运营商还是需要有一个大数据产品的爆点，一个真正能成功的案例，让各方能看到钱途，路漫漫而修远兮。", "由上可知，运营商变现的模式不能说不多，但要想真正的赚钱，每一样都需要用以百倍的努力去争取和打磨。", "在资源上，要努力争取，互联网公司创业好歹还有个风投，运营商大数据运营必然需要母公司给予政策上的倾斜，无论是数据管理，平台建设，资费定价，人力资源，渠道管理、补贴政策等等，都需要有个说法，这极大考验运营商组织和机制的灵活性。", "在能力上，要快速打磨，当前运营商在数据、平台、产品、运营等各个方面离互联网公司有相当的差距，就是机会来了也不一定抓得住，运营商应以工匠的精神去历练自己，按部就班就不要妄提大数据变现了，任何一种创新业务的突破都是靠血和泪堆起来的。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77158"]},
{"module": ["干货教程"], "note": ["\n文 | 秦路\n有不少人留言希望我推荐数据分析的书单，刚好即将春节，无论是假日学习还是年后，都值得充电。读书最好的时候是学生时期，其次是现在。内容按照 《如何七周成为数据分析师 》 的..."], "title": ["数据分析师的必读书单"], "content": ["文 | 秦路", "有不少人留言希望我推荐数据分析的书单，刚好即将春节，无论是假日学习还是年后，都值得充电。读书最好的时候是学生时期，其次是现在。内容按照 《如何七周成为数据分析师 》 的顺序。", "数据分析是一门专业且跨越多个领域的学科，虽然我每篇公众号都足够篇幅(乃至我自己觉得啰嗦)，可我还是得承认存在缺漏。如果有好书作为参考，对数据分析能力的成长更有帮助。", "这份书单权作入门级推荐，如果大家有更好的欢迎留言说明。我不能保证全部看过，毕竟基础书没必要看几本，但我尽量做到客观。建议大家根据自己基础挑选，不要贪多。", "大家多支持正版。", "知名度比较高的一套书，适合新手，优点是它和数据分析结合，而不是单纯地学习函数。学会函数适用的场景和过程比它本身更重要。", "是否需要学习VBA是仁者见仁的答案。我个人不建议。Excel VBA的最大优势是适用性广，哪怕去其他行业其他职位，都离不开Excel，这时候它就是一个工作加分的亮点。但是在互联网行业，对数据分析师，VBA的性价比就不高了。", "这里只推荐一本，因为我就翻过上面这本，还没全看…", "数据可视化的书不多。市面上多以编程为主，面向新手和设计的教程寥寥无几。 如果只是了解图表，看Excel的书籍也管用。", "内容很丰富，涉及可视化的方方面面，也囊括更类编程语言和设计软件：Python+JS+R+Excel。作者还有另外一本书《数据之美》。", "可视化是一门侧重灵感的学科，有一种入门技巧是从他人设计中学习，从模仿开始，了解他人是如何设计的，这个网络上有大量的信息图可以参考。当然数据分析师更需要的是如何发现，别只学习展示。", "英文足够好，可以看Edward Tufte的著作：《The Visual Display of Quantitative Information》、《Envisioning Information》、《Beautiful Evidence》。他是数据可视化的领军人物，他的理念是反对为艺术效果而混淆或者简化数据。暂时没有中文版。", "分析思维首推《金字塔原理》，金字塔原理有些人说它晦涩难懂，我认为是芭芭拉这个老太有骗稿费之嫌，本书包含了报告、写文、演讲等诸多内容。可以细看可以快看。另外还有一本同名案例集，有兴趣可以买。", "另外麦肯锡相关的书籍还有《麦肯锡意识》《麦肯锡工具》《麦肯锡方法》等。", "深入浅出系列是对新手非常友好的丛书，用生动但啰嗦的语言讲解案例。厚厚的一本书翻起来很快。本书涉及的基础概念比较广，包含一点统计学知识，学下来对数据分析思维会有一个大概了解。", "国外的精益系列一直以互联网创业作内容导向，本书也属于此类。如果是互联网行业相关，可以看看。它介绍了不同领域的指标，以及产品不同时期的侧重点。案例都是欧美，这部分做参考用。", "接下来的几本，是兴趣向读物。《黑天鹅》能拓展思维，讲叙了不确定性。《思考的技术》，大前研一的著作，也是咨询类经典。如果对咨询向的分析感兴趣，还可以看BCG系列，或者刷CaseBook。《批判性思维》，则是教你如何形成理性思维。", "数据库有很多种，常见有Oracle，MySQL，SQL Server等。我推荐学习MySQL，这是互联网公司的主流数据库。以后学习Hadoop生态时，MySQL也是最接近Hive语法的语言。", "MySQL不需要专门看书学习，因为数据分析师以查询为主，不需要考虑数据性能、数据安全和架构的问题。使用搜索引擎能解决90%的问题，我就是w3cschool学的。", "如果真想买书看，可以看这本，适合新手向的学习，看基础概念和查询相关的章节即可。网络上大部分MySQL都是偏DBA的。", "如果想深入，可以看《高性能MySQL》，对分析师没啥用。至于另外一个方向NoSQL，对入门者还是小众了些。", "如果有余力，就学习正则表达式吧，清洗数据的工作就靠它了。", "统计学是比较大的范围，分析师往后还需要学线性代数和矩阵、关系代数等。初学者不需要掌握所有公式定理的数学推导，懂得如何应用就行用。", "大概是最啰嗦的深入浅出系列，从卖橡皮鸭到赌博机的案例，囊括了常用的统计分析如假设检验、概率分布、描述统计、贝叶斯等。书本注重应用和趣味性，数学推理一般。", "国外的经典教材，已经出到第十二版了。国外教材都有丰富有趣的案例，所以读起来会比国内的轻松不少。如果你还在读书，不妨买这本看一看。", "名字既然有商务与经济，所以书中辅以了大量的相关案例。书内容很多，看起来不会快，适合细读。", "稍微有一些难度的英文书籍，属于进阶版统计学，国外很推崇。如果要往机器学习发展，这本书可以打下很好的基础。", "以上书籍的难度是逐步递增的。统计学是机器学习的基础，是概率、矩阵等实际应用。现在已经有很多统计工具，Excel的分析工具库、传统行业的SPSS、SAS以及R、Python等，使用过程都不用计算推导，大学考试才会考，现在都是计算机解决，轻松不少。", "不同领域的业务知识都不一样，这里以互联网举例。", "增长黑客的概念就是随着这本书的畅销传播开来。增长黑客在国内即是数据分析+运营/产品的复合型人才。这本书好的地方在于拓展思路，告诉我们数据能够做什么，尤其是连AB测试都不清楚的新人。", "实际涉及的业务知识不多，我推荐，是希望新人能够了解数据驱动的概念，这本算是我走上数据化运营的启蒙读物了。", "知乎亮哥的书籍，互联网所有的数据都是和运营相关的，如果是新手，就以此学习业务知识。如果已经工作很多，就略过吧。", "互联网不再是网站的天下，但是移动端依旧有Web，我们在朋友圈看到的所有H5活动、第三方内容等，都是依托网页实现。网站的数据分析依旧有存在空间，网站的数据指标还是能够指导我们运营。", "这本书涉及了数据挖掘，但是比较浅，可以作为数据分析师视野的承上启下，了解数据化运营的高级应用。特点是以阿里的实际工作相结合，可又因为保密原则不够详尽。", "是各领域专家众筹完成的书本，比起传统的书籍，囊括范围更广。虽然没有深度讲解技术，但是各领域的案例都是一手资料，对业务的触类旁通理解有帮助。", "业务知识我不再多推荐，以后我会通过公众号文章的形式讲解。因为从我看来，市面上也没有详尽介绍数据角度下的用户行为、产品运营的书籍，都是点到为止。这一块内容，尽量从工作中去学，收获才是最大的。", "欢迎来到数据分析的最后殿堂，Python和R都是大分支，基本是前面所有内容的实现。Python的学习以PY3为前提，毕竟2017年了，我实在想不出不用Python3的理由。", "除了书籍，Python/R更多依靠博客和文档学习。Python的学习路径不陡峭，新手水平取决于查询能力，所以也请学会如何高效搜索。", "还是深入浅出系列，完全适合零基础的新人。需要注意的是，编程学习不同于其他知识，如果计算机基础不稳固，在使用中会遇到各类问题。知其然不知其所以然，这是本书缺点：能掌握，但是Bug比较多。", "对于拥有编程基础的人，这本书系无巨细的有些啰嗦，不过对新人，可以避免不必要的坑。把它当作一本工具文档吧，当遇到不理解的内容随时翻阅。这是纸质书比电子书好的优势之一。", "非新手向的书籍，成书较早，部分内容比较老旧。虽然学习中不会有问题，但很多Pandas函数已经有更优雅的写法了，例如df.query。每段代码都敲打一遍，千万行的数据清洗基本不会有大问题了。", "Python的进阶书，如果想要掌握更好的编程能力，这是一本经典，值得时时翻阅。注意，它更偏向程序员。", "R语言的入门书籍，从数据读取到各类统计函数的使用。虽然没有涉及机器学习，依靠这本书入门R是绰绰有余了。", "这本书是将R语言和统计学结合的教材，可以利用这本书再复习一遍统计知识。缺点是书本后面的内容质量不如前部分。", "到这里，入门书籍推荐完毕，当然好书不嫌多，例如《数学之美》、《集体智慧编程》、《统计学习方法》等，有兴趣不妨阅读。", "上面的内容都吃透，不论是成为一名数据分析师，还是往后向机器学习、数据科学家、数据产品发展、都有了良好的基础。", "希望你能沉下心阅读。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76999"]},
{"module": ["干货教程"], "note": ["\n文 | 陈文\n应广大女生的要求，在虐狗节前一天放出女生脱单攻略，大家有福利啦。男生版可参阅>>>\n写在开篇\n分析师的基本职业操守，就是没有调研没有发言权，没有实践没有指导权。因..."], "title": ["女生特供 | 脱单路径大分析，科学拯救单身的妳"], "content": ["文 | 陈文", "应广大女生的要求，在虐狗节前一天放出女生脱单攻略，大家有福利啦。", "分析师的基本职业操守，就是没有调研没有发言权，没有实践没有指导权。因为陈老师不是女生，所以仅作了本文的分析部分，建议部分是陈太太现身说法，友情赞助的，此处应有掌声〈( ^.^)ノ", "女生脱单与男生的本质区别", "女生说“我是单身”，意思是：我身边没有优秀的男生可以谈恋爱", "男生说“我是单身”，意思是：我真的没有人可以谈啊!!!!", "这就是站在自动售货机前发呆和在沙漠里哀号的区别。", "女生只要愿意放低身段，主动追求是很容易获得一个男生的。但女生一旦动情就会奋不顾身，万年一遇到渣男就惨了。所以女生一定要把握尺度，注意安全。", "因此，女生脱单的核心就是两点：吸引优质男生，保护自己安全!", "去过演唱会的女生都有这种体验：你在看台上看着自己家爱豆，喊着他的名字， 唱着他的歌，挥舞荧光棒好嗨森。然而，站在你家爱豆角度是看不到你的，因为他眼中是10万名和你一样的妹纸。女生喜欢的优质男也是这样。他可能是精英前辈，帅气学长，美颜欧巴，多金二代，他面前也是至少10个妹纸在招手。", "所以想吸引优质男生，最重要的不是喊得再大声一点，而是从人群中脱颖而出，减少自己和他之间的差距。因此不论目前是什么收入水平，女生都不能放弃继续努力。越努力遇到优质男生的几率越大。这里的努力，不是指让大家加班加的更多。正如同一个泥瓦匠熬夜砌再多的墙也还是泥瓦匠一样，要成为设计师收入才能提高。因此大家需要在提升能力上多做文章，具体的，可以考虑：", "另一个好处，是当你遇到你心仪的男生的时候，你是以一个努力上进，认真学习的学妹身份出现的。你自己努力上进，别人就看的起你，你自己认真学习，就没有人觉得：“诶，这个女生春心荡漾可以吃豆腐哦”这种感觉更像是在图书馆里遇到一个清纯学妹，对于多金颜低的理工类宅男来说简直是必杀!恨不得把毕生所学拿出来教你。", "积极向上，重在积极，但钱也是要有的。不然下一阶段是没钱开展的，更不能陷入“扶弟魔”的怪圈，企图自己挣钱贴爸爸妈妈，贴弟弟，贴弟弟的儿子。亲，你还在单身，先解决自己的问题，就是对他们最大的帮助。", "实战案例：陈太太为了摆脱嫁给县城思想封建，收入低下，婆婆凶恶的男人的命运，用了2年时间从某大型国企的县公司，升到市公司，又到省公司，期间拼命工作，天天加班边哭边加，虽然到省公司时已经18岁了，但成功脱离了自己的命运。所以，年纪大不是困难，穷才是。(老婆永远18岁!!!)", "只有懒女人，没有丑女人。对女生外表伤害最大是生孩子，其次是不打扮，第三才是年纪越来越大，第四才是长相。人靠衣裳马靠鞍，三份长相七分打扮。而且，大家知道男生的思考逻辑，是：颜值X(身高+性格+收入+兴趣+出身+……)说白了直男爱网红，漂亮压倒一切。想要在图书馆一头撞到学长怀里然后被帅气学长撩到，没有美颜是不行的，只会落一句：“大姐对不起啊!”。实际上男生对女生的长相也是各有爱好的，因此要最大限度发挥自己的优势，弥补劣势，打扮出特色，比如：", "万一真要有女生不善此道，最好的办法就是直接向网红学习。比如化妆技术可以向微博老邢学习，效果堪比仪容，对直男相当有杀伤力，还有很多类似化妆教程。穿衣风格可以找一个个子身材和自己差不多网红，学习别人的服装搭配。", "实战案例：陈太太个身高真的很短，还没有网红的时候就穿five plus品牌比较多，把自己打扮成可爱的小蛋黄派，成功戳中陈老师心中萌点 ( ^∀^)现在当辣妈了就关注网红金蘑菇菇，也是娇小型女生，直接学人家怎么穿衣搭配。(老婆写完这段我跪键盘且打字打出：老婆不短)", "优质男生不会自己送上门来，所以必须社交。又有两种情况：如果是文科类优质男(管理层，销售，老师)本身社交活动比较多，实际上我们还要和至少9个妖艳贱货挣机会，所以要积极参加社交活动，发现以后主动介绍，日常多交流。如果是技术类优质男(开发，运营，分析)他们比你还要宅呢，晚上要加班，藏在IT部深闺待娶，所以必须主动请教。", "这样，既能增加自己的知识，又能够有很多机会认识到背景不错，和自己兴趣相投的前辈，学长，大哥哥，自动过滤掉了收入，学历不高的矮挫穷。这样认识优质男的几率，要远远大于那些十八九岁不上学穿着暴露混夜店的小太妹的。拜托夜店是什么地方，你又是什么身份，如果把自己的青春当商品的话，就一定会吸引来消费完就走的渣男的。", "交往过程以学习请教开始，这样自己比较安全。对有兴趣的男生可以主动把话题往个人生活上引导，也别光天天坐而论道，人家还以为你把他当工具使呢。如果对方也恰好有意思，就会有第一次的面聊，之后就看事态发展和个人喜好而定了。总之女生要留给自己喜欢的男生足够的机会追自己，不要太拘谨，对撩男能力有信心的看到优质男可以主动出击!", "陈太太是和陈老师在一个CBD饭友群里认识的，平时大家聊天中，陈老师就暴露了自己在银行上班(CBD的饭友群吗，肯定会聊到你在哪个楼，平时做什么)，聊着聊着陈太太发现和陈老师是一个学校的(985高校)就换称呼了，用“师兄”来称呼陈老师，在一堆聊天中一下就抓住了陈老师的眼球(没办法理科生真的很吃这套)随后就展开了热烈的追求。所以大家可以看到，通过工作单位，学校，轻松提升自己的交友档次，相当有效哦。", "结束了前三步，大家应该已经有了一两个自己心仪的对象可以交往，再往下就是看相处合适不合适。以一个过来人的眼光看，相处比相爱更重要，因为最后婚姻幸福不幸福，主要是看日常平淡的日子过的顺不顺，而不是激情的时候有多嗨。", "好男人是靠考察出来的，不是想出来的。一个男人靠不靠谱，合不合适，要一起经历事情，光线上纯聊天没有任何意义。所以要多交往，多观察，少动心。不要脑子一热当迷妹，不要稀里糊涂把自己的感情交出去了，不要被一时间浪漫温馨豪言壮语，一个小动作一句暖心点戳中了心眼就跪了。(当然这很难，因为小姑娘的心思是很天真单纯的，确实容易被骗，渣男下十八层地狱)。", "考察一个男人，不光看他对你怎么怎么好，而要进入他的生活圈子，看他对他身边最近距离的人(家人，同事，朋友)怎么样。特别强调一点，渣男最喜欢的就是对女生屏蔽自己的生活圈子，营造一种我的心里只有你，我猜得透你，你看不穿我的感觉。", "考察一个男人，不光看他在关键时刻的表现，而要看他在日常表现。所以最好多一些场景，不要俩人在一起只有吃喝玩乐，游山玩水，可以更多处理一些公事，聊一些家庭私事。这样看人更全面。特别强调一点，渣男最喜欢和女生在一起只有花前月下，一到办一点正经事的时候就玩失踪。", "至于如何评价这个人吗，完全看个人。如果怕自己脑子一热陷进去，可以多听身边闺蜜，朋友，同事，父母，兄弟姐们的意见，这是保护自己的最后防线。那种奋不顾身抛弃一切和男的在一起的女生一般没有好下场。如果还没和对方熟悉到相互交换生活圈子，特别是自己不了解对方的圈子，就不要考虑深入下去，毕竟很多女生单身的原因就是被一个不回头的前男友坑了", "完了?不是照例这里该有段案例吗?", "前方高能虐狗!非战斗人员可在此撤离!陈老师和陈太太经历了什么", "爬山", "看话剧", "出国旅游", "设计新家装修", "去各种餐厅吃饭", "为陈太太买投资房", "制定计划表一起健身", "制定计划学习做饭榨果汁", "陈老师帮陈太太写工作论文", "陈太太帮陈老师做参谋换工作", "陈太太帮陈老师度过亲人离世的难关", "经过交往，我们对彼此性格的认可，压倒了一切生活中具体的矛盾，最终成为了幸福的一对↖(^ω^)↗↖(^ω^)↗", "所以，祝大家明年的这一天不用再回头看这个攻略，新一年要努力哦!", "照例，再解释下为啥这个东西会发数据分析社区，是因为分析师单身妹纸多(错!)是因为，所有的分析师都要注意这句：", "所以这是一篇如何提分析建议的超长干货!", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/77281"]},
{"module": ["干货教程"], "note": ["\n文 | 联想大数据-张建伟\n编者按随着日益增长的交通“大数据”，给交通管理创新带来的新挑战，以及对交通管理工作提出的新要求，交通信息化建设必然步入大数据所带来的智慧应用阶段，利用大数..."], "title": ["架构篇 | 借助大数据解决现代交通困境"], "content": ["文 | 联想大数据-张建伟", "编者按随着日益增长的交通“大数据”，给交通管理创新带来的新挑战，以及对交通管理工作提出的新要求，交通信息化建设必然步入大数据所带来的智慧应用阶段，利用大数据破解当前诸多交通瓶颈问题成为了未来交通行业的必由之路。那么交通行业面临着那些困境，大数据又是如何解决的?正是本文所着重阐述的。", "城市交通问题是自上世纪以来，工业发达国家一直为之困扰的问题。中国自进入改革开放以后，各项事业建设进程极速加快。随着经济的发展，我国交通发生了前所未有的变化。同业也面临着前所未有的挑战，根据不完全统计目前全国机动车保有量已达到2.85亿辆，城市交通拥堵指数超过1.5的有56个城市，其中三分之一的城市拥堵指数呈上升态势，民航航班正常率不足70%，交通困局已经成为了从普通群众到政府领导都密切关注的民生问题。十三五以来，伴随着云计算，物联网，大数据等先进技术的发展与应用，凭借大数据等先进技术解决日益紧迫的交通问题成为了政府与社会各机构研究的热点。", "如果想了解交通面临的困难，必须了解目前交通的整体生态，了解交通有那些数据可用，能用来做什么?", "1. 传由于横跨多个政府部门以及企事业单位，不同单位与部门的信息化水平又参差不齐，所以信息孤岛在交通领域仍然普遍存在。", "2. 受历史沿革影响以及技术更新渠道封闭等因素影响，面对越来越多样化的数据缺乏有效的采集手段。", "3. 随着物联网技术在交通领域的深入应用，海量的设备/传感器信息得以被记录下来，但由于成本，观念，安全等诸多原因此部分数据缺乏有效的存储媒介。", "4. 交通数据除了涉及部门多，格式多，数据量大等特点，还有一个最大的问题是当海量多元化的交通数据被集中起来后缺乏有效的管控和运帷。", "5. 最后也是最为重要的一方面困难是，目前行业内数据分析的方式方法还比较单一，统计报表与综合指挥演示系统被普遍应用，大家的分析与应用方式还是集中在事后处理环节，而对事前预防与事中监控等缺乏有效的手段与方法。", "面对诸多的问题，传统的凭经验、一成不变、理论与实践脱离的管理方式与处置方法显然已经无法应对，而随着交通现代化程度的持续推进，海量的交通相关数据被记录与存储下来形成了“交通大数据”，而如何充分利用“交通大数据”解决实际交通难题成为了交通行业所面临的挑战与机遇，在互联网+的时代交通大数据有哪些?它们又能帮助我们解决哪些问题呢?那么先让我们认识一下“交通大数据”", "典型数据为地铁运行数据，轨道交通运营数据，一卡通乘客数卡数据，清算数据等，其主要应用价值体现在客流分析，站点分析，政策辅助决策，指挥调度，异常检测，广告投放，设备监控预警等", "典型数据为停车场数据，路网信息，车辆保有信息，城市基础地理信息，交通管理信息，气象信息等，其主要应用价值体现在拥堵治理，交通基础设施规划建设，出行引导，停车诱导，交通管理优化等", "典型数据为港口集装箱数据，机场航班数据，远洋及内河航道船舶数据，航路信息，气象数据，法规数据等，其主要应用价值体现在航线规划，运力匹配，配载优化，智能船舶，货物跟踪，应急预警等", "典型数据为道路事故数据，车辆违法信息，交通监控信息，交通管制信息等，其主要应用价值体现在治安防护，应急处置，交通管理，刑侦分析，协同指挥，增值信息共享等", "同样通过将以上海量的数据整合后，通过大数据挖掘与建模等方法，可以有效的实现事前预防-事中监管-事后评估-事前预防这一可自优化的生态闭环，借助大数据多带来的智慧解决交通所面临的困境。", "既然交通大数据蕴含着丰富的价值并存在了广阔的应用想象空间，那么如何实际应用呢?联想一直致力于通过新技术与新模式的探索与应用去解决实际的社会，带动整个产业的发展。 大数据做为一种新技术，如何在旧有领域中应用并最大程度的发挥其优势更是联想所一直努力研究的主要对象。", "海量的交通大数据不仅涵盖了交通相关的各个领域，同时将这些数据集中后所产生的价值更是无法估量 。预计2020年交通大数据市场规模将突破200亿，随着新商业模式(数据运营，数据变现等)的成熟，以及象联想，易华录，软通等企业的不断投入，未来交通大数据将为我们带来一个全新的交通管理方式与出行方式。", "依托大数据为核心的交通行业整体解决方案将以数据为核心驱动，以大数据技术平台为载体，通过科学的数据分析方法对内不断挖掘数据价值，优化管理决策，对外不断提升服务质量，解决交通难题。", "在以后的时间里我们将从数据感知，技术平台，业务应用等层面分别展开对大数据智慧解决交通困境的探讨。", "本文由联想大数据组独家投稿给36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据 http://www.36dsj.com/archives/76175，任何不经同意的转载均为侵权。", "更多联想大数据组内容：", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76175"]},
{"module": ["干货教程"], "note": ["\n文 | 兜哥谈安全\n前言\n通常web业务的重要数据诸如账户、交易信息都会存储在数据库中，这也使得数据库成为黑产攻击的重要目标。所谓拖库，就是通过入侵网站，非法获取数据库内容。本文将从黑..."], "title": ["黑产研究之拖库"], "content": ["文 | 兜哥谈安全", "通常web业务的重要数据诸如账户、交易信息都会存储在数据库中，这也使得数据库成为黑产攻击的重要目标。所谓拖库，就是通过入侵网站，非法获取数据库内容。本文将从黑产的动机、常见手段和如何防范三个角度讲解下拖库的相关知识。", "黑产进行拖库的动机很多，我归纳主要有以下几种。", "主要目的不是将窃取的数据变现，而是通过散播消息，从商誉的角度打击受害企业，这种对电商、P2P、保险企业尤其致命，可以让广大消费者对该企业的安全能力产生严重怀疑以至于不信任。", "主要目的是将窃取的数据变现。", "炫耀能力，敲诈勒索的前奏。", "拖库只是地下市场贩卖数据的一个环节，整个流程已经形成了完整的产业链，各司其职，我们拿个简化的模型介绍下，整个环节中有这样几个角色。", "专门负责入侵网站，获取原始的数据库文件。", "专门负责从拖库者那里收购原始的数据库文件，然后根据不同的用途从原始数据中提取有用的数据。", "专门负责从洗库者(有时也直接从拖库者那里直接购买原始数据库文件)收购洗完整理好的数据，售卖给各类买家。哪些数据可以用于哪些用途是个非常复杂的话题，这里就不展开讨论了。", "数据买家就很复杂了，处于各种目的购买各类黑白灰数据，这里只列举几种常见的：", "1. 电信欺诈，购买知名电商的近期消费数据、用户详细信息", "2. 盗号，购买用户名密码用于撞库，偷玩家装备，甚至直接转账", "3. 非法广告商，购买消费、购物车、联系方式等数据，用于精准营销", "常见的拖库方式主要有以下几种：", "这个最常见，通过SQL注入漏洞可以缓慢偷走数据库中的数据，用神器sqlmap即可。不过这个动静特别大，攻击时间长。", "通过上传漏洞或者远程命令执行漏洞上传webshell到服务器上，通过webshell的数据库管理功能的大马即可把数据库数据dump走。", "这个情况就特别多了，我见过的有phpmyadmin弱密码甚至空密码导致数据被偷走了，还有数据库对外开放，账户被github泄露了，而且还没限制账户登录IP的。", "直接利用数据库的cve漏洞，绕过认证鉴权等限制，直接把数据拷贝走。这种比较少见，因为绝大多数数据还是在内网，无法在外网直接发起攻击，需要渗透到内网才能进行这类攻击。", "由于拖库的方式很多，所以很难仅统一某一种方式可以彻底杜绝，这里从纵深防御，协防联动的角度来介绍。", "WAF无法百分百解决拖库的问题，但是可以提高攻击门槛，有效低于中小黑客的自动化攻击，对SQL注入、上传漏洞、远程命令执行导致的拖库有较好的防御效果。WEB-IDS由于是旁路部署，可以更好的使用机器学习以及语法分析能力，所以针对SQL注入、上传漏洞、远程命令执行导致的拖库有更好的检测能力。", "这个是最直接的保护方式，直接在数据库的前端处理全部对数据库的访问，但是其部署难度较大，不如WAF部署方便。另外对于通过入侵数据库服务器直接copy走原始的数据的攻击，难以防范。", "监控对数据库文件以及备份文件的读写行为，发现直接copy即报警。", "通常DLP部署在办公网防止员工泄密的，但是确实有用户把DLP设备部署在IDC，从http流量中识别高危的身份证号、邮箱等数据库的泄露行为。思路不是从攻击行为的角度，而是从造成的结果的角度，比如发现某个连接返回超过100个身份证号即报警，但是如果是通过SQL的盲注来拖库，DLP检测会失效。", "加强运维管理，尽量及时升级补丁，配置acl，数据库账户根据用途区分等。", "本文由 兜哥谈安全 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据，任何不经同意的转载均为侵权。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75684"]},
{"module": ["干货教程"], "note": ["\n文 | 汪祥斌\n快春节了写点东西。DataEye创业已经3年了我们一直在数据的路上一路走来，3年我们一直在寻找的是数据最后的商业化出路在哪里?DataEye自身的定位一直是做数据服务，我们希望能把..."], "title": ["DataEye的数据商业化反思"], "content": ["文 | 汪祥斌", "快春节了写点东西。DataEye创业已经3年了我们一直在数据的路上一路走来，3年我们一直在寻找的是数据最后的商业化出路在哪里?DataEye自身的定位一直是做数据服务，我们希望能把数据转化为实实在在的商业价值，这条路上我们一直在努力。", "我个人一直认为数据变现和数据技术变现是两回事情。数据变现是通过挖掘与分析找到数据背后的商业价值和模式去实现变现。数据技术变现其实是输出数据相关的技术产品帮助客户去实现变现从而实现自己的价值。目前这个阶段国内大部分的大数据公司其实是通过技术变现帮助客户去实现价值输出。包括我们之前做了很多统计分析类的产品，广告追踪类的产品其实都是利用自身的技术产品手段去帮助客户实现一些价值转换。", "这个模式很清晰但我觉的此类的变现本质上来说不叫数据变现，我个人觉的我们之前做的事情更多叫企业服务，不叫数据服务，更不叫大数据服务。大数据服务被滥用了，用的稀烂导致现在投资者见到这三个字都有点想呕吐的感觉。我自己就被呕吐过，真事。劝君慎用这个大杀器!", "国内很多数据公司做的事情其实是在做工具当然我们前三年也在做这个事情(平台工具的争论就别争了，这里没有投资者不会影响你的估值)，工具的模式就只有一个那就是卖，所以大部分工具类的公司都是在卖服务产品。这种模式从商业模式来说很成熟没有什么要讨论的点，但这种模式背后的逻辑就值得讨论，你如果非要加个大数据加个数据服务我就想知道数据到底体现在哪里?", "你给客户究竟是怎么样的数据服务，数据价值在哪里，商业价值点在哪里，价格怎么定，怎么衡量。2年前我记得经纬的朋友跟我讨论了很长时间的商业模式究竟是工具还是数据服务，说实话那时候我没有理解清晰，但我隐约想做的依旧是数据的价值模式，不是工具模式。", "到今天很多数据公司超越了工具的界限因为他沉淀了大量的数据开始转换为平台思维希望通过数据去尝试变现，但目前来说变现效率特别高的我没有见过几家(当然有我就不明说了，但大家都清楚你的数据源来自哪里)，大家还在摸索。因为这个阶段数据的变现效率与数据资产质量成高度正相关，注意我这里说的是数据资产质量而没有说规模，因为在国内规模太容易造了，质量无法造因为你的合作伙伴会以收益与效果来直接拷问。", "我听了太多你家覆盖终端有20亿，他家有40亿，昨天刚听说了一个80亿的，应该不出我意料春节之后绝对过百亿，两年之后直接飙千亿的。其实运营商笑了，我他妈的一年入网才几台设备。其实我想说一点大数据真不是大而在有用。在平台以及转化数据商业价值阶段，规模真不是重要因素质量绝对制约你最终的变现效率，而且是很难逾越的一个坎，不相信可以回头想想。当然如果是通过估值去获利的可以继续爆炸规模，这种商业模式不在本文讨论。", "2016年我们与国内最大的DSP流量平台深入挖掘数据潜在价值的过程当中充分印证了我们对于数据资产质量的假设。大量的低价值的数据根本无法转化商业价值出来甚至我认为是无用成本，这家DSP平台也跟我们抱怨了之前一些数据合作伙伴无法长期合作的最终原因–数据质量太差无法持续规模化变现。通俗的说，用了和没用差别不大，成本还老高。这里有两个关键字眼持续以及规模化。", "昨天看了一位技术大牛的文章大数据接下去是算法和模型，但我从一个3年数据从业者的经历喷一下，想的远了，在中国现有阶段还远没有到呢，大道理讲多了挺烦的。接下去的阶段我觉的是如何快速提升数据质量的阶段，以及实现基础价值转化阶段。这两个阶段过不去再牛逼的算法你挖给我看看。", "我们创业选择了游戏这个行业现在来看是一个巨大的挑战。因为游戏商业模式高度清晰资金流转速度非常快速，开发者体量不大因此他们对效果与性价比有远超乎其他行业的诉求。再说个俗一点的比方，在游戏行业想通过大数据忽悠点钱你绝对是超高难度的挑战，我相信做过的都知道(有一些已经不在了)。我们之前合作过的一个流量平台当时对我说了一句话，数据已经优化到这个地步了为什么游戏公司还不买单?我只回了一句话，因为投一分钱今天就得产出两分钱。话糙理不糙我讲的是大实话。", "解决质量问题之后数据商业化就是模式的设计了，我们自己在行业的经历来看数据商业模式的设计必须揉合到业务当中，千万别真以为你是做大数据的，我觉的大数据这个行业未来十年有可能会彻底消失的，因为一切公司皆是大数据公司，而你懂的只是几个工具和数字而已。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75757"]},
{"module": ["干货教程"], "note": ["\n在今天，数据被称为新世纪的“石油”，数据还成为看待现实的新角度，不仅优化了企业决策和企业营销，还形成了一种新的的商业模式。正处于数据大爆发的时代，如何对数据进行有效分析就显得尤为..."], "title": ["神策数据CEO桑文锋：重视数据源的作用，让数据分析带来新的价值"], "content": ["在今天，数据被称为新世纪的“石油”，数据还成为看待现实的新角度，不仅优化了企业决策和企业营销，还形成了一种新的的商业模式。正处于数据大爆发的时代，如何对数据进行有效分析就显得尤为重要。", "从庞杂的数据背后挖掘，分析用户的行为习惯和喜好，找出更符合用户“口味”的产品和服务，并结合用户需求有针对性地调整和优化本身。这是数据分析的重要价值。", "根据数据分析的不同状况，我们可以把数据分析分为三个层次：", "在这阶段我们拥有用户的基本数据，比如有多少用户，用户的登陆情况，用户在线时长等等；", "对数据进行深度分析，以产品运营分析来说，分析用户的行为习惯和喜好，进行产品的优化，用数据去分析业务上出现的问题，去解决问题；", "。在这阶段，可以帮助用户更为准确，快速的决策。比如在一个O2O的APP用户一周没有使用了，就自动推送一个6元的优惠券，为什么选择是6元呢？因为6元是一个最具有性价比的，也是利用数据分析出来的结果，6元针对这类客户的召回率很高。", "在大数据时代到来的今天，我们已经错过了流量红利，迎来了数据驱动增长的时代。在这个时代数据呈现出指数性增长，数据也成为驱动业务增长的重要动力。", "那么在数据分析的过程中，如何让数据释放更大的价值，更好的驱动业务增长？神策数据CEO桑文锋给出了答案——", "目前，神策数据的产品正处于创造新的业务价值层次，在未来神策数据也将朝着产品智能化的方向前进。那么神策数据又是如何让数据释放更大的价值，帮助客户带来新的价值？", "神策数据的产品有三大特点来帮助客户创造新的价值：", "。在2016年，我们看到了众多大型的数据泄露事件，私有化的部署让供应商接触无法数据，降低数据泄露的风险，保证数据安全；", "未来的数据分析是越来越深入，越来越精细的，只看一个宏观的数据是远远不够的，线上的，线下的数据都要打通，这样才可以更加准确、灵活的去分析。无数的数据分析经验也告诉我们数据源的重要性，只有足够多、准备的数据，我们才能得到正确的分析结果；", "针对不同行业，不同需求，不同技术能力的客户， 统一的分析服务肯定是满足不了每一个客户的。PaaS平台可以让客户在神策的基础上进行二次开发，开发客户独有的分析需求和系统。", "我们在数据分析的同时也要重视数据源，桑文锋多次说到过，", "这里的数据要求是准确的数据，全面的数据。这也要求我们在数据的采集的同时，要对前端、后端、第三方数据库、业务数据等等全面的采集，来保证数据源的准确和全面，更好的进行数据分析，给业务带来新的价值。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75763"]},
{"module": ["干货教程"], "note": ["\n文 | 何晓阳、王鹏\n点评\n或许我们太过于喜欢成功的故事，我们的眼中总是看到了一家家创业公司不断地从VC那里拿到一轮轮的投资，但我们也不应该忘记一个现实的数字，每年成功IPO的科技企业不..."], "title": ["一家成立7年的数据库公司RethinkDB：为什么会失败?"], "content": ["文 | 何晓阳、王鹏", "或许我们太过于喜欢成功的故事，我们的眼中总是看到了一家家创业公司不断地从VC那里拿到一轮轮的投资，但我们也不应该忘记一个现实的数字，每年成功IPO的科技企业不过寥寥数家，因此，在现实的意义上，大多数创业企业，哪怕是明星企业，也都是要失败的。只不过，有的时候，这种失败引人关注，但大多数时候，这种失败是无声无息的，我们看到的那些融了A轮的企业，我们并没有看到他们融到B轮，我们以为他们还在A轮的状态继续奋斗，但其实，他们已经死掉了。", "死亡是一个令人忌讳的字眼，正如我们在公路上开车的时候，看到动物的尸体会大惊小怪，因为，动物们自己也知道这一点，它们会选择在濒临死亡的时候离开群体，跑到僻静的角落等待死亡的降临。无论如何，在我们的认识中，死亡和失败都不是值得夸耀的事情，但正如周航在一篇文章中所说，或许，失败不可避免，成功可能仅仅是由于偶然，江湖的另一面，是由失败者来书写的，正是这些失败者，用他们的反抗、愤怒与不甘，书写了江湖另一面的历史。", "这里有一家科技公司，叫做RethinkDB，正式宣告了他们的失败与死亡，这让我想起了很多事情，RethinkDB的创始人绝非无名之辈，他们也成功的融到了来自知名VC的1200万美元，然而，在和MongoDB的竞争中，RethinkDB尽落下风，最后不得不草草收场。在这篇文章中充满了Mongo的怨念。然而，我读完之后，我觉得作者的说法不无道理，很多公司的成长对于整个领域的生态是破坏性的，在进行技术型创业的时候，我们不得不思考更多。我让同事们把RethinkDB创始人的文章翻译了一遍，仅供大家阅读和思考。", "原文：", "当我们宣布 RethinkDB 公司正式倒闭的消息时，我曾经承诺要写一份事后总结检讨。我花了一些时间来审查过去这段经历，现在我可以把它清楚地写下来。 在 Hacker News 上，网友帮忙指出了很多 RethinkDB 失败的原因，既有人性本身难以理解的刚愎自负，也有来自 MongoDB 营销团队的「精明」策划，还有人说，是因为我们缺少有经验的 Go-to-Market 团队，以及对64位以上浮点运算缺乏支持。我将这些评论和总结放在这里，希望能够给大家带来一些启发和建议：", "首先，我必须说明，其中的一部分原因有一定的真实性，但是它们仅仅是问题的表现，并不是真正的原因。比如我们没能赚钱就是一种同义反复，它并不能解释我们失败的原因。", "我们在事后进行了复盘，发现有两个方面存在问题：第一个，我们选择了一个非常槽糕的市场，并且按照错误的「优秀衡量标准」进行了产品化。甚至每一个错误都可能将 RethinkDB 的价值减少了一到两个数量级。因此，如果当时我们能把其中任何一个方面做对，RethinkDB 都可能会达到 MongoDB 的规模。如果我们能把两个方面都做对，最终我们就能达到 Red Hat 的规模也未尝可知。(备注：当然，别对数字太较真。作者说的是大概的数字，不过这应该能够让你了解这些错误造成的后果)", "我们当时的想法是这样的，一家新的数据库公司不太可能会超越甲骨文。因此，我们想创办一家新的基础架构公司，数据库市场规模极为庞大。如果我们能够打造一款优秀的产品，即使只能够占领一部分市场，那么，我们也会变成一家非常成功的公司。", "然而，公司处在哪个市场中并不是由自己的看法来决定，而是由「 用户认为你是一家什么公司」 来决定，而 RethinkDB 的用户显然认为我们是一家提供开源开发工具的公司，因为那就是我们的实质。不幸的是，开源开发工具市场是公司可能会倒闭的「 最糟糕的市场」 。目前，成千上万的用户都在使用 RethinkDB ，而且通常是在业务环境中使用，但是大部分都不愿意为产品的终身使用权支付哪怕超出一杯星巴克咖啡的价格(也就是说，用户并不愿意付钱)。", "但这不是说，因为产品太好了，已经不需要大家付钱来支持发展了，亦或是开发人员没有控制好预算，亦或者是资本市场的失败。答案或许可以用基本的微观经济学原理来解释：开发者们都喜欢建造开发工具，而且这些工具通常是免费的。因此尽管存在庞大的市场需求，而且供应量也非常庞大，进而就促进了备选开发者工具数量的增长，然后导致价格的不断下降，以致接近于零。", "如果想了解这个模式，我们可以看一下 MongoDB(估值约为16亿美元，拥有约700名员工)和 Docker(估值约为10亿美元，拥有约300名员工)。这两家公司都在各自所在的市场拥有完全的主导权。有两条关于私营发展阶段的科技公司的经验法则：估值是年收入的10倍，平均每个员工的年收入[2] 是20万美元。这就意味着，MongoDB的年收入是1.4-1.6亿美元，Docker的年收入是6千万到1亿美元之间。虽然这些数字看起来还不错，但是对比一些非开发者工具市场中的 B2B 企业，像 SalesForce、Palantir，还有 Box(虽然面临着激烈的市场竞争)，MongoDB 和 Docker 的收入在他们面前就显得有点「微不足道」。", "当然，市场上很多的成功的大企业发展也不是一帆风顺的，即使拥有良好的合作伙伴、优秀的分销策略和很多大客户资源的成熟企业尚且面临着发展的问题，处于萌芽阶段的创业公司的处境是怎样的呢?", "对 RethinkDB 来说，这还意味着难以解决的「 客户获取漏斗」模型 。如果说在机会众多的 B2B 市场，100条销售线索能转化成10个商机，最终实现1个订单的话，那么对开发工具创业公司来说，这些数字都要乘以10。你能得到大量的优质潜在客户——很多用户愿意下载你的产品，跟你亲切的进行交流，但是你需要消耗多得离谱的线索，才能最终拿下一个订单。这就带来了灾难性的「 多米诺骨牌效应」 。整个团队士气受挫，并且很难吸引投资，也很难请到顶尖的技术人才。接下来这种情况会不断恶化，进一步限制你的资源，让你无法对产品和分配进行充足的投资。创业公司存在生死危机，而初期的分销策略几乎注定了，你最终将走向灭亡。", "诚然，市场环境不好，但是其他的开发工具公司依然还是卖了很多产品，为什么 RethinkDB 还是没有做到呢?", "虽然，我们无法左右市场动态(除非我们转行换业)，但是，产品决策完全可以由我们自己掌握，所以我们想打造一款优雅、耐用而且好看的产品，因此我们按照以下标准进行了产品优化：", "[3]：我们做出了非常严格的保证，并且认真贯彻执行;", "：我们承担了安装过程中最复杂的工作，这样使用我们的应用的开发人员就免去了很多麻烦;", "：从查询语言、客户端驱动、集群配置、使用文档，以及首页的营销文案，我们都做到了尽可能的一致性。", "这些权衡条件是不是看起来眼熟?那是因为它们直接来自那篇《更坏就是更好》[4] 的文章。但是，实践结果表明，对大部分用户来说，准确性、简介的界面性和一致性都是错误的优秀衡量标准[5]。其实，很多用户想要的是下面这些效果：", "：他们希望产品在他们需要的时候已经存在，而不是三年之后才上线;", "：人们希望 RethinkDB 在他们尝试的任务量中表现迅速，而不是我们建议的「 真实世界」的任务量。举个例子，他们会写一些脚本，来测试在不用读回的情况下，插入成千上万个文档需要多长时间。MongoDB 在这个层面表现优异，而我们却在徒劳地试图教育整个市场;", "：我们打算打造一个良好的数据库系统，而用户想要的却是一种完成 X 任务的好方法(例如，存储来自 hapi 的 JSON 文档的好方法，存储和分析日志的好方法，或者生产报告的好方法等等)", "我们并不是没有尝试过加快产品发布，提升 RethinkDB 的速度，并且搭建与之相关的生态系统，来轻松完成现有的工作，我们真的努力了。但是一个正确、简洁、一致的软件需要花费很多时间来打造。这就让我们比市场落后了三年的时间。当我们觉得 RethinkDB 满足我们的设计目标，我们足够自信地向别人推荐使用它时，几乎所有人都在问：「RethinkDB 跟 MongoDB 有什么区别?」我们努力解释为什么正确性、简洁性和一致性很重要，但是最终发现，这些并不是大部分用户在意的优秀衡量标准。", "坦白来讲，这点很伤人。我们真的很受伤，我们很难理解为什么人们会选择一个那样的系统：几乎没有它本应实现的功能(存储数据);带着一个 kernel lock;随机抛出错误;执行单节点功能，却在你选择 shard 时停止运行;作为产品的核心功能之一，sharding 系统几乎不能用;基本没有正确性保证，并且展示出来的界面五花八门，毫无一致性可言。", "每次 MongoDB 发布新产品时，人们都会祝贺他们做出的改进。我心里却感到怨恨。他们宣布解决了 BKL 的问题，但是实际上他们把粒度级别从数据库降到了 collection。他们宣布增加了新的操作，但是却不是一个与系统各部分匹配的组合式界面，而是简单地加了个一次性指令。他们对 sharding 进行了改进，但是显然他们并不愿意或者没有能力实现最基本的数据一致性保证。", "但是随着时间的流逝，我学会了欣赏「 群众的智慧」 。MongoDB 在人们最需要的时候把普通的开发人员变成了「 大英雄」 ，而不是让他们等待多年。它加快了数据存储的速度，而且帮助人们加快了产品发布的速度，而且 MongoDB 也在不断发展。他们解决了一个又一个基础架构的问题，现在它已经变成一个非常优秀的产品。它也许不像我们所期待的那样好看，但是它完成了任务，而且完成得很好。", "2014年中期，我们显然已经在这场竞争中落败，于是我们努力与 MongoDB 区别开来。我们发现了一种增加实时推送[6] 的好方法，期望这能够让开发人员打造以前无法实现的新一代应用。但是那并不够。我们忽然发现自己还在跟 Meteor 和 Firebase 在竞争，在我们想到这个方法之前，他们已经在实时问题处理上投入了多年的时间和努力。这意味着，我们又比市场晚了三年，然后再一次在竞争中落败。", "有些人建议我们开发一款云产品。实际上，我们的确做了一款，因此接下来我要讲的问题，也很有意思。", "如果一家小公司要想打造一款云服务，那么其中一个非常突出问题，就是这种模式符合一个常见的创业失败模式——精力分散。开发、分发和运营一款可靠的多用户云服务真的很难。它需要非同一般的专业技能和资源，因此如果你走上这条路，你就会发现自己似乎同时在运营两个创业公司。但是当时我们面临着已经存在的威胁，而且很快就将走投无路时，我们还是不管不顾地尝试了一把，假设现在我们已经顺利完成了。", "我们的推理过程是这样的：数据库云产品有三种选择：主机托管、数据库即服务(DBaaS)、增值平台即服务(PaaS)。我们再用前面用过的经验法则，平均每个员工的年收入为20万美元，来算一下：", "所以这些市场都很小，比数据库市场本身还要小。但是会不会其中一个比另外两个选项要好一些呢?主机托管本质上就是在 AWS 上替人工运行数据库。你也可以在AWS上自行设置数据库。这很痛苦，不过倒也没那么难。因此主机托管服务的定价有个上限。考虑到 Compose.io 和 mLab 提供的 MongoDB 的用户量比我们高一两个数量级，我们推断，推出主机托管并不会引起市场注意。", "数据库即服务比主机托管更复杂，DBaaS 完全从用户那里接手了抽象节点管理。你只需要运行你的查询，系统就会处理。你不必了解系统内部运行多少个节点。这个业务富有挑战性，一方面是因为 DBaaS 公司不得不跟大公司竞争(例如 DynamoDB 和 DocumentDB)，一方面是因为在有众多备选的情况下，客户并不情愿把数据管理工作完全交给一个创业公司来做(你知道有哪家公司在用创业公司的 DBaaS 产品吗?因此 DBaaS 这个选项也被排除了。", "最后一个选项是打造增值平台即服务。我们以为这个方面有希望，因为我们拥有巨大的技术优势。Firebase 和 Meteor 需要基于 MongoDB 打造应用级的实时逻辑，这从根本上限制了实时查询能力和性能表现。与之相反，我们实现了自上而下的全栈式控制，因此我们的产品相比以上两者会具有显著优势。", "因此我们打造了 Horizon，并且开始打造 Horizon 云，这样用户就可以部署和调整 RethinkDB/Horizon 应用。一个小创业团队同时进行三个大项目的挑战(RethinkDB，Horizon，以及 Horizon Cloud)最终让我们尝到了「 恶果」 。在发布云产品之前，我们的资金就用完了。不过工程师团队还是很了不起的，他们只差一点点就能够完成了。", "我们还可以再做一层根源分析。为什么我们选择了一个糟糕的市场，而且按照错误的标准进行了产品优化呢?", "在我小时候，我想自己做一个收音机。我用胶合板做了一个盒子，往里放了一些废金属，然后给盒子连上了电源线。我家有一些电子学的书，但是我以为我不需要它们——我坚定地认为，我自己能搞定。最终我的确做出了一个能用的收音机，但是数年之后，我才终于意识到，其实，我还是需要学一些基本电子学知识。", "早期的 RethinkDB 是类似的情况。我们并没有市场和产品方面的直觉，因此我们就简单走一下过场，创办了一家公司，却并没有真正理解我们在做什么。而且我们还抱有极大的乐观主义倾向[7] 。就像医生明明知道医药公司的礼物会给他们开药方时带来一定的影响，却深信自己并不会受到这种效应的影响一样，我们相信我们也不会受到企业经营的经济和数学规律的影响。当然，这些经济和数学规律最终让我们尝到了恶果。但是，我们当时能做些什么来避免这些错误吗?就像我小时候尝试做收音机一样。其实，我们没有什么能做的，因为我们能力不足，却不自知，而且经过好几年时间，我们才意识到这个问题，为时已晚。", "一些人指出，如果我们当时能组建一个有经验的面向市场的团队，我们就能做得更好。这句话100%正确，但是我们的自我发展与公司需求并不一致。最初我们并不知道我们需要面向市场的专业人才，因此就没有在初创团队中招募这样的人才，顺便提一句，辨别缺乏强大的商业直觉的优秀商务人士，就跟辨别缺乏强大的工程直觉的优秀工程师一样难。等到我们建造出符合现实的心理模型时，我们才发现资金短缺，身处一个「槽糕」 的市场，身边遍布强有力的竞争对手，而且我们的产品比市场落后了三年的时间。在那个时候，即使是世界上最优秀的市场团队也救不了我们了。", "相信很多人对开发者工具市场怀有非常强烈的感情。因为工程师们热爱打造好用的开发工具，因此他们极其希望开发工具公司能够繁荣发展。我也不愿意贬低整个市场，一方面是因为我不想因为单方面的经验就一概而论，另一方面是因为我不想说「 不可能」 ，市场上就是存在很多特例，像 GitHub、MongoDB 和 Docker 都发展成了大企业，包括 GitLab 和 Unity 发展得也不错。", "如果你真的打算创办开发工具公司，我希望你能够谨慎行事。市场上有很多好的替代产品，用户期望很高，产品价格却很低。你需要深入思考：你能给用户带来的价值是什么。请记住一点——想让世界以某种方式运行是绝对行不通的。", "2009年，我们在 YCombinator 演示会上向一群投资者推介 RethinkDB 的早期构想(当时我们还没有软件)。我们的推介最后提出了三个要点。「 如果你只记住了RethinkDB 的三点内容，」 我们说，「 记住这三点。」 这个办法非常奏效。人们没有记住这次推介会的其他内容，但是他们的确记住了最后的三点。", "现在我给你留下要记住的三点内容。如果你能记住这篇文章，记住这些就够了：", "选择大市场，针对特定用户;", "认清团队缺乏的人才，然后拼命找到这样的人才;", "认真阅读《经济学人》[8] ，这样你们会进步得更快。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75724"]},
{"module": ["干货教程"], "note": ["\n文|李宁\n对2016年作总结后，发现自己的文章中有很多是在务虚，很多新入行的童鞋通过私下方式询问了很多比较实际的问题。但在遵守公司数据保密规定的前提下做务实的分享不是一件容易的事情（..."], "title": ["数据是这么来的 携程机票前台埋点二三事"], "content": ["文|李宁", "对2016年作总结后，发现自己的文章中有很多是在务虚，很多新入行的童鞋通过私下方式询问了很多比较实际的问题。但在遵守公司数据保密规定的前提下做务实的分享不是一件容易的事情（相当于让了一車马炮，想来一手好棋顾虑比较多），最后决定从埋点的维度来切入，将从埋点方法、指标释义以及分析方法扩展以窥全貌。", "一名数据人如果连埋点和指标模棱两可、则根基不稳，随口一反问都可能成为定时炸弹，坍塌整个分析过程。如果你认为埋点只是开发的问题，数据人是拿现成的数据来来写sql、完成分析，未来路可能会越走越窄。", "我的理解，数据分析师，可以根据埋点的质量来决定怎么使用埋点，在什么情况下用什么埋点数据会更贴近事实，很自信地说“我给出的数据是现阶段最可靠的”，面对别人的质疑时，你的数据无可辩驳。", "数据分析师不是抱怨埋点质量差而影响了自己分析，而是应该想“我如何用好现有的埋点来找到最贴近事实的数据来支持我的结论，埋点质量在不断改进，但我不会等埋点。永远敢于给出结论。”", "携程机票埋点随着业务复杂度的增加而在做加法，先后上的埋点包括ctm、action、trace、pv、服务端埋点等五个大类，每个埋点均符合其时代属性，但现在规整起来其相互间存在一定的交叉，即使冗余但有些埋点一部分还存在价值，转移起来造成的数据问题谁都不想背锅，所以埋点一直在做加法。直至在app减size的大趋势下，才顺便把无效的埋点做部分清理。", "接下来介绍的埋点在携程机票有其存在的意义，但并不代表是全局最优，如果刚开始埋点的童鞋，可以参考下面各埋点的优劣势，结合自身的需要来取其糟粕取其精华。", "全称User Behavior Tracking system，是由携程首席科学家叶亚明（Eric Ye，前携程CTO）先生发起的一套数据框架，最早从online的埋点落入和上传机制体系开始，逐步扩展至无线端app/hybrid/h5，后又增加abtest体系，现在支持携程的众多分析项目。包括数据埋点的格式、上送契约、落库、ETL以及最后的报表数据，是数据体系的总称，本文主要对于ubt体系在携程机票的埋点应用以及指标应用做说明。", "客户端埋点：ctm，action，trace，pv", "玩过GA(Google Analytics)的童鞋对utm埋点肯定不陌生，它以get方式记录页面来源，被广泛使用营销活动的收益结算，Ctripのutm即为ctm，主要用于online和h5平台，不仅对落地页面的uv评估，同时需要根据规则计算其转化。因ctm只向后传递一次，未能直接关联创单（或者hybrid页面带到native页面面临中断）。", "以机票特价页面为例，通常会根据同一天访问过特价首页的vid、sid来关联同一session的下单行为，且下单时间在页面访问时间之后，记为间接订单；在此基础上再限制访问特价页面的出发到达城市(从url截取)与订单对应，并限制下单前至最近一次访问特价页面之间未再次到访过首页（排除看完特价页面后从首页主流程正常下单的情况），记为直接订单。间接订单的含义是计算特价页面影响的用户下单意愿的程度，而直接订单是计算从特价页面而下单情况。正常情况下都是以直接订单为主要指标，间接订单作为参考指标。", "PV埋点因存在时间最久、埋点方式最简单（调用logpage方法发送pageid），所以被接受程度最高，同时也作为新上埋点验证丢失率的基石数据。app页面实现方式有native/hybrid/rn,其均申请独立pageid，对于计算页面性能影响不大（除了停留时间）。", "点击埋点平台区别较大，按照native、hybrid、online顺序说明", "\n", "\noonline埋点采用比较节省流量的方式，即在页面离开（包括进入下一个页面和当前页面刷新）的时候，将页面上所有的点击信息以{点击名称:点击数量}的json格式发送，这样可以节省流量，但是对于orderid等的记录就会缺失，如果增加额外信息需要改变结构，有利有弊。", "bi分析人员希望每个埋点都可以从开始带到创单，这样计算转化率就会比较方便；但开发认为每个页面埋点重复劳动、浪费时间和精力，而且有可能会影响页面加载速度。为了解决这个问题，推出了trace埋点，这个埋点的特点是每个主流程页面仅有一个，但将所有的业务信息记录在案。", "机票OTA承担航司很多政策任务，会在列表及中间页通过标签的形式来给客户不同产品体验，但这些政策标签能够带来多少销量的提升，以及如何决定其之间的相互影响，成为一个课题。于是在服务端从列表页开始，将所有的显示报文埋点记录下来。", "对于所有产品可以根据政策维度和航司维度进行筛选，通过展示转化比来观测各个阶段的转化，同时对于后台对应的政策业务人员可以发送针对性报表，各取所需，节省大量时间。后期将利用机器学习方法针对不同政策、价格和排序的相互关系进行测算，希望找到最优转化的显示方式。", "携程机票的埋点体系基本如上所列，能够清楚明白每种埋点的优劣势对于分析问题选用数据的时候非常有益。通过埋点反映出来的指标，尤其是二次计算指标，很多在网络上已经有详细定义和说明，我将就结合携程机票的应用以及复盘过程中的思考做一下说明，希望能有所启发。", "关联需要注意的是，不同的埋点的缺失率是不相同的，以下的关联准则是经过作者在部门实践中的反复验证所得，不一定具备普适性。", "公司的pv表的存在时间最久，而且埋点最简单，结构最稳定，所有的验证数据都是以pv表的数据为基准。经过验证下来，根据按天计算的uv数据，trace的埋点准确率在97%左右，服务端的埋点在103%左右。如果都是在同一类埋点的情况下计算转化率，分子分母是每个页面的uv，影响不大，但是跨埋点计算的时候，需要特别小心。在数量级明确之后，还存在数据格式的问题，尤其是string和int的转化，特殊字符造成的解析困难等，这些都需要在使用过程中不断验证，bi和开发相互磨合。", "埋点的准确率受很多因素的影响，主要是不畅沟通带来的各方gap，最后体现在开发对埋点的重视程度不足。每个开发对于埋点的认识不同，对于埋点上送的逻辑也不尽相同，再加上心态不同可能导致结果也会差别很大。", "hybrid页面曾遇到过只要是手指划过按钮埋点就被触发，导致新页面上线后点击数据异常暴增，其实是开发在判断触发事件的阈值设置错误，停留时间超过200ms以上才算点击，小于200ms算滑动，但是在上面那个例子中开发未做限制，导致问题。", "：在一个新埋点上线后，发现一个毫不相关的点击数据下降明显，从业务上找不到原因，后来开发查找代码的时候发现，两个埋点的上送逻辑存在ifelse关系，只有一个被上传。", "：这主要存在于开发交接时候对于埋点的上送逻辑一般不太重视，所以在业务发生变化的时候，并没有及时更改埋点的逻辑，比如pm希望某个默认埋点的默认显示被记录，最早是由服务端直接下发，客户端不做筛选，所以客户端买点直接读取服务端下发内容，但一段时间后默认逻辑在客户端加一层个性化接口，埋点方式还是直接读取服务端内容，未做更改，导致数据一直异常，经过好长时间的努力才定位问题。", "：有时候部门开发逻辑做的很完整，但是被框架的一些逻辑所限制，被背黑锅。比如为了优化速度，hybrid页面在本地app打包的过程中有些文件已经放入，在hybrid请求的时候，有些文件优先以本地为主，而公共框架部门做了一些拦截，但业务的开发可能就存在没考虑到这层逻辑，埋点数据就会全部丢失。", "\n", "\n", "在开发本身的任务都很重的情况下，埋点相对次要，在不了解其意义的情况下，往往意愿不强，怨声载道。这就需要pm或者bi很清楚地知道哪些埋点数据一定要有，哪些是可有可无，同时在整个项目上的最终数据表现上跟开发童鞋分享数据，强化埋点的价值。另外对于开发童鞋本身比较关注的kpi，如页面性能埋点，包括报错信息、加载时间、白屏等，可以辅助其建立报表来增强对数据的关注度。", "这是个历史性的难题，因为在分析问题的时候，维度在不断地细致化，而这些维度是在当初并没有想到的、或者说可能认为没有必要的埋点（没有必要的埋点不增加开发的工作量），但是问题发生之后就需要增加埋点，这也是需要与开发保持密切的沟通。", "该页面与pvid+1下一页面的starttime之差，计算方式一般采用中位数（规避异常值影响整体表现）。", "首次搜索->下单时间、末次搜索->下单时间，反映用户决策时长的两个指标，计算方式为同一session。但机票的购买决策时间比较长，从起意到最后下单在一个session完成的比例比较低，未来考虑在跨session的情况下计算其时间，尽量接近真实的停留时间。", "：停留时长的计算是利用pvid+1的页面与本页面的访问时间差来计算的（艾瑞在online端的访问时间是duration，表示激活时间，能够实际表示当前页面的停留时间），而如果native和内嵌hybrid（已申请pageid）先后加载的时候，填写页的停留时间其实就变成hybrid页面starttime-native页面的starttime，这中间的时间差其实是两页面加载的时间差，并不是用户真实的停留时间。", "对于携程机票的电商网站来讲，", "比如同样是填写页的停留时间变短，在填写页之后的转化率上升的情况下，可以理解为该页面让用户非常放心，用户需要填写和核对的信息很少，对携程的网站非常熟悉和自信，下单迅速，这是一件正向的事情；", "而如果是填写页之后的转化率下降，就有可能是页面冗余信息很多，用户想关注的信息没有找到，或者造成用户反感的信息非常醒目，导致用户立即离开而没有下单，这就成为一件棘手的事情。结合业务可能会找到很多原因，但有一点可以肯定，单纯追求停留时间的上升或者下降是没有意义的，TA需要核心指标一起来定位原因。", "可以建立每个用户的行为流表，方便pm根据uid、手机号等常用字段可以搜索到用户的页面和点击行为流，方便查找问题以及找到问题解决的灵感。因为解决问题是从特殊到一般的过程，可以通过行为流找到灵感，然后用sql来验证是否具有普适性，分析能力螺旋式上升的过程。", "在埋点新上线测试环境进行对比，实际看到埋点的数据格式是否符合预期。", "设备标识字段，vid应用于online和h5（受浏览器和cookie限制，换浏览器或清除cookie之后会更新vid），clientcode代表app/native和hybrid（仅与设备有关，在不换设备的情况下标识唯一）", "sessionid，又称为会话id，以online为例，同一设备访问同一网站30min之内为同一session。", "：pv的计数，同一用户在一天之内的携程页面访问pv从1开始标识，记录该人当天内的所有访问页面的先后顺序，再根据30min来切session。", "：事件发生时间，在pv表指页面浏览时间，在action表指按钮触发时间。", "在考虑行为的时候都用设备号来标识，在考虑订单的额时候都用uid来标识。", "携程市值从2014年的60亿到2016年的140亿美金，其中2016年机票的贡献利润超过40%，快速发展的业务必然需要大量的数据作支撑来推进整体向前发展，而发展的问题需要通过发展来解决。整体埋点体系其实比较复杂，其中的困难也不必多说。在整体趋势下，我一直秉承两个原则：", "用户产生交互的埋点一般放在前端，因为客户端离用户最近，且有些逻辑是放在客户端，点击后不一定都会发送服务；而服务端是以展示为主，可以记录整个下发的报文数据，详细分析显示转化比。", "概念非常复杂时，将相似的概念放在一起对比理解，防止概念混淆，比如退出率和跳出率；UV数/会话数/PV数；回购率和回访率等。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75791"]},
{"module": ["干货教程"], "note": ["\n文|魅族技术团队\n背景介绍以及总体规划\n首先我先介绍一下魅族大数据上云的背景，即我们为什么要上云？\n在开始之前我们默认今天参与直播的各位同学对Hadoop相关技术和Docker都有一定的了解，..."], "title": ["魅族大数据上云之路"], "content": ["文|魅族技术团队", "首先我先介绍一下魅族大数据上云的背景，即我们为什么要上云？", "在开始之前我们默认今天参与直播的各位同学对", "都有一定的了解，另外以下提到Hadoop是泛指目前魅族大数据使用的Hadoop生态圈技术，资源除特别说明则泛指存储资源、计算资源和网络资源的总和。", "我们先来看一下魅族大数据在没有上云的时候所遇到的主要问题有以下几个方面：", "基于以上这些存在的问题，我们经过一番技术预研发现，上云之后可以很好的解决我们的问题。", "在讨论上云的总体规划之前，我觉得有必要先把几个非常重要但是却容易混淆的概念先做一下简单解释，这里只是点到为止，希望这对大家理解后面的内容会很有帮助", "很早就推出了内核虚拟化技术LXC，Docker是有Docker公司研发的，它把LXC做了进一步的封装（现在已经替换成了它自己实现的libcontainer，加上镜像管理等一系列功能，变成了一套完整、易用的容器引擎。2015年的dockerCon大会上，docker和CoreOS共同推出了Open Container Project 来统一容器的定义和实现标准）。", "这里提一些圈儿内的轶事给大家提供点儿谈资：", "刚才提到的OCP（Open Container Project）的建立，google才是背后的真正推手，因为当年Docker的快速发展，打败了google曾经的开源容器技术lmctfy，Docker公司和CoreOS原本和睦，共同发展Docker技术，后来由于意见上的分歧，两家都想做容器技术的标准制定者，google暗中支持CoreOS，后来CoreOS随即自立门户，发布了自己的容器技术Rocket，并且和google的kubernetes合作发布了容器服务平台Tectonic，Docker公司和CoreOS由此彻底决裂。后来Linux基金会出面调和，google也从中协调，双方都退让了一步，才最终和解推出了OCP，但是有心人可以看一下OCP项目的成员名单就知道，Dcoker公司在这中间只占很小的一部分，google在其中扮演了重要角色。此外Docker公司也放弃了自己对Docker技术的独家控制权，做为回报Docker的容器实现被确定为OCP的新标准，但源代码都必须提交给OCP委员会。不难看出google实际上是为了报当年lmctfy的一箭之仇，借CoreOS之手狠踩了Docker公司一脚，自己也成为了容器技术领域的实际控制者", "总结下来Docker只是众多容器技术中的一种，只是由于它最近的火爆而几就成了容器技术的代名词。", "容器技术只是实现虚拟化的一种轻量级的解决方案，常见的虚拟化方案还包括", "KVM、Xen和vmware的解决方案等。", "虚拟化技术只是一个完整云服务体系中包含的一个技术手段，但不是全部，甚至不是必须的。完全使用物理机也可以搭建一个云。", "常见的云服务体系中的核心服务还包括：", "（以上的分类方式参考了亚马逊AWS的云服务体系）", "魅族大数据上云的一个整体规划，基本上分为3个大的阶段：", "（这次分享针对PART I的内容）", "魅族大数据目前已经来到第三个阶段，前两个阶段也是我们曾经走过的，由于今天篇幅的限制重点介绍第一部分，hadoop on docker的内容。", "首先来说明一下我们为什么要上docker，通过在Docker 上部署Hadoop，我们可以实现：有效的提升集群部署效率，原本需要运维手工部署的过程，现在可以通过一个命令直接从Docker Registry中pull对应的镜像，一键启动。", "在Docker中运行的应用，在没有主动配置的情况下，基本无法访问宿主机的文件系统，这样可以很好的保护宿主机的文件系统和其他设备。", "资源调控/隔离：Docker可以对应用所需要的资源，如CPU计算资源、内存资源、存储资源、网络带宽等进行调控（资源调控目前只支持容器启动前的一次性配置），不会因为单一任务的一场导致整个hadoop集群挂起。", "：只要源自同一个镜像，hadoop集群中的任何节点都可以保持统一的软件环境，不会由于管理员的人为失误导致集群环境不一致。", "：由于容器节点的轻量级特征，我们可以非常方便的部署一些针对namenode 的HA方案。另外即使遇到极端情况集群整个挂掉，我们也可以快速的重启整个hadoop集群。", "开发者只需要关心代码本身，代码编写完成后提交到Git，之后的构建，测试、build docker 镜像，提交镜像到docker registry都可以由jenkins自动化完成，运维只需要负责在合适的机器上启动对应镜像的服务即可。", "目前魅族大数据已经经历过docker方案的预研和实施，并且在线下测试和开发环境中已经开始使用docker驱动DevOps提高开发->测试的效率，目前在测试环境中docker宿主机集群规模50+，容器数1000+。", "在这个过程中，我们也经历了", "，docker的build ship run理念看起来很美，但是实际使用起来如果只有docker还是会面临很多的问题和挑战。", "具体的原因我分了以下几个方面来介绍，这些也是我", "。", "资源的隔离其实使我们在使用docker之前面临的一个大问题，经常会由于ETL同学不是特别注意写了一些有问题的任务脚本，上线后导致整个hadoop集群资源耗尽，关键任务出现延迟甚至失败，运维的同学每天被这种问题折腾的死去活来。", "而采用了docker之后，Docker通过libcontainer调用linux内核的相关技术（现在windows和其他操作系统也开始native支持容器技术了，具体进展大家可以关注Open ContainerProject和docker的官网）实现了容器间的资源隔离，资源的分配可以在docker容器启动时通过启动参数来实现，这里举几个简单的例子：", "docker run-it –rm stress –cpu 4", "–cpu代表指定CPU的配额参数，4代表配额的值，这里的配额可以简单理解为CPU的占用率，比如一个宿主机中有两个容器在运行时都指定了—cpu 4，则相当于运行时这两个容器各站CPU配额的50%，注意这里的配额并不是一个严格的限制，在刚才的例子中，如果容器1完全空闲，则容器2是可以完全占用100%的CPU的，当容器1开始使用CPU时，则容器2的CPU配额才会逐步下降，直到两个容器都各站50%为止。", "：docker run-it –rm -m 128m", "-m参数指定了内存限制的大小，注意这里的值指的只是物理内存空间的限制，docker在运行容器的时候会分配一个和指定物理内存空间一样大的swap空间，也就是说，应用在运行期占用内存达到了-m参数指定值的两倍时，才会出现内存不足的情况。", "Docker的资源限制目前必须在容器启动时手工完成，但是在大规模集群的环境下，手工逐一根据物理机的资源情况进行配额的指定显然是不现实的。而且docker并不支持存储空间的配额限制，只能限制读写的速率。", "Docker现阶段还不能满足需求，我们需要", "我们经过实践的检验，发现Docker的资源隔离相比于传统虚拟机技术还是有很多做的不彻底的地方，除了上面所属的之外，在安全上也一直存在一些比较大的隐患，比如docker一直以来都不支持user namespace。", "所以如果容器内的应用使用root账号启动，而启动时又带上—priviledge参数的话，会导致容器内的root和宿主机的root权限一直，带来非常大的安全隐患。在这里稍微补充一点，在最新的1.10版本的docker中已经支持了user namespace（通过–userns-remap参数）。", "但是魅族大数据并没有升级到这一版，这也是由于我们的服务编排和容器集群管理使用的是google开源的kubernetes，目前最新版的kubernetes1.2官方声明最新支持的docker版本只到1.9，有关kubernetes的内容有机会会在后续的分享中跟大家交流。", "定义存储结构的目的是为了实现部署环境的标准化，之前我们也尝试过直接提供一个带SSH服务的容器节点，这样感觉用起来跟虚拟机是没什么差别，但是这样一来之前虚拟机运维时碰到的各种环境不一致的问题还是会出现，docker的多层镜像结构的优势就完全没有发挥出来。", "通过一个base镜像定义好一套标准，这就像maven的parent pom一样（这个比喻有些不够恰当，maven更多的是进行包依赖的管理，而docker的多层镜像存储结构可以定义一个完整的基础运行环境，并且这个环境是可以继承的），可以给大家提供一个标准的环境基础设置，另外上层应用非常方便的可以收集容器中的数据，使用容器的服务。", "首先我们按照部署模型划分镜像结构，对每一个Hadoop生态中的服务都抽象出一个对应的镜像，这个服务镜像包含了主从架构的两套配置，使用环境变量来区分该服务镜像到底是作为master还是slave部署。", "这样做的好处是可以降低镜像的数量，方便做配置和服务版本的管理和升级，另外需要注意的是mz agent，这是我们自定义的一个镜像，用来进行临时文件的清理、日志数据收集，以及对接第三方的监控服务实现，向我们的监控平台发送自定义的一些集群健康度指标数据。", "此外它还负责接收客户端的管理指令请求，可以让集群管理员像操作一台服务器一样的对整个集群或集群中的特定主机进行操作。", "为了方便做统一的监控和存储的管理，我们在镜像内定义了一些基本的目录结构", "这样定义镜像结构可以帮助我们通过统一的标准更好的进行监控、报警、发布管理等相应特性的实现。上层的平台应用只需要将一些必要的实现诸如到base镜像中，就可以实现统一的运行环境升级。", "这里会存在一个问题，docker容器在运行时产生的数据是保存在宿主机的特定目录的，而且这些数据的生命周期跟容器的生命周期一样，一但容器销毁，则这些数据也会一并删除，如果将HDFS部署在容器中，这也是不能容忍的。", "我们如果通过volume的方式将数据挂载到宿主机则会导致宿主机中存在大量跟容器的ID相关的存储目录，而且一旦节点挂掉想要在其他节点上启动相同的服务还需要进行数据的迁移。大规模集群的环境下在每个节点上管理这些目录会带来额外的运维成本。", "为了解决这一问题，我们", "。", "由于hadoop生态圈技术的特点，docker必须部署在分布式的环境中，而这会带来很多新的挑战。如果有熟悉docker的同学就会发现一个问题，HDFS默认会将数据做3个副本。", "但docker可以在同一个宿主机中启动多个容器，如果这三个副本都落在了同一个宿主机上，那么一但宿主机down掉，三个副本就一起失效，HDFS的这种利用多副本来提高数据可用性的策略就无效了。", "不仅是HDFS，其他需要通过类似双master或多副本的结构来做HA的方案都会遇到同样的问题。", "在只使用docker的情况下，只能通过人工的方式，将namenode和datanode分开在不同的宿主机上启动。", "这样一来，一但某个容器内的服务挂掉，为了保持有足够数量的服务中容器，则需要运维的同学人工的通过监控平台去检查集群目前各宿主机的负载和健康度情况，同时结合不同容器服务的部署架构要求选择合适的宿主机启动容器。", "虽然最后部署的动作已经相对自动化，但是选择部署服务器的过程依然是痛苦的。", "再补充一点，如果希望升级Hadoop生态中的某个服务的版本，一般都是先在测试环境进行部署验证，ok之后才会将镜像发送到线上的Registry，这就涉及到了如何隔离测试和线上生产环境的问题，不同环境的存储、计算、网络资源必须相对隔离。", "如果是在运行中的集群中更新服务的版本则还会涉及到灰度发布的问题。如果线上集群已经存在了多个版本的服务，则还要考虑个版本升级过程是否有差异，是否需要进行滚动发布等，这些docker暂时都不能帮我们解决，仅仅依靠人工或者自己搭建平台来完成依然需要很大的工作量。", "这也就引出了Docker仅仅作为一个容器引擎的不足，如果需要大规模的部署，我们", "另外我们遇到的一个无法忽略的就是多宿主机环境下的网络问题，既然要在集群环境下运行，那么如何在多宿主机的环境下让各台宿主机上运行的容器都能分配到一个指定网段内的IP，并且彼此之间可以进行正常网络通信。", "由于大数据相关的应用和服务往往需要进行大量的网络吞吐，这一方案在性能上也不能有很大的损失。", "Docker自带的网络模块如上图所示，通过一个虚拟化的docker0网桥，把主机上的eth0端口和docker engine内部的container通过veth pair连接到一起，容器间的网络通讯都是通过docker0网桥来转发，容器带外网的请求是通过docker0网桥来进行NAT转换后发送到外网，这样的网络方案能够在单一宿主机上解决网络通信的问题。", "但一旦进入多宿主机的集群环境，各个的宿主机上运行的容器就无法得知彼此的IP网段和路由信息。", "在Docker的技术体系中，解决这一问题目前主流的方案有以下几种：", "这几种解决方案的优缺点和适用场景涉及内容很多，有兴趣的同学可以参考一下这篇文章。", "我们采用了上述的最后一种calico，主要是出于性能上的考虑，calico是一个纯layer 3 的解决方案，使用了一个etcd作为外部配置信息存储，通过BGP广播路由表配置，直接用主机的静态路由表定义路由，整个过程没有NAT，没有Tunnel封包拆包过程。", "使得数据包的体积减小，降低了网络开销，所以只要理论上layer3可达的网络环境，同时对协议支持的要求不是太苛刻（由于是layer3 的解决方案，所以只支持TCP、UDP、ICMP协议）同时对性能、吞吐量、网络延迟有要求的场景都是适合的。当然其他的解决方案也有他们的适用场景，具体大家可以参考上面的那篇文章。", "今天由于时间关系没办法把Hadoop on Docker方案的方方面面都做详细的介绍，因此主要选了一些重点的问题和挑战来跟大家分享，主要的内容就到这里了，但是还有一些上面提到的遗留问题没有深入下去：", "如何搭建一个存储管理的服务来托管容器运行时生成的数据，使得Hadoop集群能够随时创建，随时销毁。集群内的应用服务不需要关心运行时生成的数据存放在哪里，如何管理等等。", "如何搭建一个容器调度管理服务，来按照我们指定的策略分配容器运行的实际宿主机。同时能够根据预定义的发布更新策略，对运行中的容器服务进行动态的滚动更新（rolling update）。", "网络方面，如何在一个多集群、多租户的环境下有效的管理和分配网络资源，让各集群之间相对隔离，又有统一的网关可以彼此访问。", "如何对一个复杂的微服务环境进行编排和管理。", "这些都是构建一个完善云服务所不能避免的问题，docker自身也深知这些问题，如果细心关注docker的官方网站就会看到，他们也在通过docker-machine、docker-compose、docker-swarm等意图为docker打造一个完整的解决方案，使其拜托最初人们认为docker只是一个容器引擎这样的印象。", "目前已经有30%的任务量运行在这套容器云平台下，线下、测试、正式三套环境通过registry配合kubennetes的跨namespace调度实现任务的快速发布和更新。", "除了hadoop集群之外，我们自己的平台应用也正在逐步的迁移到这套环境当中。但为了保证线上任务的稳定性，hadoop集群和平台应用集群是namespace和宿主机双隔离的。", "今天的内容都是以单纯的docker为主，其他几块的内容以后可以在Hadoop on Container Cluster Management System和Hadoop on Cloud Operating System当中再跟大家逐一介绍。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76014"]},
{"module": ["干货教程"], "note": ["\n文 | 联想大数据（本文由联想大数据团队独家投稿36大数据，新年新开始，拒绝转载!）\n随着大数据平台型产品方向的深入应用实践和Docker开源社区的逐渐成熟，业界有不少的大数据研发团队开始..."], "title": ["大数据平台Docker应用之路"], "content": ["文 | 联想大数据", "随着大数据平台型产品方向的深入应用实践和Docker开源社区的逐渐成熟，业界有不少的大数据研发团队开始拥抱Docker。简单来说，Docker会让Hadoop平台部署更加简单快捷、让研发和测试团队集成交付更加敏捷高效、让产线环境的运维更加有质量保障，而这背后的业务场景和具体的实践方法有哪些？在Docker容器服务逐步走向完善的过程中，大数据平台产品Docker模式的应用又是如何解决的？正是本文所着重阐述的。", "在大数据平台型产品的开发过程中，经常要跟许多模块打交道，包括Hadoop、HBase、Hive、Spark、Sqoop、Solr、Zookeeper……等多达几十个开源组件，为了不影响团队成员间的工作任务协同，开发人员其实非常需要自己有一套独立的集群环境，以便反复测试自己负责的模块，可真实的企业开发环境往往只有一两个大的虚拟集群，这可怎么办？难道要给每个开发人员都配几台独立的物理机器？", "针对每一次新版本的发布，产品测试组都需要反复的重装整个平台以便发现问题，而正如本文前面所阐述的那样，大数据平台所依赖的组件繁多，不同组件模块依赖的底层库也不尽相同，经常会出现各种依赖冲突问题，而一旦安装完成，就很难再让Linux系统恢复到一个非常干净的状态，通过Remove、UnInstall、rpm -e等手动方式卸载，往往需要花费很长的时间，那如何才能快速地恢复大数据平台集群的系统环境？", "当测试人员在测试大数据平台过程中发现了一个Bug，需要保存现场，这里面包括相关的大数据组件配置、进程状态、运行日志、还有一些中间数据，可是，平台集群服务器节点数量很多，针对每个进程的配置目录和日志文件，都相对较独立，一般都需要专业的开发工程师或者运维工程师进入相关服务器节点，按照不同组件的个性化配置信息，手工方式收集所需的各个条目信息，然后打包汇集到日志中心服务器进行统一分析，而目前业界并没有一款能够自动分布式收集故障相关的日志系统，但测试工作还要继续，怎么办？", "如何把一个部署好的大数据平台快速地迁移到其它地方？", "你得注意以下几点：", "1，如果是关键业务系统，数据不能丢；", "2，如果是迁移物理机，机器可能会坏；", "3，如果是不间断实时在线业务，要保证快速平稳切换。", "想要解决这些问题，第一个想到的方案当然是用虚拟机，而笔者经历的团队，之前也确实用的就是虚拟机，但这种方式并不能完美的解决以上问题，比如：", "1.虽然虚拟机也可以完成系统环境的迁移，但这并不是它所擅长的，不够灵活，很笨重。", "2.虚拟机的快照可以保存当前的状态，但要恢复回去，就得把当前正在运行的虚拟机关闭，所以并不适合频繁保存当前状态的业务场景。", "3.虽然可以给每个人都分配几个虚拟机用，但它是一个完整的系统，本身需要较多的资源，底层物理机的资源很快就被用完了，所以我们需要寻找其它方式来弥补这些不足。", "Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案，换句话说，它可以让我们把一台物理机虚拟成多台来使用，而且它还可以保存修改、完整迁移到其它地方、性能损耗小等等好处，能够很好解决我们之前遇到的问题。", "那为什么不用虚拟机方案？", "简单来说，因为它比虚拟机更轻便，启动一个Docker容器只要几秒种的时间，在一台物理机上可以创建几百上千个容器，而虚拟机做不到。", "下面是虚拟机与Docker两种方案的实现原理：", "VM设计图", "虚拟机实现资源隔离的方法是利用独立的OS，并利用Hypervisor虚拟化CPU、内存、IO设备等实现的。例如，为了虚拟CPU，Hypervisor会为每个虚拟的CPU创建一个数据结构，模拟CPU的全部寄存器的值，在适当的时候跟踪并修改这些值。需要指出的是在大多数情况下，虚拟机软件代码是直接跑在硬件上的，而不需要Hypervisor介入。只有在一些权限高的请求下，Guest OS需要运行内核态修改CPU的寄存器数据，Hypervisor会介入，修改并维护虚拟的CPU状态。", "Hypervisor虚拟化内存的方法是创建一个shadow page table。正常的情况下，一个page table可以用来实现从虚拟内存到物理内存的翻译。在虚拟化的情况下，由于所谓的物理内存仍然是虚拟的，因此shadow page table就要做到：虚拟内存->虚拟的物理内存->真正的物理内存。", "Docker设计图", "对比虚拟机实现资源和环境隔离的方案，docker就显得简练很多。docker Engine可以简单看成对Linux的NameSpace、Cgroup、镜像管理文件系统操作的封装。docker并没有和虚拟机一样利用一个完全独立的Guest OS实现环境隔离，它利用的是目前Linux内核本身支持的容器方式实现资源和环境隔离。简单的说，docker利用namespace实现系统环境的隔离；利用Cgroup实现资源限制；利用镜像实现根目录环境的隔离。", "当新建一个容器时，docker不需要和虚拟机一样重新加载操作系统内核。我们知道，引导、加载操作系统内核是一个比较费时费资源的过程，当新建一个虚拟机时，虚拟机软件需要加载Guest OS，这个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统，则省略了这个过程，因此新建一个docker容器只需要几秒钟。另外，现代操作系统是复杂的系统，在一台物理机上新增加一个操作系统的资源开销是比较大的，因此，docker对比虚拟机在资源消耗上也占有比较大的优势。事实上，在一台物理机上我们可以很容易建立成百上千的容器，而只能建立几个虚拟机。", "可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。当然，一些容器核心模块依赖于高版本内核，存在部分版本兼容问题。", "在实践过程中，部署一套可用的大数据平台Docker环境，必需做好以下前提工作：", "1.搭建私有镜像仓库，用来统一存放构建好的镜像文件", "2.搭建一个安装包仓库，用来存放我们发布的各种版本的大数据组件安装包", "3.配置多个物理机上的Dcoker容器可以相互通信，可参考官方给出的方案", "1.既然要在Docker容器内安装我们的大数据平台，那就需要一个统一的Linux系统做为我们的Dcoker容器，像Ubuntu、CentOS等发行商都会发布自己的Docker基础镜像到Docker Hub上，如果Docker Hub上恰好没有你需要的镜像，也可以自己制作。", "2.比如用CentOS6.8做为我们的基础镜像，那么请先把它pull下来", "3.然后我们用这个镜像创建一个容器，并在里面配置一些我们大数据平台依赖的参数，比如ntpd、httpd服务等等，最终生成我们平台专属的基础镜像。", "4.这是很关键的一步，有了它以后，所有人员可以随时创建一个自己需要的Linux环境出来，以便在其内进行产品的研究和实验，且每个人的环境互不相干，当容器内的环境被破坏后，可以删掉再创建，这样一来，场景一和场景二所遇到的问题也就迎刃而解。", "我们可以把已经部署了集群的容器保存成多种镜像，如：只包含了Hadoop的集群、同时包含Hadoop、Zookeeper、Hbase的集群，或安装了所有组件的集群等等，然后上传到私有仓库，其它人需要的时候，直接启动自已需要的集群就可以了，因为免去了部暑与配置等步骤，因而大幅度提高了工作效率，也提高了产品迭代速度。", "上图是已经做好的镜像，图中共三种类型的镜像：", "Docker提供了commit功能可以将一个正在运行的容器保存起来，假如在测试过程中遇到一个Bug并且需要先保存下来，执行一条简单的命令即可，如：", "# docker commit container_name image:v2", "在以后需要复现的时候用这个镜像创建容器即可，像下面这样", "# docker run -tid –name c1 image:v2 bash", "但注意，并不是所有状态都能被保存下来，它只保存文件层面的状态，不能保存内存中的状态，所以再次启动容器的时候，容器内的所有服务都已经变成了停止状态，需要再手动启动一次，这样就导致有些类型的Bug不能复现。", "不过欣慰的是，Docker官方打算在后面的版本中加入checkpiont功能，它可以保存容器中的所有状态，这样就可以完整地复现Bug，这个新功能的用法就像下面这样：", "这个功能对很多人来说，绝对是个好消息！", "当然了，每个人都不应该把过多的精力放在怎么使用Docker的问题上，这样会为团队带来额外的工作量，最简单的办法当然是把所有重复性的工作脚本化，向每个人提供最简便的使用接口，只需要一条简单的命令就可以创建自己想要的集群环境，当不需要的时候一条命令即可删除，这样即降低了学习成本又解决了容器管理问题。", "根据笔者的实践经验，脚本化的实现应该着重考虑几个方面：", "现在已经有很多开源的Docker容器管理框架，但需求总是复杂多变的，并不能适用所有的场景。比如笔者所负责的大数据平台就需要为每个容器做端口映射、内含大数据组件的镜像在启动后还需做Hostname与IP映射等，总之，目前开源容器框架的易用性还有很大的改进空间，都存在一些手动配置的工作。", "关于容器服务，在具体的实践过程中，一定还会遇到很多问题，比如服务发现和编排。当下在应用层面虽还算不上特别的成熟，但已经使原本部署与配置很复杂的大数据平台变得简单快速，让一部分研发团队的产品迭代得到加速。当然，不管是大数据平台产品，还是Docker开源社区本身，都还在不断的完善中。", "End.", "相关阅读：", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75999"]},
{"module": ["干货教程"], "note": ["\n文 | 傅一平\n从10年前的数据仓库到当前的大数据平台，ETL也需要与时俱进，这里来谈谈个人的理解，如果你在考虑建设新的企业级ETL平台，可以作为参考。\n一、定位的重新认识\nETL作为传统数据..."], "title": ["我们需要什么样的ETL?"], "content": ["文 | 傅一平", "从10年前的数据仓库到当前的大数据平台，ETL也需要与时俱进，这里来谈谈个人的理解，如果你在考虑建设新的企业级ETL平台，可以作为参考。", "ETL作为传统数据仓库的底层技术组件，主要是服务于数据采集的，因此，一般数据流动往往是单向的，但在新的时期，我们需要拓展其概念的内涵，从ETL升级到交换，以适应更多的应用场景，这是大数据平台规划人员特别需要考虑的。", "但我们看到，在很多企业PaaS平台级的研发中，并未将交换其纳入产品的核心功能，为什么?", "ETL出来之时，的确适应了数据仓库建设的需要，毕竟系统建设之初，数据采集和整合为王， 技术驱动业务，没什么好说的。", "但在大数据时代，需要与时俱进，基于笔者的实践，感觉开放的交换平台将是未来标配，原因有以下几个：", " 随着数据应用的日益丰富，不同平台、系统的相互大批量数据交互成常态，仅仅满足于采集数据已经不适应业务需要，还需要能够为数据的目的端落地提供支撑，", "，而不是只管自己一亩三分地的ETL系统， 比如浙江移动的日常的数据交换应用早就超过了简单的数据采集需求，业务始终为王。", "，", "，不是有谁没谁的问题，我们好不容易搞了PaaS级的ETL，但交换却要考虑用另一个工具实现，同时未来大数据平台组件将异常丰富，相互之间的数据交换将是常态，必须要有个PaaS级的交换工具满足这种要求，这是个趋势性的东西。", "，描述的方式没有本质的区别，我们需要用一种工具实现所有接口的透明化统一管理，显然升级ETL是最好的方案，很多企业采集由于ETL工具存在管的还算可以，但交互的接口管理一塌糊涂，比如繁多的FTP搞晕了运维人员，付出的管理成本很大。", "以下是勾画的一种数据交换平台的功能架构，供参考。", "交换平台除了传统ETL功能， 分布式动态可扩展是必须的，现在云化交换平台产品已经很多了，应该各有千秋吧，特别强调以下几点，：", "，支持对表、文件、消息等多种数据的实时增量数据采集(使用flume、消息队列、OGG等技术)和批量数据分布式采集等能力(SQOOP、FTP VOER HDFS)，比基于传统ETL性能有量级上的提升。", "，包括ORACLE/HIVE/GBASE/IMPALA/ASTER/HBASE等等，要实现这些功能，涉及到互信等众多问题，但对于业务的价值巨大。", "，因为传统ETL可能跟应用无关，统一运维团队配置即可，但交换跟应用强相关，必须要能够授权自主配置，这个时候，多租户管理就变得非常重要。", "，能够对外输出元数据，这个其实是未来对于任何企业级平台的刚性要求，做平台的企业别老想着封闭，包打天下， 比如浙江移动有个统一的数据管理平台，不能由于交换平台的封闭，让数据管理平台废了半条腿，这是企业未来引入技术组件必须考虑的因素。", "，能够提供图形化的开发和维护界面，支持图形化拖拽式开发，免代码编写，降低开发难度，每配置一个数据接口耗时越小越好，比如以前我们采用的老ETL平台一个接口平均配置3小时，这是无法忍受的。", "，实现采集任务的统一调度，可支持Hadoop的多种技术组件(如 MapReduce、Spark 、HIVE)、关系型数据库存储过程、 shell脚本等，支持多种调度策略(时间/接口通知/手工)。", "除了BAT，业内真正能打造这类PaaS级的ETL平台屈指可数，因为要实现此类交换平台综合要求其实非常高，除了技术因素，挑战更多来自于需求理解、开放性及持续服务能力，这是我们在实践中碰到的痛点：", "客户需求的理解往往是硬伤，很多公司技术的确很强，但由于产品是卖给别人的，自己也不会用，其很难达到BAT产品的境界，未来是BAT的，不是说BAT技术有多强，而在于其产品从实践中走出来，在客户需求理解能力上是大多数公司难以项背的，客户大多数时候并不需要你的技术有多牛逼，快速解决问题就行，但此类产品经常陷入拼性能，列功能，强升级的场景，而忽视本质的东西。", "开放性也是很多公司的软肋，随便拿个可视化界面来讲吧， 大多数场景其实需要极简的界面，我们经常哀求能否开放个API出来啊，其他平台无缝集成下行不，但往往无法满足，说不符合产品路线，如果下回有个ETL公司来跟你推销产品，你首先得问一句，能开放元数据接口不?能开放API不?", "服务型公司才是未来，一个产品打天下的时代即将过去，未来是服务的时代，甭跟我提一堆概念，谁都无法预测未来，我更关注当下，既然我找你，你就要做好持续服务的准备，一个合理的优化短则一月，多则1-2年，没有哪个客户有耐心。", "ETL作为企业搞大数据核心的技术平台，在建设或选择的时候，要考虑的东西其实非常多，大多传统企业在这方面的掌控能力是非常欠缺的，很容易陷入建设的怪圈而效益却很难显现，以为搞了云化就OK了，其实仅仅解决了ETL中很小的一个问题，不被忽悠并理解自己真正想要什么其实很难。", "我上面列的那张功能架构图，任何一个点的需求即使要进行确认，投入的精力也是蛮大的， 不全面考虑，死磕到底，最后吃亏的终将是企业自己， 一个小功能的缺失就可能导致ETL的效率的大幅降低，甚至可能推倒重来，留给运维团队的也将是无尽的痛苦。", "当然如果企业的数据量不大，那怎么捣鼓都行，其实大多数企业当前并不需要重型的ETL大炮，但对于每个BI人，从大数据的角度讲，理解它又是有必要的。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76184"]},
{"module": ["干货教程"], "note": ["\n文 | blogchong\n首先，如题，这篇依然是写给那些从其他领域跨界到大数据领域的朋友的，当然，初入大数据领域，或者对大数据领域有所兴趣的朋友，也可一观。\n想起要写这个话题，已经很久了，..."], "title": ["大数据跨界，从这里开始"], "content": ["文 | blogchong", "首先，如题，这篇依然是写给那些从其他领域跨界到大数据领域的朋友的，当然，初入大数据领域，或者对大数据领域有所兴趣的朋友，也可一观。", "想起要写这个话题，已经很久了，直到最近私密群建立起来，才发现这个话题应该是更急切需要被讨论一下的。", "虽然我对于大数据培训市场一直不算太感冒，但是，如果说对于那些想要跨界进入大数据领域的朋友来说，不可否认，全面的培训是最快的方式，最少比自学来的快、更系统。", "但我一直对于大部分培训课程体系的设计不敢苟同，这是重点。", "这两年来，鉴于种种原因，也帮不少初入门或者说初入大数据培训机构的跨界朋友参考过培训课程。", "基本上课程套路都一样一样的：", "首先基本的语言基础来一套，接着是hadoop、mapreduce、hive、hbase、spark、flume、kafka、mahout给来一套，再接着上一系列的算法课程，最后来点套路的所谓项目实战。", "三四个月的课程，好几万的投入，然后就上岗开干了，细微的区别的在于可能不同课程的课时设置不同，但基本上都得来一个如上全家桶套餐。", "先不说其他的，三四个月，得把这个整个“全家桶”给咽下去，还得消化了，简直了。。。", "而在实际的工作中，你能用上其中一到两种算是正常的，能用上三四种的算是少见的，能碰过过半的说明你可以升级为数据架构师了。", "关键是很多人培训完了，依然一脸懵逼，感觉自己不知道能干啥，不知道要干啥，又或者说不知道企业到底需要什么人，而自己到底适不适合。", "好了，吐槽完毕，正文来了。", "我个人认为，给初入大数据领域或者跨界进入大数据领域的朋友灌输大数据相关的知识，第一件事不是说各种组件框架生态相关的东西，也不是各种编程语言基础。", "而是，了解清除以下几个问题：", "1 大数据领域到底包含了哪些东西，解决了哪些问题?", "2 自己的实际基础是什么，如何结合自己的基础以及兴趣爱好，在整个大数据领域链路中，找到最好的切入点。", "只有解决了上面两个问题，才能给自己最精确的定位，找准方向深入下去。", "一个人的精力是有限的，在短短的几个月时间内，你不可能把所有的东西都学的精通，哪怕连熟练都难做到，仅仅是皮毛而已。", "所以，有的放矢，把有限的时间放到该定位的地方上去。", "现在一说起大数据，简单起来就是一个词，但其实这个方向已经可以形成一个技术领域了，包含了方方面面的技术点，也提供了各种不同的技术岗位。", "所以，不同的岗位，对应的需求，工作内容都是不同的。", "我们可以根据数据从下到上，从无到有，到产生价值整个数据业务流程来拆解，并且与此同时，来看看每个环节我们需要的技术储备以及能做的事有哪些。", "数据的几大基本业务流程：", "收集 -> 传输 -> 转换/清洗 ->存储 -> 再加工 -> 挖掘/统计 -> 上层应用输出", "总的来说，可以分以下几个大块。", "在收集阶段，我们来看看数据主要有哪几种存在方式：", "1 第三方开放数据集", "2 业务数据", "3 服务日志", "4 行为上报数据", "首先针对于第三方开放数据，目前爬取第三方开放数据的形式已经逐渐被认可，并且将会越来越多的人以及企业从互联网开放数据集中获取原始数据。", "所以，针对于开放数据的获取，爬虫已经可以单独形成一个体系了，包括不同的爬虫框架，以及近年来对于分布式爬虫的技术需求等，在语言方面主要还是python以及java为主，辅助其他相关脚本知识。", "如果数据是业务数据，那么通常在传统的路子中，业务数据一般存储在传统的数据库中，那么，对于传统数据库相关的技术知识不可避免的需要有所了解，最起码简单的数据库操作得会吧。", "其次，我们需要对数据进行统一化处理，又不可避免的涉及到数据的迁移，即从传统数据存储介质中迁移到诸如hadoop生态中，那么涉及的迁移框架诸如sqoop之类的，又是不能不懂一些。", "在语言以及基础要求上，对SQL相关的知识需要补充，以及linux操作，简单的java需要掌握。", "如果数据是记录在服务日志中，那么，我们需要对linux很熟悉，各种脚本的使用，日志文件的各种操作，熟悉各种sed、awk工具等，如果体量大，我们还需要把这些日志文件丢到分布式框架中进行处理、清洗，诸如mr，spark中等。", "所以，对于这种数据的处理，我们需要掌握的一方面是linux的熟悉操作、另一方面是一些诸如离线数据处理框架的使用，语言方面还是以java、脚本类语言为主。", "最后，", "数据的传输到底在什么时候会涉及到呢?", "诸如上面说到的数据上报，在大数据模式下，通常上报过来的数据我们都不会马上进行落地的，因为涉及到不同部分其效率不一样，在峰值波动的情况下，直接落地十有八九都会导致系统宕机。", "所以，数据的传输在大数据领域中有着不可替代的左右，会出现在各种系统耦合之间，一方面用作数据的传输，另一方面用作数据的缓冲、系统解耦。", "在hadoop生态中，最有名的莫过于kafka与flume的组合搭配了，收集数据，并进行数据传输，此外还有不少类似的消息队列存在，诸如ActiveMQ、阿里的RocketMQ等等。", "在这里，我们需要理解的就是，为什么要引入这么一层组件，因为在过去的数据处理模式中，因为数据量的缘故，这一层相对次要。", "Hadoop生态中最最核心的存储组件莫过于HDFS了，这是支撑hadoop能够做大批量数据处理的基础支撑，便捷而强悍的横向扩展能力。", "除此之外，还有各种基于此之上不同形式的数据存储方式，诸如hive、HBase、甚至ES、Solr勉强都算，以及不可忽略的传统类型的SQL存储方式。", "我们需要理解的是，不同的存储方式应对于实际的应用场景是不同的，HDFS作为最基础的分布式文件系统，我们就不多说。", "诸如，Hive其更作用更多用于类传统式的SQL查询操作，其对于效应效率要求并不高，但对于数据规模的支撑性良好;而HBase则更偏向于即席查询，要求有更高的响应效率，但对于查询的复杂性支持上则相对较弱。", "而我们说诸如ES、Solr都也勉强算是一种数据存储的组织方式，其实也是有一定道理的，因为他们本身也支持这种分布式的数据存储，只不过他们是为了应对于自己框架的检索需求而设计的数据存储组织。", "此外，还有例如Redis，也算是目前大数据生态中不可缺少的数据存储方式之一，基于内容，应对于高效的数据存储与读取，在很多的实际应用场景中都用的到。", "其实这一层主要要说就是基于Hadoop的MR框架以及Spark，当然，也有其他的一些分布式数据处理框架。", "简单的，我们可以通过一些处理脚本来做，但针对于大规模的数据量级，我们依然需要依赖MR或者spark这种框架来处理。", "而针对于一些实时的场景，我们也不可避免的需要掌握诸如storm以及spark streaming之类的实时框架。", "所以，在这一环，我们不止需要了解不同的大数据处理框架，我们还需要在基于框架的基础上，做数据应用开发，进行数据处理。", "通常，在这个环节中，我们需要对于Linux比较熟练，最起码能够熟练的操作服务器，操作不同的框架系统，其次，我们在此基础上做应用开发，脚本以及java语言是必须精通的，如果使用spark等框架，对于scala还是有一定要求的。", "前面我们做了很多事，包括数据的收集、传输、处理、存储等等，但这些都是手段，都不是我们的目的。", "我们的目的是让数据产生价值，这也是企业做大数据的核心目的。", "1 基于统计分析、数据预测，做数据化运营、决策，提升效率、效果，这是最常见的应用场景。", "2 做推荐，在主体之外产生衍生价值，提升单位价值转换。", "3 画像体系，至于说画像能做什么，只要能做的准，能做的事可多了。", "4 基于数据化、智能化的搜索。", "5 实现业务的数据化、自动化、智能化。", "6 …", "在这一环中，包括的东西太多太多，包括大数据培训市场中主流方向：数据分析师，基本属于偏业务分析的路子。", "还有那些培训的算法之类的，也是为这一环服务的(但是，单纯了解算法是什么，个人认为是没啥卵用的)。", "我们先来分析一下关于大数据分析这个方向。", "我一直不认同很多大数据培训机构一直玩命给跨界的朋友培训python、R，认为那就是数据分析师的最最核心的技能了。", "其实不然，在大数据模式下，诸如R这种工具其实本身的局限性已经很大了，我们需要更多能够容纳全量数据分析挖掘的工具，而不是抽样。", "哪怕是你把Hive用的很溜，对于统计分析的理论，以及对于业务的理解能力很强，我认为可以完爆那些传统分析工具用的溜的人，最起码在这个领域中，这种需求会更多。", "所以，假设大数据培训机构想要培训数据分析师，尽量还是往大数据模式下的数据分析路子去走，及时培训R之类的工具，可以结合Hadoop-R、Spark-R之类的来做培训，而Hive这种工具更是不可缺少的，此外，对于统计原理之类的理论知识也需要进行额外的补充。", "最后，对于算法来说，单纯的培训算法其实没啥卵用，一方面本身算法这东西基础要求略高，单纯的从培训的角度来说，了解一个算法是什么是次要的，更重要的是要贯穿实际的业务场景与算法模型的映射，以及各种分布式机器学习库的使用，这就够了。", "单纯的去研究算法，别扯了，太不切实际，学完了妥妥的还是一脸懵逼，如果不把实际业务场景结合起来。", "其实如果你对第一个问题，整个流程有足够熟悉的情况下，这事就好办多了。", "清楚每个节点需要哪些技术储备，这个节点到底负责哪些事务，在整个数据生态中起到什么样的地位。", "结合自己已有的基础储备，去衡量如果自己想要涉足某一个节点，需要额外补充多少的知识才能支撑的起工作需求。", "然后，进行选择，然后有所偏重的去理顺自己的技术结构，额外去深入掌握相关的技术。", "最起码你要知道，你在学习spark，你要知道spark在整个数据生态中、实际的业务中，是做什么的，是不是可替代的。", "所以，当你看到招聘网站上各种大数据相关的JD时，诸如：大数据开发工程师、大数据工程师、hadoop工程师、数据开发工程师、数据分析师、大数据分析师、数据挖掘工程师、算法工程师、ETL工程师等等。", "你需要能够做出分辨，这些岗位到底是属于什么定位，是偏平台搭建、是偏数据架构、是偏数据处理、是偏业务分析、是偏数据业务挖掘、是偏算法研究等等，结合自己掌握的技术点，才能做更好的选择。", "当然，其中的门门道道很多，一篇也说不清楚，但这篇文章的主要目的是说，我们需要对于大局有所了解，知道是什么，想要获得什么，知道将要干什么。", "而不是闷头把“Hadoop全家桶”来一套，要知道，技术框架这东西是很容易被替代的，尤其是大数据领域，相关技术迭代太快，所以我们还是需要结合实际的业务来理解大数据，以及掌握快速学习的能力，这才是正道。", "关于选择方向这里，做一点补充，针对于哪些跨界想进入数据挖掘或者算法领域的朋友。", "个人认为如果你想进入大数据领域从事数据挖掘相关的工作，最好建议有以下两种基础最好，有其一即可：", "1 有良好的算法理论基础，通常是需要相对较好学历以及对口的在校研究方向。", "2 在大数据处理领域有足够的数据处理经验，对业务场景足够清楚，对分布式框架和工具能够熟练使用。", "对于前者，工程化能力可能相对较弱，但可以专注于算法研究;对于后者，则可以偏重业务，注重如何将实际的业务问题转换为算法模型问题。", "两者侧重点不同，一个明确模型，研究是模型更加契合业务的问题，研究的是如何提升已知问题的精准问题;另一个是如何将未知业务映射成已知的算法数学模型，需要对业务足够了解、敏感，并且能够进行工程化。", "关于这两者的区别，有时间再开单章说道了，这里就不过多细说。总之，对于普通跨界的来说，建议不要选择这种门槛略高的细分方向，因为后续你的找工作风险略高。", "最最后，关于大数据培训的出路，个人建议选择的时候尽量选择能够让你获得实习或者正式工作机会的机构，因为跨界的第一份工作算是个跳板，很重要，也少有选择，所以需要把握住机会，如果有机会留下来，甭管他是不是外包、实习是不是有工资拿。", "这只是一个跳板而已，此后，天高任鸟飞，海阔凭鱼跃，靠自己了，培训机构只是让你有入门的机会而已，所以要把握住。", "所以，培训机构最大的好处是让你有机会进入这个领域，真正的累积需要入门之后在实际的工作中自己把握机会多学习!", "最后，发现个人私密群除了能省事(省自己的事，省群友的事)，还能衍生不少值得探讨的话题，都是与群友一起讨论聊天时衍生出来的，比如：", "话题原文：我经常听一些产品这样说，运用大数据的公司pv要达到100万。", "话题原文：目前大数据在广告、电商，甚至是金融方面都已经有不少的落地点，这两年大数据开始与人力资源市场开始尝试结合。", "下次，打算就这两个话题选个一个说道说道了，或者有其他话题可以讨论的，也可以留言。", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76253"]},
{"module": ["干货教程"], "note": ["\n文 | Mr-Bruce\n在RabbitMQ下的生产消费者模式与订阅发布模式一文中，笔者以“数据接入”和“事件分发”两种场景为例，介绍了如何使用RabbitMQ来设计、实现生产消费者模式与订阅发布模式。\n生产..."], "title": ["Kafka下的生产消费者模式与订阅发布模式"], "content": ["文 | Mr-Bruce", "在RabbitMQ下的生产消费者模式与订阅发布模式一文中，笔者以“数据接入”和“事件分发”两种场景为例，介绍了如何使用RabbitMQ来设计、实现生产消费者模式与订阅发布模式。", "生产消费者模式，指的是由生产者将数据源源不断推送到消息中心，由不同的消费者从消息中心取出数据做自己的处理，在同一类别下，所有消费者拿到的都是同样的数据;订阅发布模式，本质上也是一种生产消费者模式，不同的是，由订阅者首先向消息中心指定自己对哪些数据感兴趣，发布者推送的数据经过消息中心后，每个订阅者拿到的仅仅是自己感兴趣的一组数据。这两种模式是使用消息中间件时最常用的，用于功能解耦和分布式系统间的消息通信。", "本文将继续以“数据接入”和“事件分发”这两个场景为例，来探讨Kafka作为消息系统的应用方法(High Level)。搞清楚Kafka的基本概念和应用方法是进行系统方案设计的前提，编写代码只是具体落地实施，而解决bug和性能调优是系统跑起来之后的事情了。需要指出的是，本文重点是探讨应用方法，具体应用时需要根据自身需求来做调整，没有任何技术方案是万能的。", "为了方便阅读，笔者首先重复一下这两种场景：", "假设有一个用户行为采集系统，负责从App端采集用户点击行为数据。通常会将数据上报和数据处理分离开，即App端通过REST API上报数据，后端拿到数据后放入队列中就立刻返回，而数据处理则另外使用Worker从队列中取出数据来做，如下图所示。", "这样做的好处有：第一，功能分离，上报的API接口不关心数据处理功能，只负责接入数据;第二，数据缓冲，数据上报的速率是不可控的，取决于用户使用频率，采用该模式可以一定程度地缓冲数据;第三，易于扩展，在数据量大时，通过增加数据处理Worker来扩展，提高处理速率。这便是典型的生产消费者模式，数据上报为生产者，数据处理为消费者。", "假设有一个电商系统，那么，用户“收藏”、“下单”、“付款”等行为都是非常重要的事件，通常后端服务在完成相应的功能处理外，还需要在这些事件点上做很多其他处理动作，比如发送短信通知、记录用户积分等等。我们可以将这些额外的处理动作放到每个模块中，但这并不是优雅的实现，不利于功能解耦和代码维护。", "我们需要的是一个事件分发系统，在各个功能模块中将对应的事件发布出来，由对其感兴趣的处理者进行处理。这里涉及两个角色：A对B感兴趣，A是处理者，B是事件，由事件处理器完成二者的绑定，并向消息中心订阅事件。服务模块是后端的业务逻辑服务，在不同的事件点发布事件，事件经过消息中心分发给事件处理器对应的处理者。整个流程如下图所示。这边是典型的订阅发布模式。", "Kafka是一个分布式流数据系统，使用Zookeeper进行集群的管理。与其他消息系统类似，整个系统由生产者、Broker Server和消费者三部分组成，生产者和消费者由开发人员编写，通过API连接到Broker Server进行数据操作。我们重点关注三个概念：", "，是Kafka下消息的类别，类似于RabbitMQ中的Exchange的概念。这是逻辑上的概念，用来区分、隔离不同的消息数据，屏蔽了底层复杂的存储方式。对于大多数人来说，在开发的时候只需要关注数据写入到了哪个topic、从哪个topic取出数据。", "，是Kafka下数据存储的基本单元，这个是物理上的概念。同一个topic的数据，会被分散的存储到多个partition中，这些partition可以在同一台机器上，也可以是在多台机器上，比如下图所示的topic就有4个partition，分散在两台机器上。这种方式在大多数分布式存储中都可以见到，比如MongoDB、Elasticsearch的分片技术，其优势在于：有利于水平扩展，避免单台机器在磁盘空间和性能上的限制，同时可以通过复制来增加数据冗余性，提高容灾能力。为了做到均匀分布，通常partition的数量通常是Broker Server数量的整数倍。", "，同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。同一个topic的数据，会广播给不同的group;同一个group中的worker，只有一个worker能拿到这个数据。换句话说，对于同一个topic，每个group都可以拿到同样的所有数据，但是数据进入group后只能被其中的一个worker消费。group内的worker可以使用多线程或多进程来实现，也可以将进程分散在多台机器上，worker的数量通常不超过partition的数量，且二者最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费(同一group内)。", "搞清楚了Kafka的基本概念后，我们来看如何设计生产消费者模式来实现上述的“数据接入”场景。在下图中，由Producer负责接收前端上报的数据，投递到对应的topic中(这里忽略了Broker Server的细节)，在Consumer端，所有对该数据感兴趣的业务都可以建立自己的group来消费数据，至于group内部开多少个worke来消费完全取决于数据量和业务的实时性要求了。", "再来看“事件分发”的场景，假如我们有“收藏”、“下单”、“付款”三个事件，业务一对“收藏”和“下单”事件感兴趣，而业务二对“下单”和“付款”事件感兴趣，那么我们如何进行事件订阅?不同于RabbitMQ中有数据路由机制(routing key)，可以将感兴趣的事件绑定到自己的Queue上，Kafka只提供了单播和广播的消息模型，无法直接进行消费对象的绑定，所以理论上Kafka是不适合做此种场景下的订阅发布模式的，如果一定要做，有这么几个方案：", "这种方式简单有效，缺点就是每个group都会收到很多自己不感兴趣的垃圾数据。", "这种方式适用于事件个数可以明确评估并且数量较少，如果事件种类很多，会导致topic的数量过多，创建过多的topic和partition则会影响到Kafka的性能，因为Kafka的每个Topic、每个分区都会对应一个物理文件，当Topic数量增加时，消息分散的落盘策略会导致磁盘IO竞争激烈成为瓶颈。", "在Consumer端，每个group可以拿到仅仅是自己感兴趣的数据。这种方式适用于数据量较大、但是Consumer端的消费group有限的情况，否则也会出现上述的topic碎片化的问题。", "，但是不容易控制，应尽量避免。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76292"]},
{"module": ["干货教程"], "note": ["\n文 | 巫银良\n移动互联网应用和大规模社交网络催生了海量的数据分析需求，时空数据作为记录用户和设备在现实世界分布和活跃程度的基础数据，一直为各大互联网电子商务平台和商家所关注。地理..."], "title": ["如何用SAS绘制现代化数字地图"], "content": ["文 | 巫银良", "移动互联网应用和大规模社交网络催生了海量的数据分析需求，时空数据作为记录用户和设备在现实世界分布和活跃程度的基础数据，一直为各大互联网电子商务平台和商家所关注。地理空间数据结合其他业务数据如何被分析利用，以及如何在分析中可视化呈现一直是现代化分析平台的一个重要方向。", "一方面各种地图服务越来越多地集成到应用中，成为应用增强交互的组成部分(比如“附近的服务/人”，甚至连支付宝红包都需要呈现各种方位关系，来增强乐趣)，另一方面在分析行业，如何能更高效方便地绘制各种地图成为一种基本需求。", "SAS作为分析行业的领导者和探索者，早在几十年前就考虑到地图这一基本元素在分析中的存在，因此它很早就在SAS语言中提供了能够绘制地图的能力。", "考虑到SAS并不是地图数据的生产者，只是利用数据。所以，在早些年，尽管SAS提供的地图数据来源多种多样，但SAS花了大量的时间精力来保证用户地图数据的精确性。", "随着现代卫星和测绘技术的成熟以及一些其他原因(比如不再从CIA获得世界数据)，SAS不再维护既往的地图数据，而是和第三方厂商合作来提供能够定期更新的最新数据，这样就不必考虑不同地理坐标系统和地缘政治格局变化带来的基础地理数据更新。", "在传统上，SAS系统默认提供MAPSSAS库和PROC GMAP，PROC GPROJECT， PROC GREMOVE，GEONCODE等若干过程步来支持地图绘制功能。利用这些基础数据和过程步，用户能很容易绘制世界地图，各大洲地图，各国家地区地图。", "从SAS 9.30M2 版本开始，SAS和GFK GeoMarketing合作，提供MPASGFK基础库，它能为用户提供超过240个国家和地区精准的数字邮政代码和行政区划地理数据。Gfk GeoMarketing的数字地图是世界范围内最全最完整的数字地图，坐标系统为WGS84并且定期更新。", "图1", "据调查，MapGfK基础库包括2个世界级(其中一个world_cities为世界城市)，22个洲级，175个国家级6个美国州县的地理数据与对应属性数据。虽然看起来很全，但也并非十全十美，比如我在操作中发现台湾岛没有包括在中国数据内，2010年的北京核心四区合并为两区等也没有被更新数据。", "下面，我们举个最简单的例子，来说明如何用SAS绘制现代化数字地图：", "proc gmap map=mapsgfk.world data=mapsgfk.world;", "id id;", "choro id / nolegend;", "run;quit;", "运行上面几行代码，SAS会在结果窗口中输出如图2结果：", "图2", "如果你需要绘制亚洲或者中国地图，则只需要将上面“world” 改为“asia”或“china”即可。", "proc gmap map=mapsgfk.asia data=mapsgfk.asia;", "id id;", "choro id / nolegend;", "run;quit;", "输出如图3结果：", "图3", "细心的你会发现，亚洲地图确实按照各个国家进行了准确的绘制，但中国地图则看起来黑压压的一片(这其实反映的是俺们大中华确实是地大物博啊~~)，九段线确实包括在内，但地图中竟然没有宝岛台湾。", "不要慌，下面我们介绍如何将缺失的台湾部分和中国地图合并为大中华地图。", "data mytaiwan;", "set mapsgfk.taiwan;", "id2=id1; id1=’CN-83′;", "run;", "data GreatChina;/*合并台湾到大中华*/", "set mapsgfk.china mytaiwan;", "run;", "proc gmap map= GreatChina data= GreatChina;", "id id;", "choro id / nolegend;", "run;quit;", "执行上面的代码，输出如图4结果。注意一下：台湾竟然出现在屏幕正中央(四川盆地)位置。", "图4", "为了将台湾岛移到指定位置，需要在调用PROC GMAP前执行如下代码，对台湾岛的数据根据中国的投影进行变换。", "…", "proc GPROJECT data=GreatChina out=GreatChina LATLON PARMIN=mapsgfk.projparm PARMENTRY=china;", "id id;", "run;", "proc gmap map=…", "输出如图5结果：", "图5", "在实际制作地图时，并不需要这么多的细节数据。因此我们需要将不必要的地区和县的边界删除，然后再调用PROC GMAP绘图。", "proc sort data=GreatChina out=tmpds;", "by ID1;", "run;", "proc gremove data=tmpds out=tmpds;", "by ID1;", "id id;", "run;", "data GreatChina(drop=ID1);", "set tmpds;", "id=ID1;", "run;", "输出如图6结果：", "图6", "为了给各省标注上省名，我们可以利用MAPSGFK库中已有的地图属性数据来绘制标签。此时需要利用系统自带的宏 %annomac 和 %maplabel 来生成描述数据数据。", "data GreatChina_data;", "set mapsgfk.China_attr;", "keep id1 id1name;", "rename id1=id id1name=idname;", "run;", "%annomac;", "%maplabel (GreatChina, GreatChina_data, anno_label, idname, id, font=%str(SimSun), color=black, size=1.5, hsys=3);", "proc gmap map=GreatChina data=GreatChina_data;", "id id;", "choro id / nolegend anno=anno_label;", "run;quit;", "输出如图7结果：", "图7", "我们可以看到，图7的地图显示的是英文名称，而且台湾也没有包括在内?原因是 mapsgfk.China_attr 没有包含台湾的属性数据，需要我们自己创建。另外，我们要使用中文去显示各省的标注怎么办?只需要使用 id1nameU 并且转码即可显示正确的中文。", "data mytaiwan_attr;", "set mapsgfk.taiwan_attr;", "id2=id1; id2name=id1name;", "id1=’CN-83′; id1name=”Taiwan Sheng”; id1nameu= put(‘台湾省’,$uesc200.);", "isoname=’China’;", "drop country ;", "run;", "data GreatChina_data;", "set mapsgfk.China_attr mytaiwan_attr; /*合并台湾省的描述数据*/", "keep id1 id1nameU;", "rename id1=id id1nameU=idname;", "run;", "%annomac;", "%maplabel (GreatChina, GreatChina_data, anno_label, idname, id, font=%str(SimSun), color=black, size=1.5, hsys=3);", "data anno_label; set anno_label; text=unicode(text);run;", "proc gmap map= GreatChina data= GreatChina_data;", "id id;", "choro id / nolegend anno=anno_label;", "run;quit;", "输出如图8结果：", "图8", "虽然在 MAPSGFK 基础库中有很多基础地理数据，但在现实中依然不够用怎么办? 解决方案有两种：", "直接利用实际测量的地理数据创建自定义地图;", "利用谷歌地球导出地球上任何地区/建筑的 KML 数据，然后导入到 SAS 里创建地图。", "例如图9就是利用谷歌地球数据创建的谷歌总部第40号楼的地理数据。(图10为 Google Earth里的样子)", "%MAPIMPORTX(DATAFILE=”test.kml”,out=%str(mymap), ID=201);", "data mymap;set mymap;", "x=long;y=lat;", "run;", "data mymap_data;", "attrib ID length=$15 label=’Districts code’;", "attrib IDNAME length=$55 label=’Districts name’;", "infile datalines dsd;", "input", "ID", "IDNAME", ";", "datalines4;", "201,Google Building 40", ";;;;", "data mymap_data; set mymap_data;", "length my_html $100;", "my_html=’title=’||quote(trim(left(idname)));", "run;", "goptions reset=all;", "goptions hsize=1024pt vsize=768pt;", "ods html;", "%let mymap=mymap;", "%let mymap_data=mymap_data;", "%annomac;", "%maplabel (&mymap, &mymap_data, anno_label, idname, id, font=%str(SimSun), color=WHITE, size=1.5, hsys=3);", "proc gmap map=mymap data=mymap_data;", "id id;", "choro id / nolegend anno=anno_label;", "run;quit;", "ods html close;", "执行上述代码生成的结果如图9：", "图9", "图10 Google Earth里的样子", "在互联网上，有时听见一些人抱怨SAS语言做出的图表不够美观，显得比较粗陋。其实造成这误解的根本是没有掌握SAS强大的特性控制功能和实现的灵活性。", "为了展示SAS在绘制地图方面预留的灵活性和特性控制，下面我将展示若干纯粹利用SAS代码绘制的各种现代化的复杂地图。", "图11 SAS绘制空白中国省图", "图12 SAS绘制的中国各省的卫星地图", "SAS语言天生作为面向分析而设计的语言，它保留了非常多的扩展性;我甚至发现在 SAS 地图里可以绘制天气云图(如图13所示)。正所谓倚天不出，谁与争锋?在分析行业里只有掌握了如何使用SAS这把倚天剑，才能使数据分析结果的展示一切皆有可能!", "图13 SAS 绘制的带有卫星云图的中国分省图", "SAS GMAP 提供2D(choropleth)和3D(block, prism, surface)地图的绘制和渲染，用来将分析变量和结果显示在地图上。", "既往的研究表明，SAS用户可以桥接任何地图服务商的数据，包括 MAPBOX，MAPQUEST, HERE，GOOGLE，ARCGIS和 AutoNavi(高德)的地图和他们的各种变体：SATELLITE(卫星图)，STREETS(街道图)，TERRAIN(地形图)和TRAFFIC(交通图)等。", "PROC GMAP的所有奥秘其实都藏在它的MAP和DATA参数里，至于如何实现，就需要在实际需求中与具体业务数据结合考虑。", "图14 利用SAS代码绘制现代化地图", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76391"]},
{"module": ["干货教程"], "note": ["\n文 | 兜哥带你学安全\n提要\n大多数互联网公司的业务主要依赖在线服务，DDoS攻击作为最简单有效的攻击手段，经常被黑产作为攻击互联网在线服务的首选。本文以甲方的视角介绍下常见的抵御DDoS..."], "title": ["甲方视角谈谈如何抵御DDoS攻击"], "content": ["文 | 兜哥带你学安全", "大多数互联网公司的业务主要依赖在线服务，DDoS攻击作为最简单有效的攻击手段，经常被黑产作为攻击互联网在线服务的首选。本文以甲方的视角介绍下常见的抵御DDoS攻击的手段以及甲方经常遇到的一些问题，希望可以帮助到大家。", "简单讲就是谁火灭谁，前年打P2P，去年打直播，DDoS攻击的背后多是恶性商业竞争，以不大的成本可以让竞争对手业务中断，造成巨大损失，性价比不可谓不高。", "理解了攻击动机后，其实很容易总结何时容易被攻击：", "用安全圈的一句话，在国外打垮一个网站需要一顿自助餐的钱，在国内只需要一顿快餐的钱。随便在某及时通讯软件里面一搜：", "DDoS攻击的类型非常多，但是从甲方角度讲，从结果来说就是两种：", "流量型攻击，就是把你带宽打满，用户无法正常访问导致业务中断，衡量单位是G", "CC攻击，就是把你重要的接口服务打死，流量不一定很大，但是请求速率一般很大，比如登陆，下单，支付等，衡量单位是QPS", "这是个开放性的问题，需要区分不同情况(这里的机房带宽指的是机房给你分配的带宽)：", "如果是流量型攻击，攻击流量小于机房出口带宽时，具备防DDoS攻击能力的硬件防火墙和WAF可以抵御;攻击流量高于机房出口带宽时，只能遗憾了。", "如果是CC攻击，攻击流量小于机房出口带宽时，具备防CC能力的硬件防火墙和WAF可以抵御;攻击流量高于机房出口带宽时，只能遗憾了。", "遗憾的事，相当一部分DDoS攻击轻松上几十G，用户购买的出口带宽上1G的都不多。可见，面对现实中的DDoS攻击时，本地的硬件防火墙和WAF作用有限。", "从甲方角度讲，可以按照抗D设备/服务部署的位置分为：", "云抗D服务和CDN接入原理类似，使用的是所谓替身防护的技术。用户在DNS服务器上将需要保护的web服务的域名指向云抗D中心提供的高防IP或者CNAME域名，云抗D中心将访问该web服务的请求再转发给用户的业务服务器，相当于充当了一个nginx反向代理，针对该web服务的攻击会被云抗D中心清洗，正常的用户请求才会转发给用户的业务服务器。", "问题也就来了，黑客如果直接攻击用户业务服务器的IP，就会绕过云抗D中心。为了防止这种情况出现，最简单的解决方案是，用户的业务服务器提供两个IP，IP1是平时使用的，对外暴露，IP2平时不使用，同时限制IP2仅允许云抗D中心的IP段访问，一旦切入云抗D中心后，联系机房拉黑IP1，同时启用IP2即可，通常IP1和IP2可以指向同一服务器或者负载均衡。", "这个和使用CDN情况类似，比较常见的解决方案是在http协议层携带客户端的IP，比如x-forwarded-for字段。", "在国内现实的问题跨网访问的巨大延时，为了解决这个问题，通常有两种简单办法，一个是申请联通电信移动三网的IP资源，一个是使用相对昂贵的BGP资源。对应的抗D云服务也提供了三线和BGP服务，本质上区别就是一个需要做分区解析，一个不用，价格上一般BGP高防会贵些，从跨网访问来看两者都不错。", "百度安全的ADS，阿里云的高防IP，腾讯的大禹，知道创宇的抗D保等，主流云厂商也有自己的抗D服务。", "本文由 兜哥带你学安全 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据 http://www.36dsj.com/archives/76410，任何不经同意的转载均为侵权。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76410"]},
{"module": ["干货教程"], "note": ["\n文|李宁\n携程ABtest伴随ubt系统一起，两年多的时间，从最初online寥寥几个实验，但现在单机票bu每周有数十个app/online/h5平台同时线上运行。\n在14-16のJames领导下的技术驱动的大携程体系..."], "title": ["携程机票是如何做A/B测试的？"], "content": ["文|李宁", "携程ABtest伴随ubt系统一起，两年多的时间，从最初online寥寥几个实验，但现在单机票bu每周有数十个app/online/h5平台同时线上运行。", "在14-16のJames领导下的技术驱动的大携程体系，对于项目上线的收益、年终KPI考核、CEO奖项的评比都需要拿ABtest数据说话（James对数据特别敏感，曾经在2016年hackathon决赛中，未上线前某team在台上大谈项目收益，James当即打断说“你们的项目上AB了吗？没上AB的话不要讲了”，该Team被当场淘汰。） 。", "携程市值不断增长的背后是无数个abtest的支持，而携程机票大部分的ABtest都放到前台来配置，有幸在2016年经历N多大项目的上线ABT过程，依次为背景来说明下携程机票对于Abtest的应用说明。（ABtest在携程被简称为ABT）", "其本身具备统计意义，而且具备实际意义。试想一下如果没有ABtest，那新项目上线后的收益如何排除季节因素、市场环境因素的影响，而且一个页面上如果同时做多处改动，如何评判是哪个改动造成的收益或损失？这对一个理性思维的人是不可接受的。", "简单理解为将一群人分成两类，通过展示新旧版version A/B来测试哪种版本效果好，差异是多少。", "APP启动时，公共框架会拉取所有线上abtest的试验号和对应版本（所有BU）存入本地，当用户进入机票频道时候，在特定场景触发本地实验号调用。比如往返实验，在用户首页点击往返搜索时，开发会从本地文件中查询160519_fld_round试验号该用户的对应版本，确定跳转新/旧版页面。在试验号接收到调用时，同时触发一个abtest的trace埋点o_abtest_expresult，该埋点会记录clientcode,sid,pvid,试验号及版本信息，最终经过ETL，BI会汇总一张AB实验表，将上述信息汇总，便于后续做关联计算。", "：每个设备在刚启动的时候会根据设备号+试验号+随机数组成一串N位数，对100取模的余数从0-99，假设ABCD四个版本流量 10:70:10:10的情况下，则余数0-9为A版、10-79为B版、80-89为C版、90-99为D版。A版为默认版，如果尾数异常（Null或溢出），则走A版。", "如果仅有新旧两个版本的情况下，一般会设置ABCD四个版本，其中ACD为旧版，B为新版。（如果有多个迭代新版，则从EFG开始）。", "兜底版本A：BCD的剩余流量走A版，版本异常的情况下走A版", "非正交实验，如左图展示，在旧版的基础上再做区分，会因为样本数量的问题而限制同时进行的实验个数，而且无法评估两个新版同时存在的影响。", "正交试验：右图展示，不同实验流量完全打散随机分配，上一个实验与下一个实验理论上流量上没有关联，这样可以在一个页面同时进行多项实验。", "这里再提一个Magic Number = 7，虽然理论上单页面上同时进行的正交试验数量没有上线，但是经过长期经验积累，单页面同时线上实验不要超过7，否则会造成难以捉摸的幺蛾子异常。", "像ABtest里面的埋点触发场景埋点还是由开发控制的，也还是会存在埋点不准确的情况，比如说往返的实验，触发场景是在首页点击往返搜索，理论上去程列表页的UV应该是参与式样的样本数抑制。实际情况是，去程列表页30W的UV，但ABT的报表显示每天样本为50W，经过sql验算两者交集为20W,就说明有10W人是在往返流程但并没有参与实验（数据经过脱敏处理，但不改变相对位置）。", "所以基于这样的幺蛾子，在ABT结束后，既要删除代码，又要实验流量全开100%", "流量调整100%目的：将历史版本的客户端旧版规避，需要操作100%流量。", "下线代码：保证appのsize不会过度冗余，同时因埋点场景的问题，有些时候虽然流量全切100%，但仍有部分流量走旧版（非常诡异），所以将客户端代码下掉是非常必要。", "在任何情况下，分析的基础条件就是流量随机分配，如果质疑这件事情，则整个abtest就失去意义.", "实验分流一般采用设备号clientcode,但是也可以根据uid来，但情况较少", "对于实验的显著性指标P值一般使用较少且不易理解，就不做过多解释（一年也没怎么用这个指标）", "分流调节机制，新版流量不要忽上忽下，特别是涉及到核心页面的时候，否则可能会造成用户看到的页面反复变化，增加适应时间和学习成本以及影响用户体验。", "abtest是希望通过如何改进新版优于旧版，而不是通过abtest证明新版弱于旧版而下线实验，所以需要有效地分析数据。", "图表反映时间趋势，在abtest中表现为新旧版本两条折线图，且一般会出现交叉的情况，那我们就需要判断这些交叉是有随机性波动还是实验的效果，我在实践中总结简单易用的一条原则是：", "这张图就很难用抓大放小的方式来判断差异，无法证明是新版好还是旧版好，这时候需要分解这个指标来继续分析。", "机票的核心指标是转化率CR(conversion rate)和收益(revenue)，通常他们之间的关系如下图所示。", "\n携程机票前台以scrum Team的形式迭代，每个team对于需求的评审是以roi（投入产出比，return on investment）来决定项目的优先级，而return可能是cr的提升，也可能是单票收入的提升。", "对于上线实验数据跟踪，也是以当初roi的预期来进行判断实验效果，尤其在没有达到预期的情况下，寻求解决方案。（这其中还有诸多限制条件，比如收益类项目，如果CR有明显下降需要重点关注。）", "这个分解公示也代表分析的思路，无论对于收益类项目还是CR类项目，都会先看单UV收入和CR（一般情况下，abtest不会改变每个订单的票量，这是基于整体订单估算的平均值，我们暂且认为TA是常量），当这两项都保持正向增长的情况下，那可以直接开大流量继续验证直至项目完美收官（这种案例比较少）。更多的情况是，对于重大项目，即使结果是积极正向的实验，我们也会大概了解下改进点发生在哪个页面或者哪个产品，做到心中有数；", "而当发生问题的时候，我们都会对CR和单票收入做分解：", "CR下降的情况，看主流程每个页面的CR，是哪个页面下降，从页面的来源去向和点击来看，是否有明显的异常，一般来讲，对业务足够熟悉的PM在这一步可以结合业务和这些数据大概会有一些预判，是哪些因素可能造成的影响，之后再请教bi专业人员或者自己拉sql来验证数据，从而进行改进。", "利润下降的原因，继续分解指标，可以分产品、航司、利润构成等指标来分解，找到新旧版的gap，然后结合业务场景做一些预判，进行找数据来支持这个想法，继续迭代新版。", "之前的状态是PM对于AB实验的数据有一大坨报表，但是并不知道如何使用，也不知道怎么看报表，不知道怎么分解指标，但其实对于整体进行了解之后，具备简单的分析能力，关键是有业务背景知识的情况下，这样的几个公式的八股文的分析可以解决80%的问题，对于实在无法定位的问题，可以找bi寻求帮助。", "ABtest其实核心在于如何定位问题解决问题，但是限于身份不能通过数据来进行举例说明。但其实分析思路应该是一致，比如机票场景下指标分解的核心公式来解决80%的问题，在每个行业应该都会有这样的公式，可以根据特定业务背景自己总结运用。", "PM如能够掌握这些基本的指标分析、能够看懂图表、这里面就能够自助解决80%的问题，这样的abtest效率其实已经是非常高的。", "相关文章：", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76431"]},
{"module": ["干货教程"], "note": ["\n文 | 联想大数据-张正芳\n笔者按：\n\n有人在汽车设计制造出之前就能知道其品质好坏及市场营销效果。\n有人在看到汽车行驶之前就能判断出其现在与将来的“二手车残值率”。\n有人在购买汽车之前就..."], "title": ["汽车行业大数据的营销价值—— 大数据助力传统车企打造智能营销网络"], "content": ["文 | 联想大数据-张正芳", "笔者按：", "“大数据”时代给汽车营销带来新的契机。对于传统汽车企业来说，如何借大数据之势，建立智能、精准的营销网络系统，助力企业营销决策分析、精准定位目标客户人群、监控品牌影响力、提高产品质量、改进生产工艺，逐步提升市场销售额和完善售后服务质量。以上正是本文所着重阐述的。", "营销：A为B创造对方想要的价值，建立与维持关系，以获得回报的过程。", "企业营销：企业为消费者创造其想要的价值，建立与维持关系，以获得回报的过程。", "大数据助力企业营销的核心在于：在合适的时间，基于对用户的了解，把你希望推送的东西通过合适的载体，以合适的方式，推送给合适的人。而这其中如何了解用户(S市场细分)，发现优势目标用户群体(T目标市场选择)，企业/品牌在这类群体中的定位(P定位)的精准性则决定了企业营销投入产出比(ROI)。", "大数据如何助力传统车企用更少的投入成本获取更大化的利益价值成为笔者要重点阐述的，这像极了东野圭吾先生在其小说《忘忧杂货铺》中一封来自小学生给忘忧杂货铺的信，提问是“我好想不用学习也能考一百分，应该怎么做到呢?”", "小说中杂货铺老板浪矢先生回信道：请老师进行一次关于你的考试。因为考的都是你自己的事情，你的答案当然是正确的。所以肯定能拿到一百分。具体实施即：对他进行一场“朋友测试”，出各种与他有关的问题。除了出生年月，住址，有无兄弟姐妹，父母职业，还会问到爱好，特长，喜欢的明星等。测验结果由这位同学自己公布答案，其他同学各自对答案。", "互联网时代我们有过类似的产品：社交软件 QQ的好友标签。出于对朋友的了解，用贴标签的形式来描述朋友的特征。这一产品很好的促进用户彼此了解以及关系维系。", "由此，笔者提炼一个游戏化的精准营销公式：", "“如何把贴有标签A的东西(汽车/服务)卖给贴有标签B的人(目标客户群)”", "——大数据收集、整理、分析、标签建立的过程", "汽车行业大数据包括汽车行业全价值链体系中的各个环节所产生的数据，对传统汽车企业来说，包括：汽车研发、生产、采购、销售到售后。而由其形成的标签体系，则大体可以分为两类：车辆标签体系和用户标签体系。", "车辆数据通过GPS、RFID、传感器、摄像头图像处理等装置进行车辆基础信息和状态信息的采集;车辆数据类型：车辆基础数据、保有量、维修信息、行驶信息，从标签应用上可将其归类为：车辆偏好、用车特征。", "根据人的数据类型可将标签库分为：人口属性(包括用户基本信息、用户关联关系、用户兴趣偏好)、品牌价值(用户舆论信息、用户行为信息)、车主属性、驾驶特征、客户价值、营销偏好(活动偏好、消费特征、金融特征)", "例如：驾驶数据通过车主的坐姿、开车习惯精准记录，分析建立标签集：驾驶习惯、驾龄类型、行程里程等，他们隶属于驾驶特征这个标签类型。", "大数据的营销价值在于：企业决策分析，助力汽车行业全价值链体系中各个环节进行增值服务，以实现汽车企业在服务消费者时创造的“提高产品质量、改进生产工艺、提高消费者满意度”等产品和品牌价值。", "主要帮助企业实现智能规划，在战略决策和分析时作为参考依据，为市场预测和决策分析提供支持。用到数据：车辆的基本数据和保有量。营销价值：竞争对手监测和品牌传播、品牌危机监测与管理支持、发现新市场与新趋势、市场预测与决策分析支持。", "主要帮助企业实现提高产品质量、改进生产工艺、进行产品优化。主要包括：车辆状态分析、故障诊断分析、车况状态分析、驾驶行为分析。用到数据：车辆的基本数据、保有量、维修数据、行驶数据。例如：利用维修数据，建立起各种汽车配件的画像标签，用于汽车修理企业、汽车主机厂、配件生产企业进行配件库存的调整、提高成产质量、预测零部件更换。", "从客户生活形态及用车特征两个维度对客户进行画像，从客户生命周期各触点出发，研究客户在不同环节对产品及服务的诉求点及关注度，从而形成初步的汽车客户群体画像研究机制，为建立汽车客户属性标签库及倾听客户声音提升汽车品牌服务品质奠定基础。", "汽车行业海量数据的营销价值体现在——大数据助力传统车企实现精准营销，达到企业ROI最大化的过程。归纳精准营销的应用公式：“如何把贴有标签A的东西(汽车/服务)卖给贴有标签B的人(目标客户群)”。基于此，抽丝剥茧进行数据挖掘分析，建立对象A和对象B的标签体系，结合汽车营销场景和服务诉求，构建出几个典型营销场景和业务应用系统。大数据时代的汽车营销才刚刚开始，借由此文，也希望看到更多的搭配形式和创新应用。", "本文是36大数据独家稿件，由 联想大数据组-张正芳 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据 http://www.36dsj.com/archives/76446，任何不经同意的转载均为侵权。", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76446"]},
{"module": ["干货教程"], "note": ["\n文 | 秦路\n数据化运营是一个近年来兴起的概念，它在运营的基础上，提出了以数据驱动决策的口号。\n在了解数据化运营前，运营们有没有过如下的问题：\n\n不同渠道，效果究竟是好是坏?\n活跃数下..."], "title": ["从零开始，构建数据化运营体系"], "content": ["文 | 秦路", "数据化运营是一个近年来兴起的概念，它在运营的基础上，提出了以数据驱动决策的口号。", "在了解数据化运营前，运营们有没有过如下的问题：", "这是产品和运营每天每时每刻都会遇到的问题。数据化运营，实际以解决这些问题为根本。它从来不是BAT的专属，也不是大数据的独宠，每一家互联网公司，都有适合的数据运营土壤。", "数据运营体系，是数据分析的集合与应用，也是数据先行的战略，它不仅是运营人员的工作，也是产品、市场和研发的共同愿景。从管理角度，是自上而下的推动，如果领导不重视，那么执行者数据用得再好，也是半只腿走路。", "如何构建数据化运营体系呢?以下是我的总结思考。", "我将数据化运营体系划分成四层架构，每一层架构都逐步演进互相依赖，每一层又不可缺少。", "它是以运营人员为视角的框架。", "数据化运营体系的底层是数据收集，数据是整个体系中的石油。", "数据收集的核心是尽可能收集一切的数据，它有两个原则：宜早不宜晚，宜全不宜少。", "宜早不宜晚，意思是产品从创立阶段，就需要有意识的收集数据，而不是等到公司发展到B轮、C轮才去收集。数据化运营贯彻产品全阶段，不同阶段有不同的运营方法。", "宜全不宜少，指的是只有不合适的数据，而没有烂数据。像历史数据、变更记录或者细节处的数据，都存在价值。", "举一个例子，有一家金融产品，它的征信系统会详细记录用户的行为，用户在借贷时上传担保资料，会记录用户在这些页面的操作步骤和时间。这里有一个假设，上传担保资料普通人一定是谨慎小心的，如果这步骤完成的非常顺畅快速，很可能是会违约和欠款的人群：你操作那么溜，是不是想捞一笔?属于熟练工作案。征信系统会把这些数据作为特征判断风险。", "需要收集的数据能划分成四个主要类型：行为数据、流量数据、业务数据、外部数据。", "它是记录用户在产品上一系列操作行为的集合，按时间顺序记录。用户打开APP，点击菜单，浏览页面是行为;用户收藏歌曲、循环播放歌曲，快进跳过歌曲是行为。", "行为数据的核心是描述哪个用户在哪个时间点、哪个地方，以哪种方式完成了哪类操作。", "我们可以利用其分析用户的偏好，页面停留时间的长短，浏览的频繁程度，点赞与否，都可以成为依据。另外一方面，用户行为也是用户运营体系的基础，按不同行为，如购买、评论、回复、添加好友等，划分出不同梯度，定义核心用户、重要用户、普通用户、潜在用户的分层。", "行为数据通过埋点技术收集。埋点有不同种的实现方式，采集到的数据内容倒是没有差别，主要以用户ID，用户行为，行为时间戳为最主要的字段。用表格画一个简化的模型：", "useId用来标示用户唯一身份，通过它来确定具体是谁，理解成身份证号就行。active就是具体操作的行为，需要在技术层面设置和定义，timestamp就是发生行为的时间点，我这里只精确到分，一般会精确到毫秒。用户的行为记录应该详细，比如浏览了什么页面，此时页面有哪些元素(因为元素是动态的，比如价格)，它是半结构化的NoSQL形式，我这里简化了。", "有时候为了技术方便，行为数据只会采集用户在产品浏览的页面，像点击、滑动这类操作不记录。属于折衷的方法。", "除此以外，行为数据还会记录用户设备、IP、地理位置等更详细的信息。不同设备的屏幕宽度不一样，用户交互和设计体验是否会有差异和影响，怎么拿来分析?这也是数据化运营的应用之一，是宜全不宜少的体现。", "流量数据是行为数据的前辈，是Web1.0就兴起的概念。它一般用于网页端的记录，行为数据在产品端。", "流量数据和行为数据最大的差异在于，流量数据能够知道用户从哪里来，是通过搜索引擎、外链还是直接访问。这也是SEO、SEM以及各渠道营销的基础。", "虽然现在是移动时代，Web时代的流量数据并不过时。比如微信朋友圈的内容都是HTML页面，活动运营需要基于此统计效果，我们可以把它看作一类流量数据。另外，不少产品是原生+Web的复合框架，内置的活动页大多通过前端实现，此时即算行为，也算流量数据，当我们将活动页发送到朋友圈时，相应的统计只能依赖基于前端的流量数据来采集了。", "流量数据是基于用户访问的网页端产生。主要字段为用户ID、用户浏览页面、页面参数、时间戳四类，简化模型如下：", "url是我们访问的页面，以 ***.com/*** 形式记录，param是描述这个页面的参数，我们在页面上的搜索、属性信息会以参数的形式记录。和行为数据一样，如果流量数据需要更详细的统计，也是以半结构化为佳，囊括操作记录。", "它是活动及内容运营的好基友，活动的转化率，文章被发到朋友圈的阅读量等，都是作为流量数据被记录。主要通过JS采集。", "流量数据的统计已经比较成熟，Google Analytics和百度统计都是知名的第三方工具，最为常用。不过它们不支持私有化的部署，只能提供统计，我知道这个页面有100人访问，但这一百人是谁不能定位，数据也无法记录在数据库中，这对数据化运营是一种麻烦。一些新式的工具则能支持这种更精细的需求，不过要收费。", "如果有可靠和先进的技术手段，我们是能做到将行为数据和流量数据统一到一起，这是未来的趋势。", "业务数据在产品运营过程中伴随业务产生。比如电商产品，我进行了促销，多少用户领取了优惠券，多少优惠券被使用，优惠券用在哪个商品上，这些数据和运营息息相关又无法通过行为和流量解释，那么就归类到业务数据的范畴。", "库存、用户快递地址、商品信息、商品评价、促销、好友关系链、运营活动、产品功能等都是业务数据，不同行业的业务数据是不一样的，业务数据没有固定结构。", "业务数据需要后端研发进行配置，因为结构不能通用化，最好提前和研发们打声招呼提下需求。", "行为数据、流量数据、业务数据构成了数据来源的三驾马车。统称为原始数据，指没有经过任何加工。", "外部数据是一类特殊的数据，不在内部产生，而是通过第三方来源获取。比如微信公众号，用户关注后我们就能获取他们的地区、性别等数据。比如支付宝的芝麻信用，很多金融产品会调用。还有公开数据，像天气、人口、国民经济的相关指标。", "另外一种外部数据的获取方式是爬虫，我们可以爬取豆瓣电影评分、微博内容、知乎回答、房地产信息为我们所用。第三方不可能支持你获取，很多时候会有防爬虫机制。它需要一定的技术支持，不属于稳定轻松的来源。", "外部数据因为质量难以保证，更多是一种参考的作用，不像内部数据能产生巨大的作用。", "这四类数据构成了数据化运营的基石。随着互联网公司数据化水平的提高，能够利用的数据越来越多。数据结构逐步从SQL到NoSQL;信息源更加丰富，图形和声音数据越来越多;技术由单服务器演变成分布式;响应从离线批处理到实时流式，都是数据收集的挑战。", "当我们有了数据以后，进入下面一层，数据产品层。", "它不是传统意义上的数据产品(如广告系统)，而是以发挥数据价值和生产力为目的，理解成进行数据加工的产品也可。", "原始数据并不能直接为运营所用，通常脏乱差，我们需要按照一定的标准整合、加工。", "比如行为数据和流量数据，用户在微信朋友圈看到一则活动觉得不错，于是下载APP，注册后参与了活动。这里的行为数据和流量数据是完全独立的。微信朋友圈的浏览，记录的是用户weixinOpenId和cookie，下载后则是产品内部使用的的userId，两者无法对应，这就需要数据整合，将cookie、手机号、userId等信息映射(mapping)到同一个人。", "这是技术层面的数据清洗。整个过程叫做ETL。", "数据发挥价值的方式有很多种。即能通过BI，将原始数据以维度和度量的方式聚合，进行各类可视化的决策分析，也能数据挖掘。根据业务和场景决定数据的不同使用。这里最重要的是先有指标。", "我强调过尽可能的收集数据，然而原始数据那么多，怎么才能指导我们的业务呢?这要求我们从庞大的数据中找出方向。这时我们就要建立指标，指标就是我们的方向，它是业务和原始数据的连接器。", "可以这样说，指标在数据化运营体系中是承上启下的润滑油，它由原始数据加工而来，反过来又驱动其他产品。", "需要有BI?BI肯定是围绕指标建立仪表盘;要用机器学习算法?算法的目的就是提升指标效果的;你要运营?内容、用户、活动模块的KPI也是围绕指标的。", "指标不是一个通常意义的数据产品，我更喜欢的解释，是数据届的产品经理，是驱动、规划其他数据产品以及配合运营迭代业务的。这样一说，大家就明白了。", "指标如何设立，是根据运营业务所决定，也是运营的第一驱动力。", "我们简单看一下指标如何由原始数据加工而来，下图是原始数据中记录的用户打开APP的情况。", "每一个时间戳意味着对应的用户打开过APP一次，通过该表我们能计算每天有多少用户打开过APP，这是打开量，将用户数去重，就是运营中的重要指标：活跃用户数。通过对该表的进一步复杂运算，譬如用SQL的Left Join，能获得留存率。", "文章阅读量、日销售额、活动参与人数，这些几乎都是由原始数据汇总加工而出。指标汇总以后，就是运营人员产品人员每日的报表Dashboard。", "有了指标，我们再看其他的数据产品，因为篇幅有限，我着重介绍一下用户画像。", "用户画像是常用的数据产品，对产品和运营人员往往带有神秘色彩。它有两种解释，也是很多新手歧义的根源，一种用户画像属于市场营销和用户调研领域，叫做Persona，更准确的翻译是用户角色，描绘的是一个自然人的社会属性，用于用户需求和场景的确定。", "而数据领域的用户画像，叫做Profile，是将一系列数据加工出来描述人物属性的数据标签。最知名的例子就是淘宝的千人千面：用户去购买孕期的孕妇产品，很大可能被打上孕妇标签;浏览了汽车相关商品，会被打上汽车兴趣的标签。", "用户画像是一个依赖大数据和机器学习的复杂体系。准确丰富的用户画像能呈指数级的提高运营效果。", "用户画像也有简单的用法，没有数据挖掘不要紧。用户的性别、年龄、地区这些信息不难拿到吧?用户行为简单做一个喜爱偏好区分也不难吧。那么我们就有用户画像V1.0了.", "推荐系统，精准营销、广告投放都是常见的基于用户画像的应用。你要推送化妆品促销活动，选择女性标签的用户肯定有更高的成功率，更进一步，如果运营知道女性用户偏好哪个品类的化妆品，效果会更好。", "用户画像可以通过已有数据提炼获得，比如拥有用户的身份证信息，就能准确获得性别、籍贯、出生年月这三个标签。也能通过算法计算获得，比如在淘宝购物遗留的收件人姓名，通过机器学习，以概率的形式获得买家是男是女，建国很大可能是男性，翠兰很大可能是女性。", "用户画像是基于原始数据的加工，原始数据越全，用户画像就越丰富。", "数据产品层中，我们将数据加工为指标，以其为核心，构建和规划数据产品。如何展现指标(BI)，如何提高指标(算法)，如何计算出指标(ETL)，如何与指标组合(用户画像)。", "我们现在获得了这些「产品」，接下来就是使用，运营和产品人员就是它们的用户。", "以人为主要生产力，和数据产品的计算机自动化对应。", "在我们谈及具体的方法前，强调一下人的作用。不论我们前面打造了多好的数据产品，员工的数据化运营意识提高不上去，一切等于零。", "对人的要求有三点：", "既要知道数据能够做什么，也要知道数据做不了什么。前者很容易理解，我工作中遇到很多次，在有数据可以提供决策的情况下，依旧相信个人经验。这是应该规避的思维，不是一个人，而是团队要做到。", "数据化运营也不是企业运营的灵丹妙药，得客观承认，公司体量越大，数据化运营所能发挥的效果也越好。在创业公司或者小公司，会受到一定的限制，比如没有技术支持，提升效果不够，数据体量缺乏等原因，造成优先级的延后。这是没办法的取舍问题，只能以解决问题为首先依据。", "虽然有意识地利用，可员工仅限于求平均数的水平，那么也别期待太高了。", "这一点，得通过不断地系统培训，人员招聘解决。自上而下的倡导和发起是最好的结果，高层有数据化运营的战略和意识、管理层有数据化运营的指导经验，执行层能将数据化运营的落地，那么整个体系也推行成功了。", "这是对员工的技能要求，诸如MySQL查询数据、BI多维度分析、精准营销、 AB测试、转化率分析，都是必须的。将数据相关的工具玩得顺溜，员工才能在发挥够大的价值。", "运营和产品如何进行数据运营，具体的技巧和方法论太多了，我以核心思想为引子。大家着重了解思维。", "全量运营是一种集中运营的策略，活动、内容推送、营销、用户关系维护，这些方式如果针对所有的用户，这是运营资源的浪费，你不可能通过一种方式满足所有的用户，也不可能用一种方式做到最好。", "用户间是有差异的，这种差异需要用精细化运营弥补。", "精细是是将目标拆分成更细的粒度，全国销量变成上海销量北京销量、全年销量变成第一季度销量第二季度销量，用户变成新用户老用户。电商卖口罩，是卖给北京的用户好，还是海南的?促销化妆品，目标人群选择男人女人也是显而易见的。精细(拆分)是一种数据分析的思路，也是一种运营手段。", "精益比精细更进一步，精细是手段，精益是目标。什么是精益?精益就是二八法则，找出最关键的用户。我们都知道要将化妆品卖给女人，但一定会有部分女人支付更多，20%的女人占了80%的销量，精益就是找准这20%。", "对最适合的用户在最恰当的时机采取最合适的手段以产生最大的价值。", "前面三个「最」说的是精细，后面一个「最」指的是精益：价值/目标最大化。我有CRM，那么就从CRM中找出最有价值的客户去维护;我有风险管理，就找出最可能违约的投资;要做活动，欢迎的是产出最大而不是薅羊毛的用户;积分中心，效果最好的只会是最优质的那批客户。", "这个第二个核心，", "传统的运营方式，是知晓过去已经发生的事，销量是多少，活跃数是多少，这在日益严酷的竞争环境中还不够。", "把握当下，是能获得数据的立即反馈。你要推广一个活动，可以提前挑选5%的用户做一个测试，及时获知用户的反馈，转化率高不高，响不响应，然后按照数据决定后续的运营是继续还是改进。这是技术带来的进步优势。", "预测未来，是机器学习的领域，通过数据建模，获得概率性的预测，用户可不可能流失，会不会喜欢和购买这个商品，新上线的电影会否偏好…运营则利用这些概率针对性的运营。", "如果限于技术无法使用机器学习，则需要根据现有数据趋势去估计，这取决于运营人员的经验和数据敏感性。", "数据化运营体系的搭建过程中，运营人员会用到很多的工具。", "用户积累到一定数量，我们考虑引入积分中心增加用户粘性;产品涉及到地推和销售人员，则要加入CRM(客户关系管理)以维系客群;O2O和电商，基本配置肯定有优惠券的发送;反馈越来越多，我们也需要客服中心解决各类疑问。这些与运营息息相关的工具，在数据运营体系中占据中重要的比例。", "为了更好的达成目标，会将其独立成运营模块/运营后台。好的运营后台和用户端的产品同等重要，也需要后台产品经理规划。", "以我们经常接触的优惠券为例，它肯定要设置一套规则，核心目标是财务数据，是优惠券成本和收入之间的平衡：你不能滥发，那肯定亏钱，也不能少发，用户连这东西都不知道。有哪些券、怎么发、发了多少用了多少、未来准备发多少、发了有多少没用掉，都是一套大框架的东西，于是做成了发券系统。", "优惠券能和CRM结合，CRM通过几个指标将用户划分成了不同的价值和人群。这个用户特别喜欢花钱，那么优惠券给他满1000减100，肯定比满200减20过瘾。那个用户还没有消费过，要用首单优惠刺激他。还有用户有段时间不消费了，运营们得加把劲营销。上面东西从更高的视野看，是一连串效果、ROI、盈利的评估。这就是用数据做运营策略。", "CRM又能和客服中心结合，电话号码肯定和用户的数据绑定，VIP用户电话进来了，我们选客户主管去接待，宾至如归。普通用户呢，也不能粗心，客服至少需要通过后台的用户画像知道这个用户是什么情况，这也有针对性的服务。数据运营体系不止服务于运营和产品的。", "系统化，要求的是我们把运营的整个过程和策略流程也当作一款产品去缔造：哪些方法好用，哪些手段效果好，哪种活动能持续做，把这些都固定下来，打造出一个运营用的产品后台，作为日常和招数。这种系统化思维也叫「复用」，之后则是把系统做得越来越自动，功能越来越强大，也是另外一种精益了。", "以上种种，是将数据、产品运营、系统和人员四者结合起来。系统之所以是系统，就是脱离了粗放的阶段，一切皆是有序、规则和充满策略。数据就是系统的润滑剂，你没有数据，怎么能有选择性的发券、做活动、推送，维护用户呢?", "数据产品层加工出来的各类标签、用户画像、模型…就是要在数据运营层最大化的被员工使用。数据本身没有价值，变成策略才有价值。", "我们整个体系进行到最后的环节，它需要面向用户。数据收集得再多、加工得再好，运营得再努力，如果不将它们传递给用户，体系就是失败的。", "整个体系的前三层用户都感知不到。用户直接感知到的是产品的推送通知、Banner、广告位、活动、文案、商品的展示顺序等。在与产品交互的过程中，用户会以直接的反馈表达自己喜恶。", "感兴趣的会点击，喜爱的会够买，讨厌的会退出…这些构成了新一轮的行为数据，也构成了反馈指标：点击率、转化率、跳出率、购买率等。这些指标就是用户触达层的结果体现，也是数据化运营的结果体现。", "好与不好，都需要验证。", "结果不是终点。管理学有个概念叫PDCA，翻译成中文是计划-执行-检查-改进，以此为循环。用户触达层不是数据化运营体系的结束，它是另外一种开始。通过反馈获得的数据去优化去改进。", "我的点击率5%，那么我能不能通过运营优化，达到10%?用户接受推送后选择了卸载，我们有什么方法挽回?留存率被提高，这种策略能不能应用到其他用户上面。", "也许我们数据化运营后，不会获得一个满意的结果，但如果我们连优化改进都不去做，那么连好的机会都不会有。", "你看，优秀的员工，不会以数据化运营的结果沾沾自喜，而是进行新一轮的开始。", "是终点，又是起点，此过程就是迭代，是体系的核心。", "我们将四层串联起来看待，下图是一款产品简化的数据化运营闭环。", "数据收集层：当用户打开APP时，浏览新闻，通过埋点记录用户的行为数据：何时何地是谁看了哪些新闻。", "数据产品层：计算机将收集上来的行为数据进行加工，统计用户对军事、科技、经济等不同类型新闻的阅读数。用卡方检验得到用户的阅读偏好在科技新闻，将其写入到用户画像/标签系统。", "数据运营层：近期有一个科技类的活动，需要一定用户量参与。运营不能选择全部的用户推送吧，那么就从用户池中筛选中对科技感兴趣的用户。", "用户触达层：选择用户进行精准推送，用户在手机端接收到消息。后台则会记录用户是否打开推送通知，是否浏览页面，是否参与了活动。转化率作为反馈会被记录下来，用以下次迭代改进。", "该例就是一次合格的闭环。数据化运营体系既能简单到用Excel完成，也能引入机器学习数据挖掘分布式系统等高端技术，看的是思维和应用。我们将体系中的四层简化成四个模型，帮助大家理解：", "数据收集：以用户和产品的交互为输入，原始数据(行为、业务、流量、外部)为输出。", "数据产品：以原始数据为输入，以加工数据(标签、画像、维度、指标、算法结果)为输出。", "数据运营：以加工数据为输入，以运营策略(用户、内容、活动、电商)为输出。", "用户触达：以运营策略为输入，以反馈行为(转化率、点击率、响应率)为输出。", "用户产生的反馈行为作为新的交互输入，迭代和优化，数据化运营体系就良好地运作起来。好的数据化运营体系也是高度自动化的运作，像个性化推荐，可以略过数据运营层，服务器实时计算后直接将推荐结果给用户，人就不用参与其中了。", "这是四个互相联系有先后顺序的系统，以此构成数据化运营体系。因为技术手段差异，实现方式会有不同，哪怕是Excel，也能发出数据化运营的光芒。", "以上就是产品和运营视角的数据化运营体系，没有过多的牵涉研发技术，实际复杂程度还要再高一点。当然，万千用法，存乎一心，希望大家学到的是理念和思维，实际工作中，还是有很多玩法留待大家挖掘。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/76601"]},
{"module": ["干货教程"], "note": ["\n文 | CTTCassie\n随着信息的高速发展，越来越多的数据信息等待处理，如何快速的从这些海量数据中找到你所需要的数据呢?这就是大数据的处理问题，下面我对几个经典的大数据问题进行分析~~~~\n..."], "title": ["经典的大数据问题"], "content": ["文 | CTTCassie", "随着信息的高速发展，越来越多的数据信息等待处理，如何快速的从这些海量数据中找到你所需要的数据呢?这就是大数据的处理问题，下面我对几个经典的大数据问题进行分析~~~~", "将所有的IP逐个写入到一个大文件中，因为当IP地址采用点分十进制的方式表示的时候是32位的，所以最多存在2^32个IP。可以采用映射的方式，比如模1000，将这个较大的文件映射为1000个小文件，再将每个小文件加载到内存中找到每个小文件中出现频率最大的IP(可以使用hash_map的思想进行频率统计);然后在这1000个最大的IP中找出那个出现频率最大的IP，就是出现次数最多的IP了。", "算法思想如下：(分而治之+hash)", "1).IP地址最多有2^32=4G个，所以不能直接将所有的IP地址加载到内存中", "2).可以考虑采用”分而治之”的思想，就是将IP地址Hash(IP)%1024值，将海量IP分别存储到1024个小文件中，这样每个小文件最多包含(2^32)/(2^10)=4M个IP地址", "3).对于每一个小文件，可以构建一个IP值为key,出现次数为vaue的hash_map，通过value的比较找到每个文件中出现次数最多的那个IP地址", "4).经过上述步骤已经得到1024个出现次数最多的IP地址，再选择一定的排序算法找出这1024个IP中出现次数最多的那个IP地址", "我们知道对于整形数据来说，不管是有符号的还是无符号的，总共有2^32=4G个数据(100亿个数据中肯定存在重复的数据)，我们可以采用位图的方式来解决，假如我们用一个位来代表一个整形数据，那仫4G个数共占512M内存。我们的做法是将第一个文件里的数据映射到位图中，再拿第二个文件中的数据和第一个文件中的数据做对比，有相同的数据就是存在交集(重复的数据，交集中只会出现一次).", "要解决这个问题同样需要用到位图的思想，在问题二中已经了解到采用位图的一个位可以判断数据是否存在，那仫找到出现次数不超过两次的数字使用一个位是无法解决的，在这里可以考虑采用两个位的位图来解决.", "根据上述分析我们可以借助两个位，来表示数字的存在状态和存在次数，比如：00表示不存在，01表示存在一次，10表示存在两次，11表示存在超过两次;类似问题二的计算过程：如果一个数字占一位，需要512M内存即可，但是如果一个数字占两位，则需要(2^32)/(2^2)=2^30=1G内存;将所有数据映射到位图中查找不是11的所对应的数字就解决上述问题了。", "题目扩展：其他条件不变，假如只给定512M内存该如何找到出现次数不超过两次的数字?", "将数据分批处理，假若给定的是有符号数，则先解决正数，再解决负数，此时512M正好解决上述问题.", "分析：看到字符串首先应该反应过来的就是布隆过滤器，而问题四的近似算法就是采用布隆过滤器的方法，之所以说布隆过滤器是近似的算法，因为它存在一定 的误判(不存在是肯定的，存在是不肯定的);而要想精确判断字符串文件的交集，我们可以采用分而治之的方法：将大文件切分为一个一个的小文件，将一个又一个的小文件拿到内存中做对比，找到对应的交集。", "根据不同的字符串哈希算法，可以计算出不同的key值，然后进行映射，此时可以映射到不同的位置，只有当这几个位全部为1的时候这个字符串才有可能存在(因为当字符串过多的时候可能映射出相同的位)，只有一个位为0，那仫该串一定是不存在的，所以说布隆过滤器是一种近似的解决办法。将第一个文件映射到布隆过滤器中，然后拿第二个文件中的每个串进行对比(计算出特定串的key，通过不同的哈希算法映射出不同的位，如果全为1则认为该串是两个文件的交集;如果有一位为0那仫该串一定不是交集).", "既然叫做切分，顾名思义就是将大文件切分为小文件，那仫如何切分?切分的依据是什仫呢?如果我们在切分的时候可以将相似或者相同的文件切分到同一个文件中那仫是不是就加快了查找交集的速度呢?答案是肯定的。", "知道了哈希切分的依据我们应该如何处理呢?我们可以根据字符串的某个哈希算法得到该字符串的key，然后将key模要分割的文件数(假设为1000个文件，文件编号为0~999)，我们将结果相同的字符串放到同一个文件中(两个文件中的字符串通过相同的哈希算法就会被分到下标相同的文件中)，此时我们只需要将下标相同的文件进行比对就可以了。。。", "哈希切分明显比布隆过滤器的方法效率要高，时间复杂度为O(N).", " ", " ", " ", " ", "在上面实现的布隆过滤器中引用了不同的哈希算法，有想研究哈希算法的的童鞋可参考", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75262"]},
{"module": ["干货教程"], "note": ["\n背景\ntheano 是一个python语言的库，实现了一些机器学习的方法，最大的特点是可以就像普通的python程序一样透明的使用GPU\ntheano的主页：http://deeplearning.net/software/theano/index.ht..."], "title": ["黑科技：利用python的theano库刷kaggle mnist排行榜"], "content": ["theano 是一个python语言的库，实现了一些机器学习的方法，最大的特点是可以就像普通的python程序一样透明的使用GPU", "theano 同时也支持符号计算，并且和numpy相容，numpy是一个python的矩阵计算的库，可以让python具备matlab的计算能力，虽然没有matlab方便", "MNIST是一个手写数字识别的公开数据集，我以为地球人都知道", "其他大部分资源位于deeplearning向导的主页：", "kaggle是一个供大家公开测试各种机器学习算法的平台，包括ICML和KDD cup一类的比赛都在上面进行，其中的入门测试集就是MNIST：", "目前发表的最好结果是卷积神经网络方法的0.23%错误率", "，kaggle上被认可的最好结果是0.5%。看这个架势，mnist已经基本被大家解决了。不过本着实践出真知和学习threano用法的目的，我觉得用python的theano库对kaggle mnist刷个榜玩玩也不错。", "theano的代码位于:", "我修改后的代码位于：", "原来是从cPickle导入：", "更改为读取train.csv和test.csv，先初始化四个list。", "valid_set是用来在SGD迭代过程中，用来验证效果但不参与训练的数据集。每次只有确定在valid_set上更有效，才继续进行目标函数的优化，这样可以防止过拟合。参见early-stopping", "。", "设定数据集的大小，如果是调试模式则减小数据集。", "MNIST共有7w条记录，其中6w是训练集，1w是测试集。theano的样例程序就是这么做的，但kaggle把7w的数据分成了两部分，train.csv一共42000行，test.csv一共28000行。实际可用来训练的数据只有42000行（由此估计最后的效果也会有相应的折扣）。theano把6w的训练集分为了5w的test_set和1w的valid_set，我在这里把42000行数据分为36000的train_set、5000行的valid_set和1000行的test_set（训练时用不到）。", "另外我建了一个predict_set，用来保存准备提交给kaggle的数据。然后我进行了变量初始化并从文件读取数值，读取的时候把kaggle的int转化成了theano需要的float。", "theano的convnet是由两个卷积层，一个hidden layer和一个logistic regression构成的，如图", "： ", "我们需要的是最后一层的输出，theano的样例程序在最后一层lr给了我们一个符号变量y_pred，定义如下：", "手册上说可以使用eval()对其进行实例化", "：", "但是我这样做不行，只好用了很不理想的方案，原谅我", "其中predict函数为：", "我对技术不敬畏，对不起各位了。", "这样我们就得到可以操作的数组，写入输出文件：", "以上可以差不多达到1.0%的误差，和理论值0.5%还有差距，我觉得可能是因为数据不够多，所以我对输入输出数据进行了平移预处理。 输入数据平移：", "输出的时候让平移后的9个位置进行投票，boost了一把", "ok，万事俱备，刷榜吧！", "刷到前10，我感觉可以了，再往上刷10名就要被怀疑作弊了。", "simple cell到complex cell是怎么实现的？", "两个二维向量卷积的意思就是扫一遍，类似于你在暗处拿着一个手电筒把一篇文章看一遍。扫的每一帧的具体操作就是相乘（找相似的特征，仅仅是相乘就可以了）。卷积不是目的，扫一遍算相似度才是。", "当做polling的时候，时空信息就消失了，本来是28×28维的空间，如果4×4方块做pooling，就只剩下7×7的位置信息了。取而代之的，是feature域的信息。典型的“时空样本变换”，不过这个是98年就做出来的，实在是很赞。", "学习方案是构造一个损失函数，然后用SGD求解，因为有很多层，所以损失函数的梯度计算超级复杂，参数也很多很多，不过theano有一个库，可以自动计算梯度。先进行符号计算，然后Sample一些输入数据算梯度。", "嗯，大概就是这个样子吧。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75354"]},
{"module": ["干货教程"], "note": ["\n文 | 58沈剑\n一分钟系列之《啥，又要为表增加一列属性？》分享了两种数据库属性扩展思路，被喷得厉害。第二天补充了一篇《这才是真正的表扩展方案》，分享了互联网大数据高并发情况下，数据..."], "title": ["100亿数据1万属性数据架构设计"], "content": ["文 | 58沈剑", "一分钟系列之《啥，又要为表增加一列属性？》分享了两种数据库属性扩展思路，被喷得厉害。第二天补充了一篇《这才是真正的表扩展方案》，分享了互联网大数据高并发情况下，数据库属性扩容的成熟工具及思路。", "对于version + ext方案，还是有很多朋友质疑“线上不可能这么用”。本篇将讲述一下58同城最核心的数据“帖子”的架构实现技术细节，说明不仅不是“不可能这么用”，而是大数据，可变属性，高吞吐场景下的“常用手段”。", "问：什么是数据库扩展的version + ext方案？", "\n使用ext来承载不同业务需求的个性化属性，使用version来标识ext里各个字段的含义。", "例如上述user表：", "\nverion=0表示ext里是passwd/nick", "\nversion=1表示ext里是passwd/nick/age/sex", "（1）可以随时动态扩展属性，扩展性好", "（2）新旧两种数据可以同时存在，兼容性好", "（1）ext里的字段无法建立索引", "（2）ext里的key值有大量冗余，建议key短一些", "58同城是一个信息平台，有很多垂直品类：招聘、房产、二手物品、二手车、黄页等等，每个品类又有很多子品类，不管哪个品类，最核心的数据都是“帖子信息”（业务像一个大论坛？）。", "大家去58同城的首页上看看就知道了：", "\n（1）每个品类的属性千差万别，招聘帖子和二手帖子属性完全不同，二手手机和二手家电的属性又完全不同，目前恐怕有近万个属性", "\n（2）帖子量很大，100亿级别", "\n（3）每个属性上都有查询需求（各组合属性上都可能有组合查询需求），招聘要查职位/经验/薪酬范围，二手手机要查颜色/价格/型号，二手要查冰箱/洗衣机/空调", "\n（4）查询量很大，每秒几10万级别", "如何解决100亿数据量，1万属性，多属性组合查询，10万并发查询的技术难题，是今天要讨论的内容。", "每个公司的发展都是一个从小到大的过程，撇开并发量和数据量不谈，先看看", "\n（1）如何实现属性扩展性需求", "\n（2）多属性组合查询需求", "最开始，可能只有一个招聘品类，那帖子表可能是这么设计的：", "\ntiezi(tid,uid, c1, c2, c3)", "那如何满足各属性之间的组合查询需求呢？", "\n最容易想到的是通过组合索引：", "\nindex_1(c1,c2) index_2(c2, c3) index_3(c1, c3)", "随着业务的发展，又新增了一个房产类别，新增了若干属性，新增了若干组合查询，于是帖子表变成了：", "\ntiezi(tid,uid, c1, c2, c3, c10, c11, c12, c13)", "\n其中c1,c2,c3是招聘类别属性，c10,c11,c12,c13是房产类别属性，这两块属性一般没有组合查询需求", "但为了满足房产类别的查询需求，又要建立了若干组合索引（不敢想有多少个索引能覆盖所有两属性查询，三属性查询）", "是不是发现玩不下去了？", "新增属性是一种扩展方式，新增表也是一种方式，有友商是这么玩的，按照业务进行垂直拆分：", "\ntiezi_zhaopin(tid,uid, c1, c2, c3)", "\ntiezi_fangchan(tid,uid, c10, c11, c12, c13)", "平台型创业型公司，可能有多个品类，例如58同城的招聘房产二手，很多异构数据的存储需求，到底是分还是合，无需纠结：基础数据基础服务的统一，无疑是58同城技术路线发展roadmap上最正确的决策之一，把这个方针坚持下来，@老崔 @晓飞 这些高瞻远瞩的先贤功不可没，业务线会有“扩展性”“灵活性”上的微词，后文看看先贤们如何通过一些巧妙的技术方案来解决的。", "如何将不同品类，异构的数据统一存储起来，采用的就是类似version+ext的方式：", "\ntiezi(tid,uid, time, title, cate, subcate, xxid, ext)", "\n（1）一些通用的字段抽取出来单独存储", "（2）通过cate, subcate, xxid等来定义ext是何种含义（和version有点像？）", "（3）通过ext来存储不同业务线的个性化需求", "\n例如招聘的帖子：", "\next : {“job”:”driver”,”salary”:8000,”location”:”bj”}", "\n而二手的帖子：", "\next : {”type”:”iphone”,”money”:3500}", "58同城最核心的帖子数据，100亿的数据量，分256库，异构数据mysql存储，上层架了一个服务，使用memcache做缓存，就是这样一个简单的架构，一直坚持这这么多年。上层的这个服务，就是58同城最核心的统一服务IMC（Imformation Management Center），注意这个最核心，是没有之一。", "解决了海量异构数据的存储问题，遇到的新问题是：", "\n（1）每条记录ext内key都需要重复存储，占据了大量的空间，能否压缩存储", "\n（2）cateid已经不足以描述ext内的内容，品类有层级，深度不确定，ext能否具备自描述性", "\n（3）随时可以增加属性，保证扩展性", "\n每个业务有多少属性，这些属性是什么含义，值的约束等揉不到帖子服务里，怎么办呢？", "\n58同城的先贤们抽象出一个统一的类目、属性服务，单独来管理这些信息，而帖子库ext字段里json的key，统一由数字来表示，减少存储空间。", "如上图所示，json里的key不再是”salary” ”location” ”money” 这样的长字符串了，取而代之的是数字1,2,3,4，这些数字是什么含义，属于哪个子分类，值的校验约束，统一都存储在类目、属性服务里。", "这个表里对帖子中心服务里ext字段里的数字key进行了解释：", "\n1代表job，属于招聘品类下100子品类，其value必须是一个小于32的[a-z]字符", "\n4代表type，属于二手品类下200子品类，其value必须是一个short", "\n这样就对原来帖子表ext里的", "\next : {“1”:”driver”,”2”:8000,”3”:”bj”}", "\next : {”4”:”iphone”,”5”:3500}", "\nkey和value都做了统一约束。", "除此之外，如果ext里某个key的value不是正则校验的值，而是枚举值时，需要有一个对值进行限定的枚举表来进行校验：", "这个枚举校验，说明key=4的属性（对应属性表里二手，手机类型字段），其值不只是要进行“short类型”校验，而是value必须是固定的枚举值。", "ext : {”4”:”iphone”,”5”:3500}这个ext就是不合法的（key=4的value=iphone不合法），合法的应该为", "\next : {”4”:”5”,”5”:3500}", "此外，类目属性服务还能记录类目之间的层级关系：", "\n（1）一级类目是招聘、房产、二手…", "\n（2）二手下有二级类目二手家具、二手手机…", "\n（3）二手手机下有三级类目二手iphone，二手小米，二手三星…", "\n（4）…", "协助解释58同城最核心的帖子数据，描述品类层级关系，保证各类目属性扩展性，保证各属性值合理性校验，就是58同城另一个统一的核心服务CMC（Category Management Center）。", "多提一句，类目、属性服务像不像电商系统里的SKU扩展服务？", "\n（1）品类层级关系，对应电商里的类别层级体系", "\n（2）属性扩展，对应电商里各类别商品SKU的属性", "\n（3）枚举值校验，对应属性的枚举值，例如颜色：红，黄，蓝", "解决了key压缩，key描述，key扩展，value校验，品类层级的问题，还有这样的一个问题没有解决：每个品类下帖子的属性各不相同，查询需求各不相同，如何解决100亿数据量，1万属性的查询需求，是58同城面临的新问题。", "数据量很大的时候，不同属性上的查询需求，不可能通过组合索引来满足所有查询需求，怎么办呢？", "\n58同城的先贤们，从一早就确定了“外置索引，统一检索服务”的技术路线：", "\n（1）数据库提供“帖子id”的正排查询需求", "\n（2）所有非“帖子id”的个性化检索需求，统一走外置索引", "元数据与索引数据的操作遵循：", "\n（1）对帖子进行tid正排查询，直接访问帖子服务", "\n（2）对帖子进行修改，帖子服务通知检索服务，同时对索引进行修改", "\n（3）对帖子进行复杂查询，通过检索服务满足需求", "这个扛起58同城80%终端请求（不管来自PC还是APP，不管是主页、城市页、分类页、列表页、详情页，很可能这个请求最终会是一个检索请求）的服务，就是58同城另一个统一的核心服务E-search，这个搜索引擎的每一行代码都来自58同城@老崔 @老龚 等先贤们，目前系统维护者，就是“架构师之路”里屡次提到的@龙神 。", "对于这个服务的架构，简单展开说明一下：", "为应对100亿级别数据量、几十万级别的吞吐量，业务线各种复杂的复杂检索查询，扩展性是设计重点：", "\n（1）统一的Java代理层集群，其无状态性能够保证增加机器就能扩充系统性能", "\n（2）统一的合并层C服务集群，其无状态性也能够保证增加机器就能扩充系统性能", "\n（3）搜索内核检索层C服务集群，服务和索引数据部署在同一台机器上，服务启动时可以加载索引数据到内存，请求访问时从内存中load数据，访问速度很快", "\n（3.1）为了满足数据容量的扩展性，索引数据进行了水平切分，增加切分份数，就能够无限扩展性能", "\n（3.2）为了满足一份数据的性能扩展性，同一份数据进行了冗余，理论上做到增加机器就无限扩展性能", "\n系统时延，100亿级别帖子检索，包含请求分合，拉链求交集，从merger层均可以做到10ms返回。", "58同城的帖子业务，一致性不是主要矛盾，E-search会定期全量重建索引，以保证即使数据不一致，也不会持续很长的时间。", "文章写了很长，最后做一个简单总结，面对100亿数据量，1万列属性，10万吞吐量的业务需求，58同城的经验，是采用了元数据服务、属性服务、搜索服务来解决的。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75363"]},
{"module": ["干货教程"], "note": ["\n文 | 吴裕彬\n大数据已经彻底改变了国家经济的方方面面，但其在经济学领域却没有太大反响，这是一件非常奇怪的事。经过多年经济学和大数据应用前景的研究，笔者深感到大数据技术可以对经济政..."], "title": ["大数据如何改变经济咨询服务行业?"], "content": ["文 | 吴裕彬", "大数据已经彻底改变了国家经济的方方面面，但其在经济学领域却没有太大反响，这是一件非常奇怪的事。经过多年经济学和大数据应用前景的研究，笔者深感到大数据技术可以对经济政策分析和经济学研究产生非常深远的影响。", "大数据收集的都是实时数据，现在很多企业都在利用实时数据，奇怪的是经济学研究却主要使用汇总数据，很少使用实时数据。汇总数据一般最精准的就是当天的数据，比如汇率，而像通胀数据则是当月的。", "。经济学研究调用的一般是成千上万个时间点(一般最小的时间点是天)的数据，相对于大数据而言，这样的研究样本是很小的，所以经济学研究对研究者的统计学功力依赖很大。然而大数据可以调用数百万、数千万甚至是更多的数据，因此研究者的统计学功力就显得没那么重要。", "经济学常用的数据分析方法是时间序列分析，一般只能研究两个变量之间的互动，比如狭义货币是如何影响通胀的。", "比如在零售领域，传统经济学的数据收集呈矩形，有N个观察时间点和K个变量，一般K远远小于N。而大数据记载的只是一系列消费行为，其数据并非矩形，也没有更复杂的结构，你可以用统计学方法把这些数据构造成无限多个矩形数据集。", "传统经济学认为每一个记录的数据都是独立的，或者可以集结成面板数据，归根结底也就是时间序列的衍生物。但大数据却非如此，比如社交网络上人与人之间的互动数据是高度复杂的，传统的经济学模型无法揭示社交网络上人与人之间的互动关系，必须使用大数据的方法。", "企业运用大数据的场景主要是记录运营过程和结果，并构建涵盖范围广泛的预测类算法。比如Amazon和Netflix应用预测模型为客户推荐影视剧和书籍。预算类算法的可使用范围远远超越了电子商务。比如在医疗保险领域，保险公司通过将病人的付费情况和治疗效果数据导入预算类算法，可以计算其“风险系数”，然后通过风险系数来调整保费标准，而在大数据技术之前，“风险系数”是由病人的病史和相应的统计学分析方法来确定的。其实，大数据若应用到政府领域，也可以极大地改变经济政策分析和经济学研究。", "通过管理税收系统、社保系统以及法规条例，政府积累了海量的颗粒状数据。公共管理数据在很大程度上没有被充分利用，主要原因是政府有关部门缺乏大数据硬件、软件和人才基础，另外这些数据也没有通过开放的端口给私营数据供应商使用，而且各地方政府的数据收集标准不一，难以统一维护和管理。在这方面，许多欧洲国家走在世界前列，其中央政府将各级地方政府的教育、医保等数据整合成了针对全国人口的大数据库。", "公共管理数据的潜力非常巨大，这些数据涵盖不同个人、企业和机构相当长期的各类行为和状况，一般是面板结构的，数据质量也很高。而且由于这些数据集的涵盖面是普遍的，其可以和其它涵盖面更具选择性的数据集搭配使用。", "如果政府向私营数据供应商有限开放这些公共管理数据的应用端口，对经济政策分析和经济学研究无疑将产生巨大推动。比如经济学家Thomas Piketty和Emmanuel Saez利用美国国税局的数据构建了美国最富家庭占全国收入比例的历史数据集。他们的相关研究成果对奥巴马以来的决策者产生了极大的影响，美国最富家庭所占全国收入比例和所占全国纳税总额比率的严重不对称以及日益失衡成为了决策者和立法者探讨税收政策改革的一个话题焦点。", "政府在经济活动的测算方面扮演着极其重要的角色，比如通货膨胀率、失业率和GDP等等的测算都是由政府主导的。一般而言，政府都是通过调研的方式来测算经济活动的。比如国家统计局会派出调研员去商店手动收集成千上万商品的价格，然后将这些数据汇总成不同的通胀指数——CPI就是其中之一。然而大数据技术可以更大规模地收集物价数据，甚至可以做到实时收集。比如，由麻省理工学院斯隆商学院教授Alberto Cavallo和Roberto Rigobon发起的10亿物价项目(BPP)，通过成百上千个电商网站和手机应用的端口可以收集数以10万计商品的实时价格数据，从而可以实时发布通胀数据，而不是像国家统计局那样每个月发布一次。", "并且有些公司还在用大数据进行广泛的运营实验，相比之下，公共部门在这些方面进展十分迟缓。", "政府收集了大量的公共事务管理数据，这些数据对经济政策分析和经济学研究助益极大，但利用效率却极低。比如，医保部门有过去几十年的每一笔医保索赔记录，稍加整理就可以得出无数个人的病史数据集，通过大数据挖掘和预测类算法，可以得出关于不同治疗方案和过程的非常详细的成本效益分析，从而使医保支出的效益大大提高。以此类推，政府部门完全可以用大数据技术对医保和税收等涉及财政收支的政策法规改革调整进行一定范围的精准实验，再根据效果去调整相关政策法规。", "未来，经济学家将更多地使用上面所描述的大数据挖掘技术，经济学数据分析被时间序列垄断的局面也将被打破，预测类算法将取代变量之间的因果分析，机器学习将取代统计模型。", "政府公共事务管理数据的应用端口将更趋开放，其数据衍生产品前景远大，许多重要的经济指标将可以做到实时发布，公共部门的议题探讨和决策过程将更多地获取大数据的支持。根据Statista的数据，经济咨询服务(经济咨询公司)在美国的市场规模已超过300亿美元(近2000亿元人民币)，这并不包括银行、投行和公共机构内部的经济分析部门，一旦包括起来，其市场规模将更大。经济咨询服务未来在中国也应该成为一个千亿元级别的产业，大数据技术将使该行业发生根本性的技术革命，前景不可估量。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75407"]},
{"module": ["干货教程"], "note": ["文 | 黄嘉伟\n引言\n中国的移动金融互联网行业在近两年跨入了蓬勃的多元化道路。目前市场上移动金融领域的app多如牛毛，仅是从功能上划分，就产生了诸如支付、理财、手机银行、证券、消费金融..."], "title": ["移动金融app运营应该关注哪些指标?"], "content": ["中国的移动金融互联网行业在近两年跨入了蓬勃的多元化道路。目前市场上移动金融领域的app多如牛毛，仅是从功能上划分，就产生了诸如支付、理财、手机银行、证券、消费金融等各大派系。在当下国内移动互联网的环境下，互联网公司和大型国有机构自身的基因特性又更进一步加快了各垂直领域百花齐放的局面。诚然微信，支付宝这两个场景之王几乎贯穿了生活的各方面，但银行作为金融的源头，同样也牢牢把控住现代人手机银行以及信用卡的入口。但纵有机构有千千万万，诸多移动金融app运营也有着清晰的指标脉络。", "图：移动金融app运营阶段图", "用户量是一切运营工作的开端和基础，也因此产生了移动金融app运营的第一个分歧——如何定义用户。现有行业主流的区分方法有以下几种：", "a) 下载使用app的人数(无论注册与否)", "b) 使用并注册的人数", "c) 参加过本app红包推广活动，并留下手机号码的人数(互金理财类)", "d) 注册之后，进行过完整开户流程的人数(证券类)", "不同的用户区分方法会带来不同统计口径和运营策略，厘清自己用户的定义是首要任务。", "新增用户量的指标可细化到年/月/周/日的粒度，同时需要加入与以往平均数据状况的跟踪对比，无对比不分析。例：上图反映的是某移动金融app在统计时间内的新增用户量情况，通过对比得知在上月，大量的广宣以及贴息返利活动吸引大量用户注册使用，但在活动停止后，用户即回归到正常自然增长状态，降幅显著。后续对用户转化和价值情况进行深入分析，调查本次活动吸引用户群的ROI(投资回报率)和LTV(生命周期总值)，以此来给活动的价值总结定性。", "用户获取成本往往有两类指标含义。一是全部用户的获取成本，二是付费用户的获取成本。前者为所有移动app通用的观测指标，而后者对于移动金融和电商类app尤为的重要。移动金融机构普遍会采用以往信用卡生命周期管理的方式来测算用户的终生价值(LTV)。想想这些年倒闭的P2P，除少数有资本和集团输血的机构外，没有机构可以长期在用户获取成本大于终生价值的背景下生存，更何况此处提到的“成本”还未将金融机构的核心风控、坏账率因素纳入整体成本考虑中。", "对于中大型金融机构而言，每年在运营获客工作上的花销预算通常在前一年由计财部门审核完毕，后期往往难以追加调整。因此用户获取成本不仅仅用作传统的渠道价值考核，也直接与全年目标用户量这一重要KPI戚戚相关。移动金融app的用户获取成本主要是由渠道投放费用+红包(新客)优惠两部分组成，大致如下图：", "当然，渠道推广不仅仅承担获客职责，同样也起到整体品牌推广作用。业内数家知名的P2P公司去年在渠道推广费用上花销超过1亿人民币。而红包通常被认为最直接有效的获客手段，各家移动金融app在此早已大做文章。最典型的是理财和消费金融类的app，直接赠送1-10元不等的现金红包，或者高额的信任理财贴息(通常在现有年化收益上增加0.5-2%，但为控制成本，认购金额和时间上往往有较多限制)、分期免息等优惠措施。", "国内移动金融app市场已逐渐告别增量市场，转而成为存量市场。单个用户的获取成本从早期的20-30元增长到如今看齐信用卡超过百元的地步。各领域在短期内并没有下降的趋势，获客成本会逐渐成为运营与市场、风控，计财部门争议的焦点。", "拆解移动金融的含义，即在移动端通过电子支付的方式购买产品。其中既包括虚拟的金融产品和服务，同时也包括物理产品。而绑定银行卡是这种购买行为发生的第一要素。很多机构会将绑卡量(率)视作生命线，究其原因在于：", "1) 金融的重决策链特性同样影响到移动金融领域。用户愿意将自己银行卡的交易接口与该app关联，往往代表着重要的排他信任感，是交易发生的开端与前兆。", "2) 获取用户真实的信息，为后续精准营销获得更多精确数据;", "绑卡量的因素往往取决于运营机构自身的背书以及用户业务的主观需要。运营中往往会用积分体系、功能歧视、礼金奖励来刺激用户完成绑卡行为——尽管对于后者而言往往容易引来羊毛党。同时对于绑卡指标的关注仍应建立在转化漏斗之上，观察每一步转化的用户流失情况，来制定和修改下一阶段的运营策略。", "某移动金融app在统计时间内的绑卡转化情况，由于是非银行背书类产品，其12%的绑卡转化率表现整体来说较出色。但仍需关注的在使用-注册，以及注册-绑卡阶段的大量用户流失。运营人员需与产品经理及开发人员沟通，测试优化注册步骤绑卡步骤。同时采用小范围样本调研对的方式，对于在绑卡阶段流失的用户进行电话/问卷访问，了解其未绑卡的痛点在哪里。", "依照传统的移动app分析法，活跃从日/周/月的时间维度进行测算，查看整体活跃用户在统计时间内的数量及占全量用户的比例。除此之外，考虑到目前移动金融app整体呈现大而全的趋势，集合理财、股票、支付、贵金属、甚至电商等功能，全方位管理用户资产。因此除整体活跃量的分析统计外，还需关注用户在各副业务板块的转化活跃情况，以至少完成过一次购买/参与业务行为作为活跃转化指标。", "通过对app活跃的每日监测和预测，来判断活动效果的好坏。例：上图系某移动金融app在去年下半年上线的红包活动，旨在拉新并提升活跃量。就活跃数据而言，虽然下半周的活跃止住颓势，曲线有所上扬，但考虑到其千万级别的用户量，整体上并未比未进行红包时的活动效果显著。由于当下大量的存在，用户对于现金类红包的营销活动的敏感度较低，在这之上进一步的重点包装和宣传会是后续红包活动的重点，如支付宝的集五福活动。", "图：从单向的业务活跃转化到业务关联交叉使用分析，有助于运营中进行关联业务的推荐，并分析交叉用户使用特性，建立种子用户画像特征。", "留存的概念一点也不陌生，常见的统计周期有次日、7日、14日、30日等。通常又分为新用户留存和活跃用户留存。", "1) 新客留存：统计新注册用户在统计周期内的留存情况。通常配合新上线的活动和渠道推广后的分析监测。移动金融app在短期内未完成获客工作，通常会进行大量的红包让利活动，也因此吸引到大量被成为“羊毛党”的用户，尽管现有黑/白名单、用户标签、IP监测等多种方法，但此类用户难以完全杜绝。新客留存可作为监测薅羊毛用户的指标之一。", "2) 活跃留存：活跃用户的留存情况。主要监测现有用户对于app的使用(非交易)黏性。手机银行类、信用卡类和证券类app高频使用的特点使得他们在这一指标中的表现十分突出，比如证券类app，交易日活跃用户的次日留存可以超过8成，而相对理财及消费金融类的表现相对较低。因此对于移动金融app而言，功能模块的完善除了全方位管理用户资产外，以不同的功能吸引用户使用，形成长期黏性也是利好方面。", "图：渠道投放价值示例", "该数据主要用来分析当前用户价值以及渠道获客价值，该项数据需要长期对比监测。在游戏行业中，用户终身价值=用户平均收入(ARPU)*用户生命周期(LT)。但移动金融app不同情况在于，用户在贡献极高营收(Revenue)的同时，其购买的金融产品/服务成本(Cost)不能简单等同与游戏行业中的道具，后者几乎是零边际成本。因此恰当的计算方式是用户单月贡献的利润*生命周期，比如证券行业中使用其换手费，理财行业使用管理费以及息差，通常计算周期为一个月。考虑到金融app的长决策性，计算平均用户首次购买时间与注册时间(或者下载时间)之间的平均时长，来确定合适的起始时间计算点。", "单客贡献总利润=LTV-获客成本", "ROI=LTV/获客成本", "单客贡献总利润和ROI同样用作渠道价值的评判标准。在有更多设备数据的前提下，计算手机品牌、型号的整体利润贡献量，以此来指导未来活动运营的渠道投放选择以及活动奖品的选择。", "国内现有大量数据服务商提供app数据监测，如TalkingData、易观、Questmobile等。定期观察自身app在各移动金融细分领域的排名、覆盖情况，以此了解行业及竞品情况。", "内容运营的分析指标之一，以周/月时间维度监测，同时比较竞品表现。微博、微信、贴吧、应用商店论坛以及各金融细分领域的论坛(如信用卡的我爱卡等)", "作者：黄嘉伟，TalkingData高级咨询顾问，主要提供移动金融领域数据咨询服务。", "如有问题交流，欢迎邮件至：williamhuang@outlook.com 或者微信：149394109", "本文由TalkingData 黄嘉伟 投稿至36大数据，并经由36大数据编辑发布，转载必须获得原作者和36大数据许可，并标注来源36大数据，任何不经同意的转载均为侵权。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75530"]},
{"module": ["干货教程"], "note": ["\n2016中国大数据技术大会中，上来自科大讯飞大数据研究院副院长谭昶，他带来了《讯飞大数据的实践与思考》的主题分享。他从语言谈起，分享科大讯飞在大数据领域的实践，他表示目前科大讯飞依..."], "title": ["科大讯飞 谭昶：大数据的实践与思考"], "content": ["2016中国大数据技术大会中，上来自科大讯飞大数据研究院副院长谭昶，他带来了《讯飞大数据的实践与思考》的主题分享。他从语言谈起，分享科大讯飞在大数据领域的实践，他表示目前科大讯飞依靠海量实时的语言数据，目前他们的大数据技术已经在精准营销、个性化推荐和智慧城市方面得到广泛应用。", "对于讯飞大数据能力平台Odeon他做了详细的介绍，他表示平台自2010年上线以来，目前日增数据达到100TB。另外，结合讯飞大数据与人工智能技术构建的讯飞DMP平台。已经完成人生阶段、行业偏好、购物兴趣、媒介兴趣5个大类1700个子标签的用户填充工作。累计覆盖12亿终端设备。", "随后，他重点介绍了讯飞大数据技术在金融、游戏、教育、交通的应用实践，通过实际的应用他也分享了科大讯飞在大数据领域的思考，包括服务形态、市场变化、生态体系、技术需求。", "•8.9亿用户日均交互30亿次", "•输入法3.6亿用户，语音输入超过12%", "•广告算法决策较人工运营成本降低1.2倍", "•覆盖全国26省106市近10000所学校", "•CET考试作文阅卷超过人类与家", "•个性化学习试点成绩平均提升10%", "•社管业务服务安徽、江西、吉林等数十个城市", "•前台办理人员减少50%，办理时间缩短为2-3天", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "•建立便民、便企权力清单库，为民、为企事项6727项。", "•在全国20余地市，汇集503个部门5870类政务数据资源。", "•窗口由原来8-10个减少到现在2-3个，数量减少70%，前台办理人员减少50%", "•事项办理由原先10-30天，缩减到现在1-5天，办结率99.7%，越来越多的事项当场办结", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75554"]},
{"module": ["干货教程"], "note": ["\n文 | Han Hsiao\n如今是大数据的时代，很多学校都开设了大数据相关的专业和课程。据我了解到，自2015年教育部开设了「数据科学与大数据技术专业」，首批只有北京大学、中南大学和对外经济贸..."], "title": ["全国有哪些高等院校开设大数据相关专业?"], "content": ["文 | Han Hsiao", "如今是大数据的时代，很多学校都开设了大数据相关的专业和课程。据我了解到，自2015年教育部开设了「数据科学与大数据技术专业」，首批只有北京大学、中南大学和对外经济贸易大学三所学校申报成功。", "高校填报志愿，过来人的建议，通常情况下：", "城市比学校更重要，学校比专业更重要。", "现在还有很多所高校建立了「大数据研究院」，招收一些在职or非在职研究生，具体如下表。至于如何选择，个人觉得「计算机科学」和「统计学」底子比较好的学校都是不错的选择。", "具体你可以去到各自的官方网站去了解更多的信息。", "排名不分先后：", "北京大学——", "清华大学——", "人民大学——", "复旦大学——", "中南大学——", "西南交通大学——", "贵州大学——", "南京邮电大学—— ", "很多都是在2014年到2015年间成立，来不及招生，通常采取从别的学院抽调的形式。", "比如中南大学：关于选拔2015级计算机科学与技术专业(数据科学与大数据技术方向)：", "计算机科学与技术(数据科学与大数据技术方向)主要培养大数据科学与工程领域的复合型高级技术人才。毕业生具有信息科学、管理科学和数据科学基础知识与基本技能，掌握大数据科学与技术所需要的计算机、网络、数据编码、数据处理等相关学科的基本理论和基本知识，熟练掌握大数据采集、存储、处理与分析、传输与应用等技术，具备大数据工程项目的系统集成能力、应用软件设计和开发能力，具有一定的大数据科学研究能力及数据科学家岗位的基本能力与素质。毕业后能从事各行业大数据分析、处理、服务、开发和利用工作，大数据系统集成与管理维护等各方面工作，亦可从事大数据研究、咨询、教育培训工作。", "专业名称：计算机科学与技术专业(数据科学与大数据技术方向)，本科四年制;", "选拔对象：面向2015级本科生，计划招收2个班，共60人。", "1.选拔范围：从信息科学与工程学院信息类专业选拔1个班;从软件学院、交通运输学院、机电工程学院、航空航天学院、物理与电子学院、数学与统计学院、冶金与环境学院、商学院、土木工程学院、能源科学与工程学院、化学化工学院、粉末冶金研究院、地球科学与信息物理学院、材料科学与工程学院等学院A类工科，本科专业中选拔1个班。", "2.组织方式：相关学院学生自愿报名。。", "3.考察方式：由信息安全与大数据研究院负责面试选拔工作，主要考察学生的科学素养、人文素质及英语水平，按照面试成绩从高到底录取，录满为止。", "说点题外话，预测未来所有企业，都将实现互联网化，互联网化后企业就肯定会存留海量数据。当前大数据热潮下，很多高校跃跃欲试想分一杯羹，比如金融专业和信息管理专业可以成立个「金融大数据研究院」，马克思主义专业和新闻系专业可以成立「舆情监控大数据研究院」，环境与化工专业可以成立「空气污染物大数据研究院」，中文专业和计算机专业可以成立「文本数据挖掘研究院」。数据只要和业务结合就能发光发热，相信「数据科学家」、「数据分析师」、「数据挖掘工程师」在未来将会是每个行业必不可少的岗位。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75588"]},
{"module": ["干货教程"], "note": ["\n文 | 沈浩老师\n如何制作词云?\n————————用软件Tagxedo可能需要翻墙了!————————\n近日在微博上大家都在问《个性化词云》制作方法。\n\n下面简单介绍实现步骤和思路：\n随着微博研究的深入，社会网..."], "title": ["词云制作 | 词云可视化——中文分词与个性化词云制作"], "content": ["文 | 沈浩老师", "————————用软件Tagxedo可能需要翻墙了!————————", "近日在微博上大家都在问《个性化词云》制作方法。", "下面简单介绍实现步骤和思路：", "随着微博研究的深入，社会网络分析和可视化技术的需要，面临中文处理问题，开始钻研文本挖掘的问题，过去的传统的数据挖掘一直研究的是结构化数据，文本挖掘和意见挖掘涉及内容更多，特别是中文处理是不可逾越的障碍!", "注：俺的中文不好，甚至想过把中文google translate成中文进行研究，英文的文本挖掘技术基本了解!哈哈", "从自然语言处理，网络分析、文本挖掘和意见挖掘角度看，主要解决以下内容：", "网络抓数据—MySql和Hadoop存储—API接口—创建网络数据—Knime和R语言挖掘-KOL意见领袖和网络分析—中文语料和文本语义—R或Python语言与分词—用户词典构建—情感词典建设和情感分析—文本聚类分类—归并文本挖掘与网络分析—规则建模推荐算法—PMML模型与云端部署—API插件和接口程序——网络cytoscape可视化分析—Gephi动态可视化分析—Xcelsius仪表盘与Tableau可视化分析—Echart——Javescript——大屏或移动应用等;", "在学习的过程中，我们成立了数艺智库和中国传媒大学数据可视化兴趣小组，每周活动一次。同学们都是80、90后的年轻人更有创新意识，学习也快，特别是小技巧和工具的掌握。我特别强调工具的应用，因为同学们都是文科背景，coding的能力不是我们的强项，学会和掌握最好最适合的现成软件工具是我的指导思想。", "数据可视化兴趣小组的参与同学热情很高，特别是高年级同学传帮带，象谈和、德凯、若晨等同学成为小组的主要指导教师。这个工具也是同学们先找到了，在谈和同学讲解的过程中，我提出了创意要求。", "特别是在数据可视化领域，根据兴趣不同学习了：PS、AI、PREZI、PPT、Xcelsius、Excel、D3js、Processing、Tableau、Romabi、Echart、Zoomchart等，词云的制作也是大家感兴趣的。", "下面来说说词云制作，个性化词云制作：", "首先：个性化词云制作非常简单，瞬间就可以完成，甚至可以说是一种雕虫小技，在线制作。当然，我还是希望把研究和创作思路告诉大家：", "你为什么需要制作个性化词云，是艺术品还是研究分析内容的一种表现方式，是文本挖掘技术的可视化，还是为了传播更方便。形式大于内容，在有内容的前提下，可视化也是一种分析!当然我更倾向文本挖掘后的内容可视化。当然，我也不反对纯粹为了表现或者玩玩的可视化，玩也是一种学习。", "因此选择什么主题，什么主题适合用个性化词云表现就更为重要了。比如：为大人物明星打标签，品牌logo打标签，SUV汽车打标签，电视台台标打标签都是好的创意和选择。", "个性化词云是依赖语料和抽取语料关键词呈现的，如果你有了要表现的词云标签，就可以直接制作词云了。记住这里要有两个数据：1)标签关键词 2)关键词词频，词频决定关键词的显示大小。语料的收集依赖你的主题和想法，从应用的角度我们主要是为了分析微博，所有微博是最好的语料来源，也是文本挖掘的结果。当然你可以从任何途径和资源活动要表现的语料。", "@数据挖掘_PHP", "交作业啦~对数据库内1,065,827用户分性别做昵称词云图,抽查了几个,貌似不错.女性爱用叠词做昵称,比如”佳佳/木木/妞妞/果果/格格”,男性爱用状态词做昵称,比如”胖子/路上/大侠/俱乐部/魔王/奋斗”,", "当你获得语料后，如果需要抽取关键词，就需要学习中文分词。中文分词对一些人是技术和障碍，但是现在中文分词是一个简单而通用的技术，很多软件和在线工具都可以完成一般意义下的分词，例如：Rweibo、weiRbo、中科院张华平老师ICTCLAS2012、武大沈阳老师的ROST CM、bosonnlp等，这里推荐考虑用Python的结巴工具入手，分词主要考虑是否可以用户自定义词典和剔除、词性标注等。少量语料的分词比较简单，但海量语料的分词要有一定难度和数据处理能力。", "特别强调：任何中文分词如果能够加入人工干涉和主观判断都会提升准确性和有效性;下面介绍的在线个性化分词本身也可完成分词(包括中文分词，体验效果：不同语料和多少，时好时坏)", " ", "分词和抽取关键词、词频完成后就可以制作个性化词云了，但有个前提，你需要收集与主题相关的个性化主题图片。这是一个艺术和技术融合的过程，原则上可以不用任何修饰就可以完成词云，但是如果图片不理想或者找不到你需要的构图，就需要自己抠图和PS一下了。当然软件自带有很多预设模板可以选择，比如：高跟鞋、烟斗、鸡鸭鱼动物、形状等。", "你已经看到了制作软件，Tagxedo词云 http://www.tagxedo.com 这是一个在线制作词云的工具，不用license。其实我不用讲，你应该能够学会的，操作非常简单，这里只是稍微介绍些中文词云要点：", "上面基本就完成了个性化词云操作，但往往有些细节要注意，比如把软件的水印剪裁掉，如何将一组词云拼接，增加必要的说明和意义解读，当然别忘了说是@沈浩老师 教的。哈哈", "注意：我更愿意看到用词云制作的产品与原来有冲击力的图片进行PS拼接，一个人像的真实头像与词云的融合一体，可能更艺术。", "下面是同学们的作品：(感谢同学们的作品，不一一提及了)", " ", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75629"]},
{"module": ["干货教程"], "note": ["\n文 | Fiona   译者 | 陈炎昌\n1、 前言 Preface\n数据中心研究，业内也俗称为“数据中心风火水电”。“风火水电”按照字面说法，那就是数据中心决策主要考虑的是能源层面，包括电力供应和自然冷源..."], "title": ["大话数据中心，分享数据中心决策和选址的业务价值"], "content": ["文 | Fiona   译者 | 陈炎昌", "数据中心研究，业内也俗称为“数据中心风火水电”。“风火水电”按照字面说法，那就是数据中心决策主要考虑的是能源层面，包括电力供应和自然冷源。这个说法有一定道理，但却不能解释很多数据中心选址决策的现象。如果只要电力供应好，有自然冷源利用的时间，那么都往北方电力充足地方建数据中心就可以了，可为什么南方有这么多数据中心?", "从地理角度再放大看看，香港、新加坡，这些都不见得电力供应和自然冷源有优势的地方，甚至处于热带气候，却成为亚太数据中心集中地?如此看来，“风火水电”也不能全面揭示数据中心选址背后的逻辑。有很多时候，一些数据中心选址和决策根本就不是技术驱动，完全可以是业务驱动。在很多公司决策逻辑优先级里面，往往不是技术优先，可能会有以下顺序：", "相信这种逻辑，很多读者不会陌生，不要说建设数据中心选址，对于制造业工厂选址，办公室选址，研发中心选址，发电厂选址，大部分盈利机构选址都很类似。在商业决策中，技术更多是为商业价值服务。以日本为例，无论中国GB50174规范还是TIA 942，都不建议数据中心建设在地震带附近。", "如果完全按照技术分析，那就等于东京不适合建设数据中心。但是东京有多个数据中心，包括KDDI、NTT、Equinix等等，他们的解决方案就是建筑抗震等级满足日本较恶劣情况。所以在业务和客户需求面前，很多技术问题无非就是，花多少成本(人/财/物/时间)解决这个风险而已。在大部分公司里面，数据中心和IT部门并不是盈利部门，对价值的贡献都是间接的，技术部门更多处于执行者和解决问题者角色。", "所以，笔者比较认可一个观点，能用钱解决的技术问题，不算是问题，关键得说清楚业务价值，或者通俗点，能否赚，赚多少。", "笔者参考了国内同行专家关于数据中心决策和选址的文章，发现大多数以技术性思考为主，缺乏技术结合商业价值的思考。笔者也还没有发现一条比较清晰的逻辑脉络阐述相关决策背后的逻辑，以及如何体现价值。本文章着眼实干，旨在填补技术和商业价值之间的空白。", "同时，目前国内关于数据中心选址的文章都是找个空地自建，而没有包含如何选择租赁机房。除了自建机房，租赁机房选址也有不同的考虑点。所以，笔者想结合近几年参与过一些国内外数据中心考察和选址经验，借鉴国内外同行总结最佳实践，和读者分享关于选址的几点：", "1) 选址最终为了什么价值?", "2) 什么是选址和决策的核心要素?", "3) 资产模式，技术选择，成本模型是如何影响决策?", "本文相关素材图片均为笔者原创或来自网络上公开图片和素材。因笔者水平有限，文中若有谬误，请各位专家同行指正，欢迎探讨。", "Business value behind site selection", "数据中心选址，笔者认为不需要有什么神秘，关键是抓住有什么因素影响价值。某种意义上理解，数据中心选址未必复杂过买房选址。当然，数据中心规模通常比普通商品房大很多，那我们对等比较，大型住宅区选址，其实也可以很复杂。简单可以想象到的是，怎样可以使人更多购买这个小区，而且人们觉得小区升值。大多数购买房地产的用户无非考虑重点也就几个因素：学校学位、户口、交通、生活配套作为关键价值点，然后就是均价，户型，物业服务，绿化环境，首付比重，贷款金额等等。这么看来，买房也是为了实现价值，只是根据每个人的诉求不同，对价值的权重有所差异而已。", "笔者再举一个从百度百科摘录如何对", "文中提及快递网址选址也有几个核心因素：", "大家不妨搜索一下餐厅选址，银行网点选址，背后都有很多相似的地方。通过两个日常例子会发现，其实各行各业选址都有一门学问，但归根到底，是考虑如何创造更多业务价值。同时，这个例子也说明了，不同行业因为业务诉求不一样，盈利、客户、市场模式不一样，核心因素及其权重也有所差别。所以，数据中心选址，关键是抓住核心要素，然后使业务价值最优化。", "在下一章开始介绍核心要素时，笔者想重申一点，由于各个公司的业务和盈利方式不同，选址的核心因素权重是有较大差异的。以美国五大科技公司为例，他们在欧洲数据中心选址就有较大差异：", " ", "读者看完本文后，可以思考这些公司的业务，然后根据核心要素去分析这些选址背后的逻辑。一个不可否认事实，这些几千万过亿数据中心投资的选址决定背后，绝大部分都是业务、技术、财务等多个部门综合分析后收敛的结果。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74391"]},
{"module": ["干货教程"], "note": ["\n文 | 飞林沙\n2015年底，我和朋友讲，我从未经历过这样忙的生活。\n2016年底，我和朋友讲，2015年的忙，那算个屁啊!可是相比于如此忙，我从未经历过如此不开心的生活。\n入职公司两整年，看着..."], "title": ["数据价值与数据产品孰轻孰重？ 大数据科学家飞林沙的年度总结"], "content": ["文 | 飞林沙", "2015年底，我和朋友讲，我从未经历过这样忙的生活。", "2016年底，我和朋友讲，2015年的忙，那算个屁啊!可是相比于如此忙，我从未经历过如此不开心的生活。", "入职公司两整年，看着公司从即将B轮后的公司，走到今天即将D轮的公司;看着公司从几十人变成几百人，感觉像是一眨眼的事儿。", "婚后生活的第一年，我不想为自己找工作忙这类的理由，也许自己就是个自私的人，爱到最后，要么伤人，要么受伤。我更是自私地希望对于我而言是后者，不是因为善良，而是这样我可以不必太过自责。", "和以前一样，想到哪儿写到哪儿。", "对于一个互联网服务而言，尤其是对于一个企业级服务而言，网络的稳定性以及容灾策略成为重中之重，开发者服务发展至今，性能本身往往不再是评估标准，尤其是对于大部分中小客户而言，云服务往往不会因为性能指标不达标，而多是因为后续的稳定性，以及在处理中国糟烂的网络环境上，当然这其实也是稳定性的一部分。", "毕竟这不是一篇技术文章，更不是一篇广告文，没必要写的太细致。简单说说，是希望明年，后年我可以进步到打自己的脸，或者嘲笑今天自己方案的肤浅。", "在中国，最突出的问题就是南北互通的问题，对于大部分互联网服务而言，完全可以靠大量的CDN来解决问题，但是对于云服务这种基本没有静态内容的情况，CDN变得不再可行。对于我们而言，南北互通主要体现在两点，第一是全国各地，或者说世界各地终端用户与服务端的长连接维持情况，第二是全国各地客户分散在不同区域的机房到服务端API的网络连接情况。", "对于前者而言，解决方案其实很固定，就是尽可能多的接入点，然后通过服务端与客户端的配合来为客户端找到最佳接入点，这里后续会再提一小点心得。后者其实是最麻烦的，客户的机房情况多种多样，从南方机房到北方机房的时延在高峰时段往往延迟都会达到100ms以上，更不要说很多客户在一个什么鬼机房里或者某些不靠谱的云服务厂商那。", "为了解决这个问题最简单想到的就是建设南北双机房，这样可以同时解决网络和多机房容灾的问题，不过折腾了一段时间后发现这个事儿完全不靠谱，南北拉专线的成本是我们负担不起的，那么中间庞大的数据同步，同步过程中各种由于网络波动引发的失败就成了我们短期内没办法完美解决的问题。", "这个时候我开始思考两个问题，如果南北专线我们拉不起，那么同城专线我们拉不拉得起;如果南北数据互通我们解决不了，那么不互通到底能不能解决我们的问题。顺着这两个思路想下去，一个技术上不完美但是足够高可用的实用主义方案就产生了。", "其实问题如何解决并不重要，不同的公司不同的业务不同的方案，更重要的是通过上面的问题我产生的是两个思考。", "我先举两个例子吧：", "。客户端每当连接不到服务端的时候，就会采用失败重试策略。如果这个时候正巧这个机器过载，那么反复的重试会导致这个机器彻底崩溃掉。 再进一步，如果客户端再尝试连接某个IP的时候失败，然后他会做的是什么呢?连接IP列表里的下一个IP。如果正巧这个IP所在的机器挂掉了，然后所有连接该IP的机器都开始连接下一个IP;下一个IP的机器也不堪重负也挂掉了，这就是雪崩效应。", "但是IP信息往往是不可靠的，那么如何通过客户端和服务端的配合找到最合适的接入点，而客户端连接后是否可以提供合适的样本数据为服务端作为训练参考，为后续连接客户端提供更好的接入服务。", "在一个公司中，同样的问题会有很多，不仅仅是客户端与服务端的配合，还有服务端与运维的配合，运维的职责是保持整个服务高可用，自然希望把上线节奏放缓走完整流程充分测试;而对于服务端来说，快速迭代却是他们所希望的，我相信没有任何一个工程师愿意在上线前走一遍冗长的上线流程，整个时候怎么办?", "如果说这些还都是相通的，那么涉及到业务端和数据端配合就显得更复杂，数据端大多是数学系背景，精于数学模型的计算，而在工程能力上欠缺;而业务端计算机系的背景会导致和数据端的思维方式的不同，而在数据存储、交互模型，尤其是在线推荐引擎上，往往会产生巨大的意见分歧。", "针对这个问题我请教了几个不同公司的人，大家的解决方案无外乎两类：", "，最终总是能得出一个不好也不坏的折中方案。这明显是一种被迫的方案。", "即引入架构师或者技术委员会这样的三方角色，他们不会投入到具体的编码执行工作中，因此可以跳出利益纷争纯粹从技术方案上来思考问题。", "从我个人而言，我更认同第二种方案，只是对于一个中小公司来说，到底这个角色的工作饱和度如何保证是我最让我犹豫的地方，也许这也是我新年需要解决的问题之一。", "用过去我强调了无数的数据概念来说，模型的用处是什么?现实世界的情况复杂多变，以至于基本的规则方法无法充分解决问题，或者需要几百个甚至几千个规则才能解决问题。这样不仅难于维护，更麻烦的是这样的规则引擎无法适应现实世界的复杂变化，尤其是面对人类所难以洞察的高维数据 和 难以处理的海量数据(尤其是噪音数据)时，且不说规则引擎的覆盖性和灵活性，更麻烦的是无法区分某条规则是否是属于过拟合。", "先用这个方法论来看技术方案的选型，对于大部分业务来说无外乎对应到的就是几种情况，所以不要寄望于用一种开源组件/技术组件能搬过来完全解决问题，将完整的业务场景拆分，然后思考每种业务场景的技术重心和技术难点。做数据的同学不妨把这个当成一个Boosting的过程，无外乎分成两类，第一种是按照场景选择分类器，第二种是用多个分类器共同完成同一项任务。", "前者随便举个例子，比如说将同一份数据存成多份，然后分别做不同的索引结构，按照查询条件分发。 后者最常见的其实就是用Redis+MySQL共同应付查询。那么后续还能做什么事儿?做机器学习的同学肯定非常了解，就是：1. 调参。 2. 将选择分类器本身也根据一些Feature变成一个分类器，从而提高覆盖度。 3. 将简单的投票选举的组合方式变成类似AdaBoost的优化方式。", "其实思路都是互通的。", "这一年，大大小小做了三四款数据产品，我给自己勉强打30分。在过去的时间，我过度思考了数据的价值，而忽略了数据产品的本质依然是产品。无论是数据产品、开发者产品还是用户级产品，思考的方式都不是我能做什么，而是站在用户/客户的角度，我希望在你这儿得到什么，而作为一个合格的产品经理，需要的能力是我能够将用户的需求抽象开来，用我的方式来满足。而作为一个优秀的产品经理，是让用户可以惊叹原来你居然可以这样做这件事情。", "列举几个自己犯下的错误，我们在做一个应用洞察类的项目，既然是叫做应用洞察，那么自然关注度都集中在”应用洞察”这件事情本身上，于是我开始从各个角度剖析应用，现在回想起来，这真的是可笑的”name-based product”。那么回想一下做一个用户级产品的思路是什么样?先思考这个产品的目标用户是谁?这个用户有什么痛点?我们如何用这样的方式来解决问题?再接下来去看其他的同类产品是如何解决问题的，我们是否有更好的方式来解决?", "数据产品同样如此，当整个数据产品一直在谈数据价值的时候，可能本身就是数据产品的失败吧。", "公司成长为一个近500人的公司，团队从几十人变成几百人，管理方式的调整对我而言其实是最大的挑战。", "A. 在过去的几年，我花费大量的精力在技术上，因为一直努力希望自己做一个技术领导者，而不是一个技术管理者。当随着团队规模的增大，这种方式也变得不再持续。", "避免出现精力过度分散而导致每个产品都无法观其全貌，做到满分，然后用每周的固定时间来review其他产品的进度，做技术管理者。", "B. 在Google SRE的一书中，其中有一章讲到", "这就像人的身体一样，如果从来不生病其实是很可怕的事情，反而是那种经常生个小病，更新下病毒库反而才更健康。最近我听到两个人说了这句话，以至于我不知道出处是哪儿。只要这个错误不会把公司搞垮，就放手让大家去搞吧，在错误中成长也许必须的事情。", "C. 我和几位不同的从初创到上市的朋友聊过，在团队的逐渐成长过程中，他们都分别做了怎样的改变，各自有各自的说法，但是在一点上是达成的统一。", "在前几天我批评过一位部门的Leader，我很赞赏他努力的工作态度，也并不怀疑能力本身，但是恰恰是这种努力带来的问题却是缺乏了技术的前瞻性。换句话说，作为一个技术领导者而言，一定是不能让自己陷于”忙”的状态中的，否则一定会做的多思考的少，作为领导者来说，这是最可怕的事情。", "在小团队中，人少事多，每个人都背着繁重的需求压力。但是当团队成长到一定规模，公司发展到一定阶段时，必须要将业务工作和平台性工作分开。例如数据开发和数据平台必须剥离，否则必然出现大家都忙于应付日常的需求处理而缺乏平台性建设;例如高端职位招聘必须与常规职位招聘分开，否则必然会出现高端招聘被搁置。简而言之，简单的事情必须与复杂的事情分开，否则必然会处于一直忙于简单的事情，而缺乏时间建设平台，然后导致恶性循环。", "有几位候选人都问过我，你们公司加班么?我都讲，确实加班，但是我希望公司不要加班。", "今年公司的某几个部门加班加的有些过于疯狂了，我在感激大家的同时其实更多的是自责。", " 无论是哪种原因，作为CTO都应首先被问责。", "频繁加班的坏处是显而易见的，无论是恶性循环，还是缺失学习的时间，还是当被需求赶着走的时候缺乏对于基础设施的积累。", "这算是我新年最大的愿望吧，希望无论我做怎样的改变，都希望团队不要再加班得这么凶。", "关于生活，是我可能最不想讲也不想总结的话题。", "读了27本书，创了近年来的新低，一方面是因为时间被打的太散，没有足够的时间，一方面也是心里压力过大，没有办法静下心来高效地读书，这实在是很可怕的事情。", "30几部电影，同样创了近年来的新低，这一年，走入影院的次数屈指可数。", "美食?今年一年吃过的餐馆数量同样创下了近年来的新低，一方面是结婚后不太方便再约人一起吃饭，一方面我下班的那个时间只有楼下的早餐包子铺还在开张了。想想，也好可悲。", "朋友?这一年来新结交的朋友一只手都数的清，好吧，也许砍掉几个手指都没问题。已有的朋友有的去了其他城市，有的断了联系，又没有新的朋友认识。我还真是可笑。", "那生活?我好像真的快没有生活了。我和老板讲，能不能给我在公司装一个浴室，我自己带个床，然后我就把我租的房子给退了。也许他以为我在开玩笑吧，可是我是认真的。", "有人问我，那你开心么?我说我不开心。他说人生就那么短，你要让自己开心得活着。我说我不知道怎么样能让自己开心，对于生活，我感到绝望。他说，那你打算怎么办?我说，我想得绝症，那种可以马上死掉的绝症。 我去体检，不是希望自己没事儿，而是希望能查出自己有些什么问题，可能我会更加开心。", "我在清理自己的朋友圈，一年前我写，我希望有个女朋友，可以陪我深夜泡咖啡厅，可以陪我加班，可以陪我写代码，陪我解数学题，可以陪我聊娱乐八卦，也可以陪我聊文学历史。可是这些似乎离我越来越远，我有过这样的女朋友么?也许曾经有过。可是我们吵架吵的很凶，因为解题思路不一样，因为历史观点不同，因为，当然因为我们都太自信了。所以即便出现了，我想自己可能也不会幸福。", "是的，所以我对感情也绝望了，我觉得自己不会幸福了。当然，我现在无比享受一个人的生活，一个人加班，一个人逛图书馆，一个人去咖啡厅，一个人吃饭，一个人看电影。我在想，我到底讨厌的是两个人的什么呢?", "做数据做了太久，自然会用数字化来思考问题，我们没给对方带来了任何收益，而又让对方增加了责任、牵绊和压力。那收益就是负的，我们为什么还要在一起呢?", "可能我太过于自私了吧，工作对我已经是最重要的事情，耗尽了我大多的能量，也许我不再想在生活中再给我添加一点点不快乐了吧。", "之前每年的年终总结，我都是开开心心地，然后丢下彩蛋，那我许一个新年愿望吧，希望明年的年终总结，是有彩蛋的。", "更多数据科学家飞流沙文章>>>", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74309"]},
{"module": ["干货教程"], "note": ["\n文 | 江和慧\n基于数据库的分布式存储和计算是使用分库分表的思想实现数据存储，使用mapred的思想事项sql计算。\n计算过程：将输入sql经过此法，语法，语义分析，集合表结构信息和数据分布信..."], "title": ["基于数据库的分布式存储和计算（PPT）"], "content": ["文 | 江和慧", "基于数据库的分布式存储和计算是使用分库分表的思想实现数据存储，使用mapred的思想事项sql计算。", "计算过程：将输入sql经过此法，语法，语义分析，集合表结构信息和数据分布信息，生成包含多个阶段(简称stage)的执行计划，这些阶段具有一定的依赖关系，形成多输入单输出的任务树。", "每个阶段包括两种sql，称为mapsql和redsql，另外每个阶段包括三个操作，map,数据洗牌和red;map和red分别执行mqpsql和redsql。", "先在不同的数据库节点中执行map操作，map操作执行mapsql，它的输入是每个数据库节点上的表里面的数据，输出根据某个字段按照一定的规则进行分割，放到不同的结果集中，结果集作为数据洗牌的输入;", "然后执行数据洗牌的过程，将不同结果集拷贝到不同的将要执行red的数据库节点上;", "在不同的数据库节点中执行red操作，red操作执行redsql;", "最后返回结果。", "\n", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74424"]},
{"module": ["干货教程"], "note": ["\n作者：胡晨川 （微信号：川术）\n说实话，这个问题非常大，以我浅薄的经验很难解答。一千个人可以做出一万种报告，硬给运营报告一个模板是不现实的。怎么写好商业报告，我建议大家去读《金字..."], "title": ["数据到底怎么用？运营报告的6个必备要素"], "content": ["作者：胡晨川 （微信号：川术）", "说实话，这个问题非常大，以我浅薄的经验很难解答。一千个人可以做出一万种报告，硬给运营报告一个模板是不现实的。怎么写好商业报告，我建议大家去读《金字塔原理》这本书。在这里，我只从自己的经验出发，尽量提炼出几点必备的要素。", "在我看来，数据报告分为两种。一种是追踪型的数据报告，或者成为dashboard。它是对日常业务数据高频率的展现，关键在于发现问题，而不是解决问题。它一般用于回答“怎么了”。这类报告往往是规律性地长期进行制作。另一种就是解决问题的数据报告，它一般是专题型的研究报告，用于回答“为什么”。这类报告往往是不定期地进行制作，而且很有可能是由浅入深的系列报告。", "一份好的报告，关键信息一定是在报告的最前面提现出来的。所谓“开门见山”。", "在追踪型的报告中，关键信息就是最核心最宏观的指标变化，你可以想象汽车的仪表盘，主要指标是永远不会变的。", "对于研究型的报告，关键信息并不光是结论，还要包括能看懂结论的必要信息。这一点是许多分析师忽略的。他们的报告会把结论在最先点出来，但对研究的背景、目的、方法毫无阐述，导致报告的读者没有上下文，也就不能理解结论。这部分的写法我建议参考论文的“摘要”部分。不仅把结论浅显地点出，还要让读者对目的、过程和方法有一定的了解。", "在追踪型的报告中，一般数据以时间序列呈现，建议只遵循“宏观→中观→微观”这条单向的逻辑线展开。追踪型的报告往往需要有多维度的下钻，所以下钻的逻辑一定要单向，不然读者很容易混乱。比如，收入指标可以从地区、用户类型、产品类型三个维度下钻。那么你的追踪报告一定是宏观指标（收入值在时间维度的变化、）、第一层下钻（地区维度展示、用户类型展示、产品类型展示，用饼图展示结构，用折线图展示变动趋势），第二层下钻（地区×用户类型、地区×产品类型、用户×产品类型）。千万不要一会儿是“地区×用户类型”，一会儿又变成“用户类型”。", "这里需要一点心理学的知识。人脑理解信息的层次关系，并不会依据信息本身的层次关系，反而更依赖接受信息的先后顺序。如果我们将某些更微观的数据展示在宏观数据的前面，人们就非常容易将微观数据作为宏观决策的依据，这是非常危险的。所以我们报告的呈现逻辑，千万不能随意。除非，你想用数据撒谎，用某些微观的“漂亮”数据来修饰宏观的“悲催”数据。有本小书叫《how to lie with statistics》，讲述了一些应用统计学撒谎的“歪门邪道”，我只读了一点点，但感兴趣的读者可以去领略一下“用数据撒谎”的魅力。", "对于研究型的报告，建议在做之前梳理好清晰的研究路线图，然后根据研究路线图来展开研究的结论和过程。同时，在现实业务场景中，结论的重要性远高于研究过程。读者极有可能看不懂你用的方法，你的任务也不是向你报告的读者说明白方法，真正重要的是让读者理解结论，并能应用到业务中。", "上面所说的，维度下钻必须遵循宏观到微观的逻辑，但并不是意味着在每一层都要将所有的维度组合（细分数据）展示出来。我们只需要展示最重要的细分数据即可。比如，收入的追踪报告，收入在地区间的差异是长期稳定的，而在产品类别间会有频繁的变化，那么我们就更应该展示产品类别这个维度的细分，因为这里涵盖的信息更多，更有跟踪的必要。", "一份报告中，缭乱地堆积上各种数据图，在外行人看来，可能觉得这个报告“很专业，很高大上”，但在我看来，这份报告的信息获取效率或许很低。", "而我们往往将多个图表的组合做成了“炫目的连连看”，非常可悲。", "所以，我建议报告中要用上三种展示信息的工具：表格、文字、数据图表。首先是表格，一种单位面积承载信息量最大的展现形式。如果一个较复杂的问题，能用一个表格加上些许文字说清楚，那就一定不要做出N张图表。其次是文字，如果一个论点能用简单的话论述清楚，那就不要去做图表。最后是图表，展示的图表一定要简单直观，千万不要在图表中放上太多的图形元素（数据指标），以免扰乱视觉和思维。", "当然，除了不要铺上过多的图表，更重要的是用对图表。什么图表用于什么问题，本书的前三章有详细的介绍，这里不做累述了。", "我认为，我们向人展示数据报告时，不能光展示一个报告，而应该让报告读者能够基于报告内容继续思考。如何做到“继续思考”呢？", "第一， 要让读者有思考的依据。即一份好报告，能打开读者的深入思考之门。一旦他要深入思考，那我们就要能提供他继续挖掘信息的可能。这时，我建议在报告发送时附上用于制作报告的明细数据，供别人做进一步的探索。", "第二， 要让读者能复制制作过程。这一点在研究报告发布时尤其重要。一般情况下，我会要求我的小伙伴在报告发布时，附上取数SQL代码、计算分析代码和详细的过程说明。尤其是在运营相关的研究结果发布时，运营人员能复制我们的过程，也就能快速地学会我们的方法。这样能使知识快速反哺到业务中。毕竟数据分析师是少数，若一个个手把手教运营人员，效率太低了。", "对于数据分析师该不该给出指导业务决策的建议，我曾经犹豫过。但后来我坚定地认为：一个分析师若不能深入理解业务，不能从业务决策者的角度去研究问题，那他只能被称为“取数机”或者“计算器”，最终会极快的被新技术所取代。所以，真正重要的是分析思维，是将业务问题抽象为数学问题的能力，是驾驭分析方法的能力。所以，分析师的分析报告，应该一定必须提出基于自己的思考所得出的策略建议，且一定是能落地执行的建议。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74726"]},
{"module": ["干货教程"], "note": ["\n文 | 艺林小宇\n以流量为中心、野蛮的运营时代已经结束，接下来的时代是以科学的数据作为依据，围绕着用户紧紧做精细化的运营时代。\n我之前在文章《后产品时代的运营之道：数据分析的五种方..."], "title": ["运营人必备的7大技能：数据分析能力是未来运营的分水岭"], "content": ["文 | 艺林小宇", "以流量为中心、野蛮的运营时代已经结束，接下来的时代是以科学的数据作为依据，围绕着用户紧紧做精细化的运营时代。", "我之前在文章《后产品时代的运营之道：数据分析的五种方法论》提到：", "数据驱动运营是未来运营的趋势，也是我们运营人的一个分水岭，在运营的刀耕火种时代已经趋于没落的时候，精细化运营以及变得尤为重要，数据驱动决策是我们运营人必须要面对的挑战也是我们要下意识学的一门技能。", "但也是很多刚进入运营领域的新人一个头疼问题，因为他所涉及到的数据分析方法、方法论、逻辑分析能力以及一些工具的使用，而且一堆数据也是很多运营人员不愿面对的。本章节我们就从如何获取数据、如何分析数据以及一款产品都关注哪些数据维度。", "在我们分析数据之前，就必须得有数据供我们分析，所以我们就得拿到数据，怎么拿到呢?", "数据的来源渠道主要有两种：", "自有数据分析系统——公司自有的数据是最源质化的数据，也是最可靠、最全面的。一般而言，有条件的情况下都是以内部数据为准;", "第三方数据分析工具，这个是借助外部工具获得数据。", "下面给大家介绍主要5款的数据分析工具：", "支持iOS、Android应用数据统计分析", "growingio强大的地方在于无需埋点，就可以获取并分析全面、实时的用户行为数据，以优化产品体验，实现精益化运营。", "仅针对iOS，查看App Store总榜和分类排名。查看产品在App Store 里的搜索度得分，评判ASO效果的标准之一。", "支持ios和android平台。另外，开发者在嵌入统计SDK后，可以对自家产品进行较为全面的监控，包括用户行为、用户属性、地域分布、终端分析等。", "仅支持android平台应用监控。开发者可以查看应用在主流市场下载量、排名、评分评论、关键词排名等数据，还能系统地与同类竞品进行数据对比。", "当然了，数据分析工具不止这5款，如果你们正在使用其他的，也是可以的。使用分析工具我们可以得到以下内容：", "记录那些点击信息，包括没有与网站产生交互的信息;可直接生成链接的百分比，点击分布图和热力图;可统计用户的悬停，将用户潜在行为可视化", "获取数据的方式其实多种多样，关键在于，作为运营人员要了解什么样的数据是重要的，对于这些数据的前后关联，是怎样的，这是一个联动的过程，不是一个单一的行为。", "有了这些数据之后，我们该怎么去分析这些数据呢?哪些是可以为我们所用的额，又有哪些是可以剔除掉的。", "从第三方数据分析工具或者自家的分析后台拿到这些数据后 ，该怎么去分析呢?我相信很多运营人在拿到数据时，都是没多少思路的。要么胡子眉毛一把抓，要么无从下手。这都是缺少分析思路的表现，需要宏观的方法论和微观的方法来指导。", "在上几期的文章中，在艺林小宇的文章《「后产品时代的运营之道」数据分析的那些方法论》中，罗列在我们进行数据分析时经常会使用到方法论，这些方法论在我们进行数据分析时扮演宏观指导的角色。所以说在我们进行数据分析时，应该先找到适合自己的方法论进行指导。主要会用到的方法论：", "数据分析的方法论很多，这里不能一一列举;没有最好的方法论，只有最合适的。下面我详细介绍一下 AARRR 方法论，对于精益化运营、业务增长的问题，这个方法论非常契合。", "对于互联网产品而言，用户具有明显的生命周期特征，我以一个APP为例阐述一下。", "首先通过各种线上、线下的渠道获取新用户，下载安装APP。安装完APP后，通过运营手段激活用户;比如说首单免费、代金券、红包等方式。通过一系列的运营使部分用户留存下来，并且给企业带营收。", "在这个过程中，如果用户觉得这个产品不错，可能推荐给身边的人;或者通过红包等激励手段鼓励分享到朋友圈等等。需要注意的是，这5个环节并不是完全按照上面顺序来的;运营可以根据业务需要灵活应用。AARRR的五个环节都可以通过数据指标来衡量与分析，从而实现精益化运营的目的;每个环节的提升都可以有效增长业务。", "在使用这些数据分析方法论要明确他们的作用：", "再比如，我们在分析APP的数据维度时，会使用到趋势分析法，因为趋势分析是最简单、最基础，也是最常见的数据监测与数据分析方法。通常我们在数据分析产品中建立一张数据指标的线图或者柱状图，然后持续观察，重点关注异常值。在这个过程中，我们要选定第一关键指标，而不要被虚荣指标所迷惑。", "如果我们将我们分析的APP的下载量作为第一关键指标，可能就会走偏;因为用户下载APP并不代表他使用了你的产品。在这种情况下，建议将日活跃用户作为第一关键指标，而且是启动并且执行了某个操作的用户才能算上去;这样的指标才有实际意义，运营人员要核心关注这类指标。", "我们都知道，运营人每天都会跟各种各样的数据打交道，那一款产品都有那些数据维度是我们经常会分析到的呢?", "一款产品(特指APP)的数据指标体系一般都可以分为：用户规模与质量、渠道分析、参与度分析、功能分析以用户属性分析。", "：包括总用户数、新用户数、留存用户、转化率。用户规模和质量是APP分析最重要的维度，其指标也是相对其他维度最多，产品负责人要重点关注这个维度的指标。", "主要是分析各渠道在相关的渠道质量的变化和趋势，以科学评估渠道质量，优化渠道推广策略。渠道分析尤其要重视，因为现在移动应用市场刷量作弊是以及业内公开的秘密。渠道分析可以从多个维度的数据来对比不同渠道的效果，比如从新增用户、活跃用户、次日留存率、单次使用时长等角度对比不同来源的用户，这样就可以根据数据找到最适合自身的渠道，从而获得最好的推广效果。", "主要是分析用户的活跃度，分析的维度主要是包括启动次数分析、使用时长分析、访问页面分析和使用时间间隔分析。", "主要包括：", "功能活跃指标：某个功能的活跃用户，使用量情况;功能验证;对产品功能的数据分析，确保功能的取舍的合理性。", "页面访问路径：用户从打开到离开应用整个过程中每一步骤的页面访问、跳转情况。页面访问路径是全量统计。通过路径分析得出用户类型的多样、用户使用产品目的的多样性，还原用户目的;通过路径分析，做用户细分;再通过用户细分，返回到产品的迭代", "漏斗模型：是用于分析产品中关键路径的转化率，以确定产品流程的设计是否合理，分析用户体验问题。用户转化率的分析，核心考察漏斗每一层的流失原因的分析。通过设置自定义事件以及漏斗来关注应用内每一步的转化率，以及转化率对收入水平的影响。通过分析事件和漏斗数据，可以针对性的优化转化率低的步骤，切实提高整体转化水平。", "5.用户属性分析不管在我们的产品启动初期，还是战略的调整，分析用户画像都有着重要的意义。比如我们在产品设计前需要构建用户画像，指导设计、开发、运营;产品迭代过程需要收集用户数据，便于进行用户行为分析，与商业模式挂钩等等。", "用户属性一般包括性别、年龄、职业、所在地、手机型号、使用网络情况。如果对用户的其他属性感兴趣的，可以到自的微信呢公众号后台或者其他诸如头条、uc等后台看用户属性都包含哪些维度。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74531"]},
{"module": ["干货教程"], "note": ["清华大数据思享会工业大数据系列之“工业大数据管理技术”，清华大学数据科学研究院工业大数据研究中心总工程师、昆仑数据公司CTO王晨分享了大数据和数据管理的理解，并对工业大数据驱动力、特..."], "title": ["一文读懂工业大数据的技术与实践 如何建设工业大数据管理系统？"], "content": ["清华大数据思享会工业大数据系列之“工业大数据管理技术”，清华大学数据科学研究院工业大数据研究中心总工程师、昆仑数据公司CTO王晨分享了大数据和数据管理的理解，并对工业大数据驱动力、特点、难点和实践路径等进行通俗而深刻的阐述。主题分享后，来自工业信息化领域、煤矿安全领域、设备运维领域和工业实时数据库领域及学术研究等领域的参会朋友就工业数据存储架构、工业领域数据采集、数据建模分析过程中行业专家与数据专家的配合等方面问题进行深入交流和讨论。以下是分享的主要内容：", "数据分析应用是真正能够直接解决企业问题的，是显露在外可以直接观察的部分，但是在这之下有很大部分支撑数据分析应用的就是数据管理技术。数据分析应用需要一整套的处理和加工过程，数据就是原材料，需要把数据有序地存储和管理起来。然后是数据的整理、清洗、集成，这个过程主要由数据工程师（DataEngineer）来完成，最后由数据科学家借助数据分体的工具和平台根据业务问题等实际需要采用不同的算法和方法等进行数据分析。大数据的概念是由麦肯锡提出来的，后来有3V、4V、5V的解释。就像盲人摸象一样，大数据一直没有准确的定义。到底什么是大数据呢？ 本质上大数据就是数据驱动的分析，大数据管理其实就是支撑数据应用的平台的管理技术。", "最早数据存放在文件里，没有中间件进行数据管理，数据的存储和读取完全由应用软件完成。由此也造成用户无法专注于应用逻辑，而且还要清楚数据管理逻辑。在此过程中发现有越来越多共性的需求，从而提炼出数据管理技术，作为独立的中间件形态。从文件到数据库的技术过程最早是百花齐放的，类似网络数据库、层次数据库，到70年代突然只剩下关系型数据库了，其背后的逻辑就是应用驱动，因为在70年代只有银行愿意为数据库的技术买单，现在数据库技术的许多概念都是源于金融体系。数据库有一个很重要概念transaction，即事务，需要保证数据的ACID，任何一个操作要保证其原子性、一致性、持久性、隔离性。同样是在应用驱动下，后面又出现了很多种类型的数据库，因为只有一种关系型数据库无法满足所有需求。如有些数据分析的时候对数据库写入操作很少，但是有大量复杂的读和查询的操作，数据仓库技术就出现了。2010年左右大数据技术的出现，因应了新的数据类型或者是数据的使用场景，各类NoSQL和NewSQL的数据库开始逐步涌现。", "Wikipedia上关于大数据是用排除法做的定义，如果数据大、复杂，各方面处理的实时性高，传统的数据管理技术和传统的数据处理软件处理不了或者处理不好，需要一些新的技术来解决，这些新技术就把它统称为大数据技术。从科学研究的角度来说这是非常糟糕的定义，这个定义里面充斥了各种形容词，但这已经是我们能够找到最好的定义了。以前做事务管理的是关系型数据库，数据通过ETL的工具导入数据仓库，在数据仓库上建维表，建数据集市，在上面跑各种统计分析，是后面做各类报表，可能还要讨论很多算法模型。如今我们突然发现这个传统的数据流程走不下去了，需要找到新技术来实现，这就是通常意义上所谓的大数据管理技术。", "大数据是应用驱动的，传统的技术满足不了应用的需求，就需要找到新的数据管理技术来解决，这些新技术统称为大数据管理技术。那么大数据管理技术又是怎么来的呢？", "\nGoogle是最早尝试使用新技术来满足业务需求的公司，Google搜索先要把全世界的网页扒回来存下来，然后把关键词抽取出来，对这些网页做排名，在线搜索的时候要根据用户输入的关键词找到相应的网页，这里怎么把网页定位出来，这些用传统技术都不能很好的解决。Google抓取回来的海量数据用传统的数据管理技术已经无法存储，所以Google第一个搞出来的是GFS(Google File System)即Google的分布式文件系统，可以把小型机的内置盘连在一起，就像集中存储一样。Google的GFS存储的不再是单个文件，所有的数据是大批量写进去、大批量读出来，没有很多随机的访问，也可以做块的读取，采用分布式小型机存储，通过MapReduce的分布式技术框架来支撑大规模机器的计算。搜索的时候根据事先建立的索引，从存储结果里面去找关键字key，对应的value就是要找的网页。", "所以在数据库方面Google先研发出了BigTable，BigTable现在开源版本叫HBase，BigTable与关系型数据库的区别主要表现在两方面：第一，它支持key/value的查询，通过一个键可以找到相应的值，而不需要复杂的sql查询；第二，每一个键对应的值有很多，一个关键词会有多少个网页，每个网页需要存储的内容大小和网页的复杂度都不一样，关系型数据库表结构根本没法设计，所以在BigTable里引入了列组，每一行数据的列数都可以变。Google所有的这些创新，也都是根据自身业务的需求、数据的特点，找到适合的解决方法。Google本身的技术堆栈也在演进，在14年已经放弃使用MapReduce了，数据存储管理的工具BigTable现在也不用了，Google已经又有新的技术创新。", "从2016年世界的大数据图景中，我们不难发现当今世界上大数据领域是如此的蓬勃，在里面可以看到好多公司的名字，几乎每个公司背后都会站着不止一家投资机构。到底该选什么样的产品或公司？今天的大数据世界到底是什么样子？从哲学的观念来看，世界就是合合分分、分分合合，就像之前有网状数据库和层次数据库等，逐渐演变成关系型数据库一统天下，后来又关系型数据库分裂成了多种数据不同的管理机制。", "这方面Apache的开源项目大概归成四类：第一类是数据管理，主要是数据库相关的技术；第二是数据分析框架，从下往上依次包括资源管理、计算框架、分析算法库和任务工作流等组成部分；第三是数据工程的工具，包括Flume采集工具、Sqoop导入工具、Kafka传输工具等；第四是系统管理工具，有分布式协调、系统监控。", "Apache开源项目上的发展现状背后是什么驱力的呢？第一是应用驱动，所有产品和服务的发展其实都是应用驱动的。如果今天有公司说研发出通用的数据库，这与现实趋势是背道而驰的。第二是生态化，每个组件的应用过程都有特定的场景。比如MapReduce框架背后的假设实际上是每个算法可以实现对数据的线性切分，线性切分之后在每个部分进行同样的分析，然后再把各部分的结果线性组合。Google之所以搞出这套框架，是因为文本数据的处理上可以对数据进行切片，切片的每块数据可以相对独立的计算。但很多情况下通常并不可以这样来操作，比如社交网络的图数据，如果把图数据分割在十个节点上，原本相互联通的两个节点很有可能就被切断了，因此需要类似GraphX的框架专门支持图运算。所以，最终会发现所有的技术都只能解决一些特定领域的问题，或者是在某一个垂直方向上它具有一定的泛化能力，一个完整的大数据架构需要不同的组件拼成一个完整的生态。第三是开源化。现在很多的产品都是开源的，开源背后的商业模式是什么？将来真正能形成一个产品吗？科学研究需要一个转化过程，首先从科学变化成技术，然后是技术的工程化后变成产品，变成产品后可以带来商业回报，才能让整个项目持续下去。今天这些开源产品背后大部分可能就是一家公司，但是目前开源项目的盈利模式仍然不是很清晰。所以开源社区作为一种趋势往下会如何发展，跟商业化怎么结合，依然是需要不断探索的问题。具体到工业大数据也是一样，工业大数据怎么做的，仍然没有多少人知道。即便是predix大概每三个月改版一次，做出来很多东西改版时丢了，可能下次又把这个东西捡回来了。", "大数据管理的技术发展的背景是从“互联网”到了“互联网+”，即从消费互联网向产业互联网发展。以前互联网应用的领域主要在搜索、电子商务、社交等几类当中，现在已经渗透到一个比一个复杂的领域：企业管理、政府治理、公共服务、现代农业、智能制造、自主可控。我们国家自主可控强调得较多，但是在工业领域的数据利用水平依然很落后。其中主要面临两大方面的变化。第一是人才的变化，以前用大数据是互联网公司的复合型极客，这些人有很强的数学功底、编程能力、数据管理技术、分布式计算技术，同时掌握领域的业务知识，是具备四大方面的全面型的人才。在产业互联网领域里的人更多的是熟悉领域业务知识，而计算机能力真的很有限。第二是数据种类的变化，以前互联网领域是大量的文本数据、社交数据、多媒体数据等，而产业互联网领域是大量的传感器产生的实时数据、企业内部的业务过程数据，大量的非结构化工程数据、仿真数据、设计的CAD数据，这些数据跟传统互联网的数据都不太一样。目前开源产品或项目基本是针对传统互联网数据的，在产业互联网领域并不能很好的适用，这就是今天大数据管理技术的整个发展趋势。", "五、工业大数据管理技术的驱动力是什么？", "工业大数据管理技术的驱动力概括起来就是“加减乘除”，加法就是要提质增效，也尝试拓展更多的业务；减法就是要降低成本、次品、和消耗；乘法就是要做一个平台把所有供应商的数据都整合到一起，使得所有的业务和上下游可以更加有效协同；最后，除法就是希望能够通过平台使得供应链各环节可以精确的分工，实现轻资产的运营。", "前面所提的加减乘除都是工业的业务变革，从业务角度上实现加减乘除，必然会涉及到信息化的部分。信息化与大数据是有区别的，大数据永远替代不了信息化，信息化系统与大数据系统是并存的。传统信息化做的是数据、流程、业务的电子化，数据只是其中一环。具体到数据包括两方面，一是OLTP的数据，即支撑业务系统的数据处理；二是OLAP的数据，即数据分析，除去流计算，大数据绝大多数的应用场景是在数据分析部分。所以信息化和大数据本身是两个层面的事，数据收上来后采用新的方式帮助业务目标的实现，通过这样的过程提高效率。但是更好的维度是通过数据的分析让工业生产和业务变的更智能，发现和找到别人不知道的知识。", "数据分析基本上可以分成两个层次，第一是初级分析，第二是高级分析。初级分析是传统企业的BI。高级分析包括两个层次的分析，第一个叫预测性分析(Predictive Analytics)，通过数据分析预测未来会变成什么样。第二叫“What-if Analytics”，如果预测的结果不好能否通过控制调节结果。", "工业大数据分析有别于商业大数据分析。商业大数据主要是对用户，分析用户需要的业务，比如说阿里分析可以给用户做推荐，分析用户购买某商品后可能买什么，并推荐给用户相应的产品。而工业大数据背后支撑的是物理机理模型，通过数据分析解决工业相关的问题也需要满足一定的机理。商业大数据通过相关性分析就非常有帮助，而工业大数据只有通过因果性分析才有用。如果一台机器出现故障了，故障的原因一定是能找到的，如果找不出来，可能会是两方面的原因，一是我们今天对物理世界的探知能力还不够，也许有新型的传感器就能解决，因为目前已知的变量里面还没有能够表示相应的原因。二是今天我们的认知能力不足，也许变量已经有了，但是变量间非常复杂的合成过程仍未找到。工业大数据分析就是尝试把后面的原因、规律找出来。", "在我们现有的实践中，工业大数据的应用基本上可以归结为三大类：一是与设备维护相关，二是与运营优化相关，三是与2C的消费品营销与设计反馈相关。这三个方面也只仅仅是工业大数据应用的冰山一角。", "传统工业可以分成信息化和自动化两块，信息化收集了大量数据，自动化也有很大进展，但是自动化域的信息化做的不够。工业实时数据库里存放的数据是不是真的已利用起来以及怎么用？实时数据库和关系数据库在写入的能力上相关两个数量级。海量的工业数据可以定入实时数据库，但要存放到关系数据库必需降频。但很多时候机器设备的数据大概要到ms的精度才能分析，进行故障诊断，像鼓风机的频率是4k～8kHz。传统的数据库管理技术并没有很好的解决这类高频数据的存储和管理，所以传统信息化领域并没有很好把自动化域的数据管理好。", "\n工业大数据主要来源于机器设备数据、工业信息化数据和产业链跨界数据。今天做工业大数据分析，不仅要看自己数据还要看别人的数据，比如优化供应链的时候还需要市场销售的数据、供应商的数据等。风电优化分析除了利用风机的数据，也需要结合气象的数据。很多外部数据原来工业界从来没有尝试过管理些这数据，这是大数据分析的时候传统工业上管理数据的机制遇到的一些挑战。", "第一，多模态就是非结构化数据。区别于传统互联网领域非结构化的语音、文本、图片、视频等，工业领域非结构化数据更加复杂，如仿真数据、CAD的文件等。模态是指同一家公司的不同仿真软件和CAD软件，不同版本之间的时间间隔不一样且不兼容，不同学科使用的设计软件不一样，如在航空航天里面有上百种的软件，数据的格式都不一样。", "第二，数据通量大。工业领域大量的传感器是实时数据，高频采集使得工业大数据分析不能在以前秒级、分钟级层面进行分析做图表可视化展示。工业大数据需要做到毫秒级甚至毫秒级以下的数据采集和分析。", "第三，多学科协作。例如在卫星上要加个部件或减个部件关键是看重量，如果重量绝对不能减，总设计师需要多学科协作，学科之间数据集成与融合，找到相应的规律，确定需要加的部分和减的部分。其中工业领域的数据集成，有别于关系型数据库里基于数据表结构的集成，主要是指语义集成。例如一个时间序列的片段代表了某种故障，而该故障曾经在维修报告里出现过，这就需要能把语义提炼并关联起来", "工业大数据的工作步骤包括三个层面：第一个层面是数据的存储管理，即建立数据的采集体系、数据持续采集与清洗、工业数据存储；第二个层面是数据治理，包括机器数据建模与元数据管理、数据质量分析和数据关联与语义集成；第三个层面是数据分析应用，通过可视化进行数据探索、数据分析和结果反馈。", "拿数据质量举个例子，数据质量主要看数据是否有假、是否会错及是否有漏的。工业数据来自于设备，工业数据同样可能会造假，设备会带来很多真假数据的问题。通过对三一机械设备位油分析的用户的使用和加油行为，发现里面至少可分为三类：第一类对设备的保养特别好，每次用一半就加油（左上角图）；第二类反映车主和司机之间极其不信任，每用一次就加一次1、200块油（右上角图）；第三类是要等到油用光了之后才加的土豪（左下角图）。", "这张图是油位传感器传回来的实际数据。如果传感器有问题，数据就收不准，实际数据也根本没法分析，因为最早测油位的传感器用的是浮子，不平的路面和油的粘滞力，都上下浮动的油位传感器测不准。", "自动化部门与信息化部门之间脱节，自动化部门更换了传感器，信息化部门并不知道，更换传感器后采集的数据是真实的（黄点），但数据分布已经变了，如果仍用同一个模型进行分析显然已经无效了，所以至少针对新的传感器采集到的数据重新建立模型。", "此外，还有传感器的错误导致数据的问题，如车开着油温传感器数据为零，这种情况肯定导致了数据错误。", "所以现实中各种各样的问题导致今天的数据质量存在大量问题。如何来解决这个问题，大家也都在研究中，陆续将有一系列的方法来解决这个问题。", "多模态数据语义融合的目标是构建具有制造语义的知识图谱，以工业领域的BOM为核心，基于知识图谱建立结构化与非结构化实体和语义标注，构建具有制造语义的知识图谱，因为在BOM里很多实体关系比较清晰。通过知识图谱的构建，跨领域本体可以从语义标注中找出相互间的关联性，时间序列片段带有的故障码就可能通过知识图谱识别和发现。", "工业大数据系统的核心在于“效能+易用性”，首先要保证系统的速度足够快、系统足够强，即要满足高吞吐量，事务处理模型全局的一致性。第二要保证支持数据的变化、人员的变化，保证系统好用，保证系统在工业领域都能用起来，即系统能够支撑起专业化查询和分析引擎，易于管理和维护，可以水平扩展，并且具有容错和故障恢复的能力。", "工业大数据管理系统的建设包括战略层面和战术层面。战略层面上首先业务目标必须明确，其次是系统架构要通盘考虑，第三是业务需要充分参与，仅有信息化人员的参与没有业务人员是不可能建设出符合业务需求的系统。战术层面有三方面，第一是数据要尽早收集，即使没有很好的机制，用文件的方式也要先把数据存下来，没有数据或者只有三个月、一年等少量的数据，根本无法满足分析的需求。第二是应用小步快跑，应用需求明确了就可以根据已有资源先实现，很多应用是不依赖于大数据平台本身的，用关系型数据库甚至是Excel就可以直接实现。第三是人才系统培养，将企业内部的人朝复合型人才去培养，外部人才和服务可能都因成本过高企业难以承受，而且人才培养是长期的，可能至少培养一年到两年才能达到较好的水平，所以业务要尽快和充分的参与进来。", "工业大数据平台的实施路径一定要在业务和数据的双轮驱动下进行。业务层面需要牢牢把握信整体业务目标、具体业务提升和转型方向、业务流程改造目标以及业务流的映射。数据层面需要围绕数据同步、交换、关联和集成、数据质量、数据的存储、管理和使用、以及数据的特点和规模与来源等方面。限于时间今天具体的技术和应用都无法展开，希望以后有机会再交流。", "清华大数据思享会是面向清华大数据产业联合会成员的思想交流的平台，定位为小范围的深度交流，目标是希望通过思想交流与碰撞促进产业的数据创新，以及大数据与产业和资本的融合，为各参会来宾带来数据创新方面的新灵感和启发。", "校对：丁楠雅", "VAI:THU数据派", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74683"]},
{"module": ["干货教程"], "note": ["\n文 | 纯洁的微笑\n回想起从公司成立敲出的第一行代码算起到现在也快三年了，平台的技术架构，技术体系也算是经历了四次比较重大的升级转化（目前第四代架构体系正在进行中），临近年底也想抽..."], "title": ["从零到百亿互联网金融架构发展史"], "content": ["文 | 纯洁的微笑", "回想起从公司成立敲出的第一行代码算起到现在也快三年了，平台的技术架构，技术体系也算是经历了四次比较重大的升级转化（目前第四代架构体系正在进行中），临近年底也想抽出时间来回顾一下，一个小公司从最开始的零交易到现在交易量超过百亿背后的技术变迁。", "在互联网金融行业一百多亿其实也算不上大平台，也就是二级阵营吧，其实每次的架构升级都是随着业务重大推进而伴随的，在前一代系统架构上遇到的问题，业务开发过程中积累一些优秀的开发案例，在下一代系统开发中就会大力推进架构升级。一方面可以平滑过度，一方面公司资源可以大力支持，同时技术的小伙伴们可以使用到前沿的技术，更有开发的成就感，就这样我们大概也就是9个月就行系统架构一次升级，就到了我们现在的这套架构中。", "很多网友经常会问，你们平台的TPS是多少呀，最大并发是多少呀，性能怎么样，说实话我们是一个小公司，最夸张也就上万人同时抢标，但是做为一个中型的互联网金融平台要做的事情也真的不少，远远不只是这些参数可以说的清楚。", "我们也不是什么高大上的平台，使用的技术也是目前比较主流开源产品，但在公司不断发展的过程中也遇到了很多的问题，也尽量去使用比较主流的、开源的、适合我们的一些解决方案来构建整个系统，在这里分享平台发展背后技术换代的变化，同时希望和大家多做一些交流，多提一些建议。", "我们进行了四次大的架构变化，每代架构都用一句话来总结：", "第一代架构特点：业务比较集中、功能满足投资理财需求、快速上线", "\n第二代架构特点；分布式系统改造，平台化初具规模，各项垂直业务系统搭建上线、产品端极大丰富用户投资、大数据平台研究并使用", "\n第三代架构特点；SOA治理，使用zookeeper作为注册中心，dubbo做监控和调度中心；cas实现单点登录，使用shiro做权限控制", "\n第四代架构特点；全面启用微服务开发模式，springboot+springcloud技术桟做为第四代架构技术支撑", "2014年应该算是互联网金融元年，在之前其实已经有很多互联网公司用着各种模式在生存，一直不温不火，但是到2014年突然火爆了起来，首先是网贷之家，网贷天眼这种第三方网站流量突然增加，接着是媒体报道不断跟进，再后来就报出各种互联网金融公司获得XXX美元投资的报道越来越多，政策也慢慢明朗，于是很多大型公司（集团）也就趁着这股热潮跟进，其中就包括我们。", "第一代系统最主要就是抢时间，公司希望用最短的时间内保证系统上线，那时候移动浪潮已经启动，于是决定优先上线移动端，网站可以暂不考虑。公司当时有PHP和Java两种开发语言技术储备，因为PHP在快速开发上面有着非常大的优势，因此决定采用前端PHP+后端Java这种模式。", "系统分成了三层：用户层：安卓和IOS移动端；接口层：php提供用户和交易接口；后端：后端有两部分，后台和定时系统。后台用PHP开发和接口层公用了一个系统，另一个是定时系统，负责计息、派息、到期等定时任务等使用了java开发。", "基础服务和中间件，mysql做了最基本的主从来支持，第一代系统只是使用了mysql的主库，从库只是同步备份；memcached用来处理用户抢标的并发问题，也只用了这一块；ActiveMQ用来使用二级市场的转让撮合以及其它一些异步消息通知。项目部署：php使用apache部署，定时服务使用tomcat6来做应用服务器，使用lvs来做前端apache的负载，基本上第一代也就这些技术了，下面是第一代系统的架构图。", "第一代系统上线之后，网站和H5(手机浏览器或者微信端)系统建设就变的特别突出，作为一个互联网金融公司没有官网不能忍，于是又开始马不停蹄的开始开发网站和H5系统，在这个期间PHP之前做的后台这块摘了出来，用java从新规划了一版，至此PHP就负责了网站、APP接口、H5这三个系统，三个系统共用的一个核心交易，java这边负责后台管理和定时服务，我们一般给这个架构叫做1.1代架构。", "第1.1代系统架构图，绿色部分为变动部分", "第一代系统的缺点是业务过于集中，仓促上线，后期问题较多。", "第二代系统的背景是随着公司业务量的快速发展，很多初期所欠的技术债务统统爆发，线上出现了很多问题，最严重的一次是给个别用户重复派息，各种被骂，现在记忆犹新。另一方各业务部门需求不断，公司产品需求不断，所以这个阶段就是忙着修复各种生产问题，一边还需要开发垂直业务系统。那段时间差点被逼疯了，第一代系统是封闭开发，回来还没缓过劲，这边又赶马上架，真是疼并快乐着。", "第一个垂直子系统上线的是：合同系统，当时用户投标后没有一个合同，很多用户很不放心，就把优先级提到了前面。后来就单合同系统就改了三个版本，第一个版本只是生成pdf,第二阶段上线电子签章，第三个阶段加水印，自定义动态生成pdf;紧接着开发：", "积分系统：用户邀请，投资等生产积分，用来兑换抵现卷等；", "抽离出消息系统：站内消息、短信、邮件等；", "上线监控系统、业务监控和服务监控，业务失败预警；各业务部门继续不断提需求，", "上线财务系统：财务人员统计核算金额；", "风控系统：监控异常用户，异常交易；", "给销售开发了销售系统；因为和很多第三方系统对接，又开发了对外接入系统。", "一代系统做的很赶，产品界面又很烂，随即启动规划了网站2.0、APP2.0、H52.0，针对前端系统的需求，在后端开发了CMS系统来发布项目、公司的公告新闻等。", "第二代产品端普遍规划了很多大数据分析的一些需求，会在官网展示全量数据分析后投资偏好、投资的金额都跑到哪里去，前端用地图来展示，对于个人也会有还款日历，代收数据分析等，因为需要跑全量数据，在规划的时候都是设计离线来处理，将数据从mysql从库同步到mongodb的集群中，利用mongdo的mapreduce技术来处理大量的数据，于是我们的数据库层就变成下面的这个架构", "mysql实时同步到mongodb，我们使用的是tungsten-relicator这个工具，会在mysql服务器端启动一个监控agent，实时监控mysql的binlog日志，同时在mongodb的服务器端也起了一个服务端，agent监控到数据变化后传送给服务端，服务端解析后插入到mongodb集群中以达到实时同步的效果，如上图。", "当初写了一篇文章来介绍：大数据实践-数据同步篇tungsten-relicator（mysql->mongo），其实这个工具在使用中，也不是特别的稳定，但是当初的选择方案并不多，幸好后期慢慢的熟悉后算是稳定了下来。", "数据清洗系统我们大胆的使用了golang来开发，当时使用的golang版本是1.3吧，现在都1.8了，以前也是没有接触过也是锻炼了队伍，好在golang语言本身非常简洁和高效，虽然踩了N多坑，但是最终我们还是按时投产了；后来又使用了golang开发了一个后台，是在beego框架的基础上来做的。", "大数据分析系统后来又升级了一代，在前端的各业务系统，UI用户层做了很多埋点来收集用户数据，通过activeMQ传输接收最后存储到mongodb，在进行数据清洗，将清洗后的结果存入到结果库中，供前端业务系统使用；后来利用beego+echart重新做了一版数据分析系统。", "大数据系统的架构图", "因为后端数据库的压力不断增大，后端管理系统、业务系统均作了主从分离；后台管理系统增加缓存，启动了redis做缓存；使用nginx搭建了独立的图片服务器；第二代系统开发过程中，也是公司发展最快的阶段，上线了N多的活动。", "第二代系统架构图：", "稍等总结一下：", "\n第二代架构上线了各业务系统，做了主从分离，搭建了大数据平台为以后更多的数据处理提供了技术基础；", "\n缺点：各业务系统切分之后，各项目之间调用复杂；后台系统繁多、各系统之间有单独的账户系统，运营需要来回切换完成平台运营监控。", "第二代系统开发完成之后，留给我们了三个问题很痛苦：", "第一个是随着业务系统不断增多，系统之间的调用关系成指数级别上涨，在第三代系统初期，我们又开发了很多基础组件，更是加剧了这个问题；", "第二个问题和第一个问题相辅相成，系统之间调用关系太多，如果移动其中一个子系统，可能需要修改关联系统的配置文件，重新启动服务，经常因为更新一个系统，其它系统也需要被动更新，投产和出问题切换很复杂；", "第三个问题是我们开发了很多的后台系统，但是账户没有统一，每个子系统有各自的账户中心，运营和业务人员需要来回登录才能完成日常工作，随着业务量增大这个问题也日益突出。", "于是又开启调研、系统选型等，解决第一个问题就是引入SOA服务治理，通过服务的注册和发现解决系统之间的解耦，当时考察了很多，最后选型dubbo，原因无它，有大量群众使用基础该趟的水的趟过了；", "解决第二个问题就是引入配置中心，当时调研了360的Qihoo360/QConf、Spring的spring-cloud-config、淘宝 的diamond、还有百度的disconf，最后纠结半天选定了disconf，完美和spring cloud擦肩而过，但是正是从这里开始让我们注意到了spring-cloud、Spring-boot为第四代的架构选型做了基础，其实最后disconf也只是在少部分项目使用，也没完全推广开；", "解决第三个问题就是账户中心，使用了cas实现单点登录，shiro做权限控制，dubbo来提供登录后权限列表等服务端接口。", "改造后的架构图", "在这个基础上面，我们又抽离出来很多基础组件，comomn组件处理共用的基础类，包含字符类、日期类、加密类….，搭建了fastDFS集群来处理文件系统，做了redis集群的测试；单独开发了定时调度系统，将所有的定时任务统一集成到调度系统，那个系统需要定时任务都可以在页面自动添加调度策略；前端PHP做了系统改造，主从分离、静态优化等", "在后来，公司又启动众筹平台的建设，这次系统完全采用java语言开发，app端采用混合开发模式，其中APP的所有一级页面全部采用原生开发，所有的二级页面都是H5+vue这种模式，后端全部采用dubbo做服务化，最终的架构如下：", "图里面系统只罗列一部分，使用其它服务来代替", "第三代系统启动了SOA服务治理，引入统一账户中心、基础组件；缺点是开发环境较复杂。", "人总是不满足，技术呢也总是希望可以使用最好架构体系，在三代系统架构的开发中，了解到了spring cloud和spring boot，在不断的学习之后，越发的感觉到springboot的便利性，快速开发的优点甚是喜爱，spring cloud体系也完全满足一个大型系统需要考虑的方方面面，微服务的概念不断的被提出来。", "以上为技术背景；另一方面国家开始严格要求P2P公司必须与银行存管，分析了银行的相关接口后发现如果严格按照规则走，我们的系统需要大改造，同时公司为了满足监管要求，又开发出白条相关产品也是一个大项目，趁着以上的两个背景，我们决定在进行银行存管和白条项目的同时全面拥抱微服务。", "至于为什么我们要抛弃dubbo转而全面拥抱spring cloud原因有三：", "1、dubbo多年都没有更新了，spring cloud不断的在更新升级；", "2、dubbo主要做服务治理和监控，spring cloud几乎考虑了微服务所需要方方面面，比如统一配置中心、路由中心；", "3、spring cloud更是无侵并且完美和spring其它项目整合，开发效率更高。", "既然选定了使用spring boot+spring cloud来改造，微服务技术选型这边就定了下来，那么如何开启改造呢，毕竟在进行新一代系统改造的同时也不能影响原有业务，其中最主要的问题就是最初的系统虽然都是按照分布式的开发模式来进行，由于老系统的原因有的系统还是共用了一个数据库，微服务要求每个独立的子系统有自己独立的库操作，别的系统如果需要修改或者查询子系统的数据，需要根据服务间接口调用来获取。", "因此计划先从新开发的项目和需要改造的项目中启用springcloud项目，别的系统暂时先通过路由器模式来通讯，最终的系统架构图如下：", "在架构的这条路上面没有终点，变化就是永远的不变，架构的升级更是为了更好的支撑业务，二者相辅相成。", "在这几年中我们也想对开源世界做一点点贡献，总共开源了两个软件：", "在项目中大量使用了mybaits，我们对mybaits的generator做了改造，并且做了一个系统界面，方便根据相关参数自动生产相关代码（只需要设计好表结构，系统会自动生成mappper、Entity、dao层的代码），最后也开源了出来generator-web", "界面如下：", "为了锻炼技术学习springboot我们开发了一个云收藏的开源软件，使用的技术主要是springboot/Spring data jpa/redis/thymeleaf/gradle，功能主要是可以帮助用户在云端收藏、分享和整理自己的收藏夹。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74808"]},
{"module": ["干货教程"], "note": ["文：胡晨川\n“数据化运营”，提这个概念的人越来越多。以我个人的理解，“数据化运营”指的就是从对业务数据的认知中，抽象出运营策略，以数据追踪运营策略的执行过程，最终由数据来评价运营策略..."], "title": ["项目数据化运营指标的应用和运营活动的量化"], "content": ["文：胡晨川", "“数据化运营”，提这个概念的人越来越多。以我个人的理解，“数据化运营”指的就是", "我们在运营当中所指的“数据”，是狭义的数据，其实就是各种各样的指标。", "我觉得将“指标”这个词对应于英文的indicator，是比较准确的。它是一种指示器，是将某件事情量化而成的一串数字。", "其实大家在工作中，都会有意无意地应用指标。能用就好，又何必去管它定义是什么呢？重要的是怎么用好它。", "就运营而言，我觉得使用指标时，一定先区", "所以，过程型指标与结果型指标并不是割裂开的，关键在于你的运营目的。", "一个指标的好坏，我们需要从", "和", "两个方面去衡量。", "所谓信度，指某指标衡量某事物的", "。所谓效度，", "经过上面两个例子，你应该能明白，一个好指标应该怎么去构建了。除了信度和效度，好指标的特征还需要：计算简单快速、易理解", "，可持续性（口径稳定，长期可用）等特征。", "在许多业务场景中，我们需要盯住的指标不止一个。以我们目前的业务场景为例，衡量一个城市运营团队的综合实力，我们需要从运力规模、接单能力、客户评价、团队稳定性、配送成本等多角度出发，若每个角度都引申出1-2个指标，你将陷入混乱。", "因此，多于多维度综合衡量，我们推荐使用综合指数。最常用的就是加权综合指数：", "需要注意的是，综合指数计算时，里面各个指标都先需要标准化，转化为统一单位的数字后，才可以进行加总。而p1-p5即权重，哪方面更重要，就给予更高的权重，", "针对目前我们的业务，我觉得大家需要关注的核心指标是：", "这些也是结果型指标。仅此而已，真的没有那么复杂。从这三个核心指标，结合两条维度的组合：", "足已帮助我们发现问题并定位问题。目前我们尤其关注午高峰的核心指标。数据积累也就能帮你到这里了，不要期待数据库告诉你一切，不可能的。", "发现并定位问题后怎么办？调查、走访、问卷，各种方式你都可以用，找到问题的根源，然后，制定运营动作去解决问题。", "当然，数据支持组会不断地产出研究成果，让大家更容易更准确地去制定运营策略并更好的执行。", "在之前的篇幅中，我们将指标的相关知识进行了梳理，有了这个基础，我们可以开始讨论运营活动的量化了。", "以我的理解，一个优秀的运营活动至少要有：", "1.明确的活动目标和预算", "2.清晰的响应关系和执行流程", "3.可靠的过程型指标", "4.准确地结果型指标", "5.科学合理的效果评价方式", "6.知识的沉淀", "7.一个聪明的负责人", "以我们目前的O2O配送业务的运营为例，如果我们做的是一个短期的活动，我们所要追踪的过程型指标，至少需要：", "对于一个长期的活动或策略，我建议只追踪结果型指标的变化，而结果型指标就是我们要关注的核心指标。", "这或许是本篇文章中最核心的部分了，以我的角度，运营活动的评价，只是回答两个个问题：", "活动目标达成且预算控制住，那就是个好的运营活动！", "活动目标一定是围绕核心结果指标来制定的。那么判断活动的效果如何，就是衡量核心结果指标是否有质变。如何展开评价？", "首先，对核心业务指标做1-2层的拆解。为什么这么做呢？因为核心业务指标往往受多重因素的影响，我们尽量拆解到我们的运营动作能够直接影响，同时与核心指标的关系又比较近的一层。", "其次，要从时间维度对结果型指标进行对比，也就是活动前后的对比。需要注意，活动前后的数据对比，一定要跨满一个业务周期,比如，我们的业务有明显的7天的周期波动，那么运营活动一定要跨满七天。同时采用均值的方式，对比平均水平的变化。这点大家尤其要重视！", "那么，活动前后对比数字差别达到多大，算是有区别而不是随机波动呢？粗略的估计方式是5%，即活动前后的数据差别达到5%以上，才能说活动有显著的效果。精细的方式需要拉长时间后，对活动前后两段数据做方差分析，这里不累述了。", "另外，对于结果型指标，其实并没有必要在活动期间天天追踪。真正重要的是，活动的跨度必须超过一个业务周期，在超过一个业务周期后，对结果型指标进行汇总，与活动前对比。", "最后补充一点，如果条件允许，建议在活动施行的时候，找一个参照系，横向对比活动的效果。这样能排除掉系统性的影响。比如，全国各地的接单率都在涨，在某地上了一个活动，发现接单率涨了，说是活动带来的效果，就很牵强了。我们已经做了城市的分群，在同群组里面的城市，可以寻找规模相似的，最为参照系。", "任何运营，都不能脱离对预算的考虑，这也是我们目前最薄弱的环节。活动前，估计好预算总额，在活动执行的过程中，时时关注预算的消耗情况（每天统计），这是非常有必要的。", "关注预算总额是否超支以外，我们需要计算ROI，即投入产出比。不同的活动，投入产出比的计算方式一定不同。", "例子6：比如我们唤起运力的活动，花了多少钱cost，唤起多少运力rider，这部分运力送掉多少单order，我们就需要计算cost/rider+平均每运力一天的补贴，cost/order+平均每单补贴。", "除了计算ROI，我们还需要关注在执行活动时，整个城市的成本情况，是否符合我们的宏观的预算限制。最好的情况是，活动执行的同时，整体成本并没有上升，或者在下降。即我们要把钱挪到该用的地方。", "一个短期活动做完后，我们还有必要追踪活动结束后的结果型指标如何变化。最理想的情况是，一个活动结束后，结果型指标依然保持在活动时的优秀水准，也就意味着，一个短期活动带来了长期的正向效益。", "\n我们不能忽视有些", "不同时期进行的相似活动甚至是相同活动，我们要", "，来观察活动间的对比。许多", "比如价格探底测试，我们应该分阶段，分地域类型，进行系列测试，从各次的数据存档中，去总结规律。", "随着运营的逐步精细化，我们", "！", "小结", "写了那么多字，实非我愿。但无奈要表达的东西太多，简单总结为以下几点：", "文中提到的信息，希望能帮助大家在未来的运营工作中，思路更清晰，执行更高效，效果更显著！~", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74772"]},
{"module": ["干货教程"], "note": ["\n文 | 傅志华\n刚刚过去的2016年，是大数据从概念到务实落地的一年。在过去的一年内，互联网行业、电信行业、金融行业、房地产行业、汽车行业、娱乐行业、教育行业、零售行业、能源行业、医药..."], "title": ["企业实施大数据的五大关键（更新版）"], "content": ["文 | 傅志华", "刚刚过去的2016年，是大数据从概念到务实落地的一年。在过去的一年内，互联网行业、电信行业、金融行业、房地产行业、汽车行业、娱乐行业、教育行业、零售行业、能源行业、医药行业、政府机关等都在不同程度的接触和实施大数据。很多正在实施大数据的企业或机构并不成功，为什么?他们实施大数据都存在共同的问题，最为典型和严重的是三个问题是：", "很多企业业务部门不了解大数据，也不了解大数据的应用场景和价值，因此难以提出大数据的准确需求。", "由于业务部门需求不清晰，大数据部门又是非盈利部门，企业决策层担心投入比较多的成本，导致了很多企业在搭建大数据部门时犹豫不决，或者很多企业都处于观望尝试的态度，从根本上影响了企业在大数据方向的发展，也阻碍了企业积累和挖掘自身的数据资产，甚至由于数据没有应用场景，删除很多有价值历史数据，导致企业数据资产流失。", "因此，这方面需要大数据从业者和专家一起，推动和分享大数据应用场景，让更多的业务人员了解大数据的价值。", "企业启动大数据最重要的挑战是数据的碎片化。在很多企业中尤其是大型的企业，数据常常散落在不同部门，而且这些数据存在不同的数据仓库中，不同部门的数据技术也有可能不一样，这导致企业内部自己的数据都没法打通。如果不打通这些数据，大数据的价值则非常难挖掘。", "大数据需要不同数据的关联和整合才能更好的发挥理解客户和理解业务的优势。如何将不同部门的数据打通，并且实现技术和工具共享，才能更好的发挥企业大数据的价值。", "很多企业或机构在实施大数据的时候，只是简单的建立大数据技术部门，仅从技术、算法角度考虑。企业往往不能科学的考虑大数据团队内部应该招聘和培养哪些方面的人才。同时，更不会考虑不同大数据团队和业务部门如何更好的协同作战，导致大数据不能充分有效的在业务场景的中落地。", "那么，在大数据时代，企业如何更有效的实施大数据战略，如何规划、如何实施、如何保障大数据的相关工作可以顺利开展。我们认为，企业要实时大数据战略，需要从五大关键方面规划：", "1.制定大数据规划找准切入点;", "2.强化大数据领导力设立CDO;", "3.设计合理的大数据组织结构;", "4.搭建富有执行力的大数据团队;", "5.用制度和文化保障大数据实施。", "本文将从这五方面展开。", "成功的大数据规划聚焦于四个核心要素：应用场景、数据产品、分析模型和数据资产，企业着手实施大数据战略要着重考虑这四方面，管理者需要在这四方面做好规划，才能让大数据发挥价值，从而给企业带来更好的业务价值。", "，企业需要确定不同业务投入大数据的优先级，确定大数据的切入点。企业需要优先考虑业务的哪些方面投入大数据可以为企业提升绩效。在企业中，大数据应用场景包括业务运营监控、用户洞察与用户体验优化、精细化运营和营销、业务市场传播、经营分析等常见的方面。", "当然在人力资源、IT运维以及财务等方向也可以引入大数据。企业不可能所有应用场景都一起实施，因此，企业高管需要和各业务的整体负责人以及数据专家一起开展研讨会，分析哪些业务投入大数据可以使得业务的绩效提升最为显著，从而确定不同业务投入大数据的优先级，找准大数据的切入点。“数据能够在哪些领域实现业绩的大幅提高?数据能在哪些领域实现企业运营效率的提升”这些问题很重要，一开始就必须提出来。每个重要业务部门和职能部门都需要考虑这个问题，并展开相关的研讨。", "企业高管实施大数据战略的时候需要高度重视这一步,但在国内很多企业往往忽略的这一方面，投入大数据往往不是以提升业绩导向，而是以纯技术导向，或者是为了“赶潮流”，使得很多企业实施大数据的看不到数据对企业绩效提升，从而使得大数据战略流产。", "在确定了大数据的业务投入优先级后，我们需要考虑的是如何通过数据产品来帮助提升业务的绩效。为什么是“数据产品”而不是“数据工具”，这是因为“数据产品”比“数据工具”更加强调易用性和用户体验。", "数据和分析模型本身的输出可能会比较复杂，比较难理解，这样往往导致经理或者一线员工等数据用户不能理解，更称不上运用。所以，只有数据产品在业务具体的场景运用的时候，以非常简单易用的方式来呈现，才能让更多的数据用户使用。企业数据用户(往往是业务、产品、营销负责人等非大数据专业人士)在实际运用大数据的时候，更关注的是大数据的产品在哪些方面可以直接帮忙提升绩效，不需要关注大数据这些产品背后的逻辑、分析模型等“黑洞”。", "如果我们在提供数据产品的时候需要数据用户理解很多“黑洞”，那么数据一定运用不起来，数据的价值就会大打折扣。比如，数据产品可以告诉营销人员，您这次合作的营销推广渠道有所带来的用户40%是作弊而来，我们把这些作弊渠道带过来的用户叫“假量”，数据产品不需要告诉营销人员“假量”是如何计算的，但知道结果和优化方向即可;或者数据产品可以直接告诉营销人员哪些产品和其他产品可以做交叉销售，数据产品可以根据用户需求自动化实现交叉销售，从而进一步提高销售额，", "数据产品背后的“黑洞”是数据模型。数据的简单堆砌不会创造太多的业务价值，需要数据模型、数据挖掘的方法来实现海量数据的商业洞察。", "常见的模型如预测和分类。", "在预测方面，如通过高级的模型来预测哪些用户可能会付费，他们的特征是什么，经常在哪里出没;通过数据模型来预测付费客户的数量，以提前发现考核期结束后付费客户数量和KPI的差距以及优化方向;通过预测模型来洞察用户的未来购买需求;", "在分类模型方面，我们可以通过分类模型结合大数据实现更准确更实时的用户细分;或者通过分类模型对不同价值的客户进行合理的分类，确定服务的优先级和服务内容。", "企业在制定大数据战略方向时，需要介入数据专家根据应用场景和数据产品的输出来选择模型以及优化模型，从而确定模型研发的方向和优先级。", "有了应用场景、数据产品和数据模型这三大方面，我们就能更清楚的知道：为了实现这三大方面，我们需要哪些数据，什么数据是企业现在拥有，什么数据可以通过合作产生，什么数据需要外部整合，什么数据需要进行购买或者投资。", "有了前面这三大方面(应用场景、数据产品和数据模型)的规划，大数据的采集、整合、管理的策略便能比较容易理清头绪和相应的规划。当我们合理的整理企业所拥有的数据，并整合有利于业务发展的外部的数据，形成系统化的管理，才能很好的形成企业的数据资产。", "但在国内，最大的问题常常是各业务部门、各事业部以及职能部门的数据经常各自为政，数据存放在不同的数据库中，数据无法整合打通，企业内部形成各种孤岛，导致企业数据资产无法发挥整合效益，数据资产流失。要让企业的数据成为长期的数据资产，企业高管则需牵头规划，整合不同业务部门、不同事业部的数据，推动建设高数据质量的数据治理标准。", "值得注意的是，为了加快大数据的推进速度，企业高管同时需要确定什么方面企业自己开发实现，哪些方面委托第三方实现，哪些方面需要购买。在数据产品和数据模型方向，不一定所有工作都需要内部员工实现。领导层需要根据时间和自身资源(尤其是人力资源)的情况判断，哪些数据产品自己开发、哪些数据产品可以委托第三方公司开发、哪些数据模型自己开发、哪些数据模型委托第三方公司开发。", "在收集外部数据的时候，我们既可以组建自己的团队进行数据收集，或者委托第三方公司帮忙收集，或者直接采购，或者收购相关的数据公司，总之，在企业需要根据自身情况进行合理的规划。", "在互联网和大数据高速发展的时代，大数据正在深刻地改变商业前景。如果企业要想抓住这个机遇，企业高管的数据决策力，数据管理能力也需要加强。抓住和大数据相关的机会可以增加企业营收、提高企业运营效率，甚至开拓出全新业务。", "大数据在推进的过程中，最关键是要高管重视，高管重视不仅仅是挂在口上，而是要考虑在决策层构建数据方向的决策力和领导力，否则企业很难把大数据用好。因此，需要在决策层增加数据高官，如果不增加新的数据高管力量，很多组织的大数据大计将难以启动。", "因此，高管团队中需要有专人负责制定大数据战略、跟进、监控和指导大数据战略的实施。如果没有在高管团队设立相关的数据负责人的职位CDO(首席数据官)，则很难把数据分析和数据挖掘所发现的机会应用于企业战略层的业务发展决策以及相应的组织层面的变革。所以，我们建议，如果企业确实要推动大数据，一定要考虑设立CDO职位。", "CDO是一个综合能力要求非常高的职位。CDO需要跟各业务负责人有很好的互动，深入了解业务和未来两三年的业务发展规划，在此基础上，制定在数据应用场景、数据产品化、数据建模、数据资产管理的战略并推动实施。CDO在推动数据战略的实施过程中，还需要构建企业的数据化思维方式，推动构建相应的数据企业文化和制度，通过文化和制度使得大数据技术更有效的促进业务绩效的提升，企业运营效率的提升，甚至是新商业模式的变革。", "这里面还有一个比较重要的问题是CDO的回报对象的确定。很多人会问，CDO汇报给CEO、COO还是CTO哪位更合适。企业往往陷入一个误区，觉得数据是技术工作，所以不少企业设立数据高管后，让数据高管直接向CTO汇报，这样的做法有可能导致数据和业务会有较大的脱节。", "我们建议是，要根据企业实际情况来选择CDO的汇报对象。如果您的大数据战略刚起步，还需要从搭建整个企业的大数据底层平台开始，大数据的业务应用短期内较少，那么大数据可以向CTO汇报;如果企业已经有一定的数据基础，数据平台已经成型，那么，我们建议CDO可以向COO或者CEO回报，这样数据才能离业务更近，更能敏捷的应用于业绩的提升上。", "我们所看到的大数据运用的较好的企业，数据负责人经常和业务负责人一起制定公司大数据实施计划，一起推进大数据在业务绩效提升。", "企业的组织结构是企业战略能够顺利实施的基础，所以，大数据团队合理的在组织架构设置对于大数据战略能否成功实施尤为关键。国内很多企业往往忽略的这一方面。很多企业设立数据团队缺乏统一的规划，哪个事业部需要数据人员则在该事业部(或业务部门)设立，如下图的“组织结构1”，这种组织架构是国内最常见的，这种组织架构最大的问题是数据分散，缺乏统一管理和整合，企业内部各事业群(或业务部门)数据各自为政，形成数据孤岛，数据无法整合使用，导致数据资产流失。", "另一种常见的做法是在公司只设立一个中央数据部门，该数据部门统一服务各个事业部(或业务部门)，各个事业部(或业务部门)没有数据人员或者团队，如图中的“组织结构2”。这种组织架构的问题在于数据虽然集中管理，但数据远离业务，导致很多数据人员不理解业务，无法挖掘数据的价值，无法通过数据很好的辅助业务提升绩效或者运营效率。", "由于数据人员无法理解业务，导致数据库中存储的很多数据变成“死”数据，数据的业务含义少有人理解，数据的价值便容易流失。", "我们认为较为合理的数据团队在组织架构应该这样设立(如图中的组织架构3)：首先，设立公司级的中央数据部门，集中存储和管理数据;其次是每个事业部(或业务部门)设立数据团队;第三是在总办设立CDO的岗位。", "这样的好处在于数据能够集中管理，数据贴近业务，可以很好的发挥数据的价值;同时，在总办(高管团队)设立CDO岗位，可以让数据更好的为决策层服务，数据分析所发现的商业价值也可以更快的应用于业务战略调整。", "大家比较关心的是，在这个组织结构下，中央数据部门和各事业部(或业务部门)的数据团队有何差异。我们可以从两大方面来区分：", "事业部的数据团队负责人向所属事业部的总负责人汇报，中央数据部门的负责人向CDO汇报，这样的汇报关系的好处在于，前者让数据能为具体的事业部服务辅助提升业绩，每个事业部必然有其不同的数据分析重点，这样可以让数据服务更有针对性，后者让数据更有大局观，能为总办做深度的数据洞察服务。", "中央数据部门负责数据的规范化集中存储和管理，负责公司各业务线数据的整合打通，形成公司级统一的用户(客户)画像，负责标准化的数据产品并应用到各业务线中，形成深度的公司级的数据模型和算法，做出公司集团层面视角的分析和洞察;", "事业部中的数据团队负责该事业群的日常统计分析和事业群专题类的深度洞察，并辅助事业群的技术人员合理的把数据规范的上报到中央数据部门，与中央数据部门合作，共同深刻理解该业务的数据结构、做更精细且与本部门关联性更高的用户画像等与业务关联度更高的数据工作，推动该事业群所有数据的集中化到中央数据部门，并辅助推动公司级的数据产品应用到本业务部门或者向中央数据部门提出数据产品化、数据建模的需求。", "人才是大数据战略实施至关重要的方面，因此，设置符合大数据能力要求的团队显得尤为重要。如果组织缺乏合适的人才或能力，大数据战略实施的结果很可能会令人沮丧。因此，企业做好相应的人才规划，按照合理的规模和构成来建设人才库。", "在上文提到，在合理的大数据组织架构下，有两类数据团队，一类是各事业部中的数据团队;第二类是中央数据部门的数据团队。上文提到两类团队其职责不同，因此，能力要求也不一样。事业部的数据团队能力要求是数据分析为主，招聘主要为数据分析师或者数据分析专家。", "而中央数据部门的数据能力要求较为复杂，包括六大方面的能力，即数据分析、用户研究、数据产品、算法工程、数据统计和数据平台。在此我们展开介绍中央数据部门六大方向的能力要求：", "(1)数据分析团队负责公司级的业务数据体系梳理和建设、公司级的业务专题数据分析和收入分析;此处的数据分析团队能力要求与事业部中的数据分析团队类似，区别主要是他们分析时的视角有所不同，中央数据部门的数据分析团队要站在整个公司角度去审视业务，洞察不同业务的问题，发现机会;", "(2)用户研究团队负责用户调研(调查问卷、座谈会、访谈以及眼动仪等)、口碑监测、产品体验分析等方面。用户研究团队主要面对的小数据，但由于用户研究可以发现大数据所不能发现的用户使用行为背后的动机及态度等方面，所以用户研究团队与数据分析团队两者结合将能实现大小数据结合全方面洞察用户的作用;", "(3)数据产品团队负责把分析能力产品化、或者基于算法或者模型所产生的数据产品(如渠道防作弊系统、个性化推荐系统等)、数据平台相应系统的产品化、数据可视化等方面的工作。该团队人员类型有数据产品经理、前台开发以及交互设计师等;", "(4)算法工程团队主要负责算法研究并把算法能力嵌入到业务的流程或者业务产品中，帮助业务提升业务绩效或者提升运营效率。研究的方向包括分类算法、个性化推荐算法、基于数据挖掘的客户生命周期管理等方向。算法工程团队主要是招聘算法工程师，对数据敏感，要求数学和机器学习方面的能力较高，同时算法工程化的能力较好;", "(5)数据统计团队主要负责完成各事业部提出的统计需求，并把统计结果展示到报表系统，同时还负责元数据管理、数据处理、数据统计、数据质量控制和维护等方面工作。", "(6)数据平台团队主要负责数据统计产品的后台开发、数据仓库建设、数据接入系统、计算任务调度系统、元数据管理系统和实时计算能力的建设等方面工作。", "大数据的顺利实施还需要构建数据决策的企业文化和相关的制度来保驾护航。大数据没有企业高管的重视，没有一线员工积极的参与，在实施时会变得效率很低。通过企业文化和相关的制度调动组织的积极性，才能让大数据的实施取得更好的效果，具体做法有三大方面：", "(1)转变思维方式，形成数据决策的文化。企业文化本质是老板文化，如果要构建数据数据决策的文化，企业老板们则需要形成看数据的习惯，老板要带头看数据，比如通过邮件看每天的关键指标的日报、看每周的周报，看月报、季报等。无论是日报周报还是月报，一旦发现数据有异动，则马上回复邮件问数据异动原因。同时，老板在做相关决策的时候，形成用数据决策的习惯，让下属提供充足的数据决策依据，这样会驱动员工才更关注数据。", "(2)相关岗位能力增加数据分析能力。在企业可能用数据较多的职位如运营岗位、客户服务岗位、营销岗位、人力资源、产品设计等增加数据分析能力的要求，员工在各自方向晋升的时候，需要评审其数据分析能力，需要举证相关的数据支撑日常工作的案例。通过这样的要求，员工自然会对数据的使用度更高。", "(3)通过系统化的培训来培养员工的数据分析能力。由专业数据分析人员和算法人员设计的数据分析解决方案或者产品必须以简单易用的方式提供给一线员工，同时，更为重要的是加强相关的解决方案的或者数据产品的系统化培训，让更多的员工意识到这些解决方案或者产品的价值，并乐于在日常工作中使用。", "有很多企业往往陷入一个误区，将绝大部分资金如超过80%用于建立模型或者开发数据产品，仅有很少的资金投入到一线的使用。我们的建议是，如果让大数据产品或者解决方案更为广泛被一线员工接受，数据建模/数据产品研发的费用和培训的投入应该是对半分的。为了更好的推进培训，企业还可以考虑成立兴趣驱动的数据协会，让更多的员工加入到该协会中，定期举行培训课程、研讨沙龙以及聘请外部专家作相关分享以开拓视野。", "总之，在2017年企业要启动大数据战略，让大数据提升企业运营效率以及提升业务绩效，需要从大数据整体规划、高层团队的CDO设立、组织结构的调整和优化、大数据团队的架构和企业文化与制度等五大方面制定符合企业情况执行方案。只有这样，我们才能够让大数据真正渗透到企业的“骨骼”和“血液”，让大数据和企业经营融为一体，发生化学反应，驱动业绩增长。", "文：傅志华", "关于作者：傅志华先生为中国信息协会大数据分会理事，北京航空航天大学软件学院大数据专业特聘教授，中科院管理学院MBA企业导师，首都经济贸易大学统计学院硕士生导师。傅志华先生现为某大型互联网企业大数据中心副总经理，曾为腾讯社交网络事业群数据中心总监以及腾讯公司数据协会会长。傅先生谙熟数据分析和数据挖掘方法，在数据驱动企业数据化运营和营销方面以及大数据驱动的新商业模式研究有丰富的经验。", "傅志华先生同时也是我们36大数据网站的专栏作者，不断的在网站发布着干货，将在工作实践中的领悟和大家分享,", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74845"]},
{"module": ["干货教程"], "note": ["\n文 | 100offer\n前段时间跟候选人聊天，一个有多年工作经验的资深 iOS 工程师告诉我，他最近正在学习 Machine Learning 相关的知识。他觉得，对于程序员来说，技术进步大大超过世人的想象，..."], "title": ["普通程序员，如何转为当前紧缺的大数据相关人才?"], "content": ["文 | 100offer", "前段时间跟候选人聊天，一个有多年工作经验的资深 iOS 工程师告诉我，他最近正在学习 Machine Learning 相关的知识。他觉得，对于程序员来说，技术进步大大超过世人的想象，如果你不跟随时代进步，就会落后于时代。", "我其实已经听过很多人跟我说过类似的话。只不过不同人嘴里提到的词汇各有不同——大数据、数据挖掘、机器学习、人工智能…… 这些当前火热的概念各有不同，又有交叉，总之都是推动我们掌控好海量数据，并从中提取到有价值信息的技术。", "程序员对这些技术跃跃欲试，知乎上「深度学习如何入门?」「普通程序员如何向人工智能靠拢?」等问题都有很高的关注度。我们在招聘市场也能够看到，越来越多的技术候选人在跳槽时会思考，能否从事相关岗位的工作。", "从 100offer 平台上的数据来看，大数据相关职位的面试邀请占比也与日俱增。", "当前，很多候选人对大数据相关岗位的青睐并非偶然", "处理器速度的加快，大规模数据处理技术的日渐成熟，让我们从 Big Data 中快速提取有价值的信息成为可能。几十年前神经网络算法被提出之初，捉襟见肘的计算能力很难让这个计算密集的算法发挥出它应有的作用。而现在，PB 级别的数据也可以在短时间内完成机器学习的模型训练。这让格灵深瞳、科大讯飞等高度依赖深度学习的图像、语音识别公司得以对产品进行快速迭代。", "互联网行业的快速发展，让不少公司拥有了成千上万的用户数据，各家都想挖掘这座储量丰富的金矿，由此延伸出数据在自家业务不同应用场景中的巨大价值——京东、淘宝等电商网站利用用户画像做个性化推荐，PayPal、宜信等互联网金融公司通过识别高危行为的特征实施风险控制，滴滴、达达等出行、配送业务利用交易数据进行实时定价从而使利润最大化……", "还有一些公司，借助大数据相关技术创造出新的业务模式——比如利用算法做个性化内容推荐的今日头条、一点资讯，比如通过监测服务整合海量数据、做数据价值变现的 TalkingData，当然还有一些底层架构的支持服务商如阿里云、UCloud 也开通了托管集群、机器学习平台等服务。", "这些企业整体对大数据、数据挖掘相关人才的需求非常之大，导致行业内人才的供给相对不足。因而薪资通常也相对高一些。", "再加上这些岗位相比于传统的软件工程，有更高的挑战空间和更大的难度，自然引得更多人才进入到这个领域。", "最近，为了了解大数据相关工程师的招聘现状，我们走访了几家紧需大数据相关人才的公司，与他们的技术 Leader 聊了聊相关人才的招聘现状。", "从各家招聘的工程师来看，与大数据打交道的核心工程师通常分为这么两大类", "他们的工作重心在于数据的收集、存储、管理与处理。", "通常比较偏底层基础架构的开发和维护，需要这些工程师对 Hadoop/Spark 生态有比较清晰的认识，懂分布式集群的开发和维护。熟悉 NoSQL，了解 ETL，了解数据仓库的构建，还可能接触机器学习平台等平台搭建。", "有些大数据开发工程师做的工作可能也会偏重于应用层，将算法工程师训练好的模型在逻辑应用层进行实现，不过有些公司会将此类工程师归入软件开发团队而非大数据团队。", "此类工程师的工作重心在于数据的价值挖掘。", "他们通常利用算法、机器学习等手段，从海量数据中挖掘出有价值的信息，或者解决业务上的问题。虽然技能构成类似，但是在不同团队中，因为面对的业务场景不同，对算法 & 数据挖掘工程师需要的技能有不同侧重点。因而这个类目下还可细分为两个子类：", "这类团队面对的问题通常是明确而又有更高难度的，比如人脸识别、比如在线支付的风险拦截。这些问题经过了清晰的定义和高度的抽象，本身又存在足够的难度，需要工程师在所研究的问题上有足够的专注力，对相关的算法有足够深度的了解，才能够把模型调到极致，进而解决问题。这类工程师的 Title 一般是「算法工程师」。", "有的团队面对的挑战不限于某一个具体问题，而在于如何将复杂的业务逻辑转化为算法、模型问题，从而利用海量数据解决这个问题。这类问题不需要工程师在算法上探索得足够深入，但是需要足够的广度和交叉技能。他们需要了解常见的机器学习算法，并知晓各种算法的利弊。同时他们也要有迅速理解业务的能力，知晓数据的来源、去向和处理的过程，并对数据有高度的敏感性。这类工程师的 Title 以「数据挖掘工程师」居多。", "没有一个技术 Leader 不希望自己手下是一班虎将。他们期盼团队中每个工程师都是能独当一面的全才。", "基础的逻辑、英文等素质是必须的，聪明、学习能力强是未来成长空间的保障，计算机基础需要扎实，最好做过大规模集群的开发和调优，会数据处理，还熟悉聚类、分类、推荐、NLP、神经网络等各种常见算法，如果还实现过、优化过上层的数据应用就更好了……", "嗯，以上就是技术 Leader 心中完美的大数据相关候选人形象。", "但是，如果都以尽善尽美的标准进行招聘的话，恐怕没几个团队能够招到人。现在大数据、数据挖掘火起来本身就没几年，如果想招到一个有多年经验的全才，难度不是一般的高。在这点上，各位技术 Leader 都有清晰的认识。", "不过，全才难招，并不代表 Leader 会放低招聘要求。他们绝不容忍整个团队的战斗力受到影响。面对招聘难题，他们会有一些对应的措施——", "刚刚提到了，要想为大数据相关岗位找到一个各方面条件都不错的人才，难度非常大。因而技术 Leader 会更加务实地去招聘「更适合的人」——针对不同岗位吸收具有不同特长的人才。", "以格灵深瞳为例，这是一家计算机视觉领域的大数据公司，团队中既需要对算法进行过透彻研究的人才，把图像识别有关算法模型调整到极致，也需要工程实力比较强的人才，将训练好的算法模型在产品中进行高性能的实现，或者帮助团队搭建一整套视频图像数据采集、标注、机器学习、自动化测试、产品实现的平台。", "对于前一种工程师，他需要在深度学习算法甚至于在计算视觉领域都有过深入的研究，编程能力可以稍弱一些;而对于后一种工程师，如果他拥有强悍的工程能力，即使没有在深度学习算法上进行过深入研究，也可以很快接手对应的工作。这两种人才需在工作中进行密切的配合，共同推动公司产品的产出与优化。", "即使在算法工程师团队内部，不同成员之间的技能侧重点也可能各不相同。", "比如个性化内容推荐资讯平台——一点资讯的算法团队中，一部分工程师会专注于核心算法问题的研究，对解决一个非常明确的问题(比如通过语义分析进行文章分类的问题，如何判断「标题党」的问题等等)，他们需要有足够深度的了解;另外一部分工程师，则专注于算法模型在产品中的应用，他们应该对业务非常有 sense，具备强悍的分析能力，能够从复杂的业务问题中理出头绪，将业务问题抽象为算法问题，并利用合适的模型去解决。两者一个偏重于核心算法的研究，一个偏重业务分析与实现，工作中互为补充，共同优化个性化内容推荐的体验。", "对于后者来说，因为对核心算法能力要求没有前者那么高，更重视代码能力与业务 sense，因而这个团队可以包容背景更丰富的人才，比如已经补充过算法知识的普通工程师，以及在研究生阶段对算法有一些了解的应届生。", "雇主对大数据相关候选人的经验、背景有更大接受空间，这就给了非大数据相关候选人进入大数据、算法团队的机会。此时，梳理清楚自己现有技能对于新团队的价值非常重要，这是促使新团队决定吸收自己的关键。", "现在在云计算服务商 UCloud 工作的宋翔，过去四五年一直致力于计算机底层系统的研究。在百度，他曾经为深度学习算法提供支持，用硬件和底层系统优化，加快机器学习算法的运算速度。进入 UCloud 之初，宋翔主要研究的方向也是如何利用 GPU 服务器进行运算加速。", "后来，考虑到越来越多企业依赖机器学习进行数据挖掘，UCloud 期望推出一个兼容主流开源机器学习系统的 Paas，使得使用这个机器学习平台的工程师能够专注于模型训练本身，而无需考虑模型部署、系统性能、扩展性、计算资源等问题。", "宋翔在底层系统优化上的特长刚好可以在这项工作中发挥，因而他立刻被赋予主导这个平台搭建的任务。", "让算法在机器上运转得够快，才能够缩短模型迭代的时间，加速模型优化的过程。大部分算法工程师可能对此了解甚少，但是宋翔可以充分发挥自己的特长，利用硬件和底层系统加速机器学习算法。", "当需要训练的数据量特别大的时候，比如几十 T 以上甚至 PB 级的时候，在分布式系统中， I/O 或者网络可能成为瓶颈了，这时需要系统工程师的介入，看怎么优化数据传输使得 I/O 的使用率提高;看怎么去存储，用 HDFS 还是用 Key Value Store 或者其他存储方式，可以让你更快地拿到数据去计算，或者你用磁盘的存储还是 SSD 存储 或者 in-memory 的存储。这其中，系统工程师也需要平衡成本和效率之间的关系。", "系统工程师还可以帮助你设计一个系统，让算法工程师快速地提交任务，或者方便地同时训练多个模型，尝试多个参数。", "系统工程师非常擅长把本来串行的工作拆分之后变成并行工作。比如可以把数据预处理和深度学习运算做一个并发，等等。", "除了对底层系统有深入了解之外，他现在也在了解机器学习的算法。他带领的小团队中，除了有2名系统工程师之外，还有两名算法工程师，他一直鼓励两种工程师互相学习，共同提高，这样才能够让整个团队效率最大化。如果系统工程师对算法不了解的话，可能也不知道怎么去优化算法运行的效率;算法工程师也应大概了解不同模型在CPU、GPU机器上的运算速度，帮助自己设计出更高效的算法。", "对于期望转岗为大数据相关的普通工程师来说，一旦通过自身擅长的技能切入新团队之后，就有了更多横向发展的机会，帮助自己在大数据相关领域建立更强竞争力。", "无论何种工程师，雇主都希望人才具备综合素质，而非片面苛求当前的技能水平。特别是对于当前市场供给偏少的大数据相关领域，已经在大数据、算法方面有所建树的人才毕竟只占少数。具备不错的基础素养，并拥有巨大潜力的工程师也很受企业青睐。这些工程师可以利用已有的工程实力完成一部分基础工作，并在经过1-2年的锻炼之后，接手更复杂的问题。", "我们可以把大数据相关工程师能力模型抽象为以下的核心技能金字塔", "越是偏金字塔底部的素养，对于企业来说越是重要。最底部的基础素养，代表的是未来的成长空间。当前互联网高速发展，每家企业都是跑步前进，如果一个当前技能不错的工程师，未来成长空间有限，也可能变成企业的负担。", "再上一层的计算机基础 – 基本的算法与数据结构，某一门编程语言的精通，是几乎每个工程师岗位都重视的能力。一个基础不扎实的程序员，可能会让企业怀疑其学习能力。扎实的基础，会为应用技能的学习扫除障碍，更容易建立深度的理解;而数学基础对于算法理解上的帮助十分重要。", "这最下方的两层构成了一个工程师人才的基础素养。如果底层的基础比较扎实，掌握应用层技能所需要的时间也许比我们预想的要少一些。", "格灵深瞳技术副总裁 – 邓亚峰提到：", "但是如果你算法、数据结构比较强，编程语言上对 C++ 比较理解，那你在应用层的学习上，可能会比其他人快很多。比如在深度学习上付出 1-2 年的时间，在图像 domain knowledge 上付出半年到一年就可以有基础的了解。", "其实现在计算机视觉领域更加依靠深度学习之后，特征选取等依赖 domain knowledge 的门槛已经降下来了，因而我曾见到不少有很好基础的人，包括一些基础扎实的应届生，在图像领域工作了半年到一年之后就能拿到不错的成绩。", "在看待大数据工程师的招聘上，TalkingData 的技术 VP 闫志涛和首席数据科学家张夏天也提到：", "TalkingData 的 两位 Leader 也为我举了一个自家团队中的例子：", "他们在14年招收了一位专科学校毕业的工程师，在上一家公司做过一点推荐算法，会写 Hadoop Mapreduce，但是并没有在大数据上有深入的研究。这位工程师当时的大数据技能并不能达到 TalkingData 的招聘标准，不过好在他思维清晰，看待问题有自己独特的想法。加之 Java 基础不错，在上一家公司做事情也很扎实，所以就招聘进来了。", "说到这里，两位 Leader 坦言「当时幸好还不怎么挑简历，也许按照后来的标准未必能把这位工程师招聘进来。」", "不曾想到，这位工程师主动性非常强，Leader 只需给到工作方向，他就会驱动自己学习相关知识，快速完成目标。2年以后，这位工程师的 Spark 能力已经锻炼得非常强悍，用 Leader 的话说「可以以一当十」;他对大数据、机器学习都有浓厚的兴趣，Spark 基础夯实之后，又转岗到了算法工程师团队，写出了 TalkingData 机器学习平台的核心代码，这个平台大大提高了团队的机器学习效率。", "从上边的例子中，我们也可以额外收获一个信息，相比于跳槽转岗，内部转岗会更容易一些。因为在公司内部中，企业有充分的时间考察工程师的能力、潜力。企业对工程师的认可度提升之后，才会更加放心的予以新的挑战。", "赵平是宜信技术研发中心的一位工程师，加入宜信之前，他曾帮助中国移动机顶盒业务的后端架构进行服务化转型。抱着对基础平台架构的浓厚兴趣，赵平加入了宜信。他在这家公司做的第一个项目是分布式存储系统的设计和开发。第一个项目完美收官之后，他的学习能力、基础能力备受褒奖。当宜信开始组建大数据平台团队时，赵平看到了自己理想的职业发展方向并提交了转岗申请，基于他过往的优异表现，顺利地拿到了这个工作机会。", "转岗之后，赵平也遇到了一些挑战，比如大数据涉及的知识点、需要用到的工具更加丰富，Spark，Scala，HBase，MongoDB…，数不清的技能都需要边用边学，持续恶补;比如思维方式上，需要从原来的定时数据处理思维向 Spark 所代表的流式实时处理思维转变。不过基于他扎实的基础，以及之前做分布式存储系统经验的平滑过渡，加之整个团队中良好技术氛围的协助，最终顺利完成第一个大数据项目的开发工作。", "在文章的末尾，我们基于文章中提到的多个案例，总结一下帮助普通工程师走向大数据相关岗位的几个 tips 吧：", "无论各种岗位，基础是成长的基石。", "从能够发挥自己现有专长的岗位做起，可以让新团队更欢迎你的加入。比如算法模型的工程化，偏重于业务的数据挖掘，大数据平台开发，机器学习系统开发等等，这些工作对于普通工程师更容易上手。而普通工程师直接转偏研究方向的算法工程师，难度更高。", "请预先做好相关知识的学习，有动手实践更佳。如果没有一点准备，雇主如何相信你对这个领域真的有兴趣呢?", "在同公司转岗阻力更小。亦可考虑加入一家重视大数据的公司，再转岗。", "也许你不会以此为职业，但是可以多一技傍身。", "也许，未来这些技能对于程序员而言，就好比现在 MS Office 对于职场人一样普遍。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74869"]},
{"module": ["干货教程"], "note": ["\n文 | 田军\n今天，继续来谈一谈“大数据项目如何落地?”这个话题。从事过多个大数据项目的规划方案及项目落地工作，在这里与大家分享一些心得，主要是关于大数据项目如何成功落地并取得预期目..."], "title": ["大数据项目如何落地之路线图探讨"], "content": ["文 | 田军", "今天，继续来谈一谈“大数据项目如何落地?”这个话题。从事过多个大数据项目的规划方案及项目落地工作，在这里与大家分享一些心得，主要是关于大数据项目如何成功落地并取得预期目标，也可以说这些是实践出来的观点。", "对于一个大数据应用项目/产品的落地，可以大致总结为五大步骤阶段：", "数据规划、数据治理、数据应用、迭代实施、商业价值。如下图：", "大数据项目落地路线图", "一个成功的大数据项目，需要有一个良好的开端，即做好数据规划阶段的各项工作，具体包括：", "上述Action完成后，需要从架构和落地角度，进一步深化：", "架构规划：根据已完成的产品(项目)规划、场景规划和需求评估，从落地的角度完成数据架构规划，架构规划是项目成功落地的重要环节。", "有的大数据项目，还需要引入第三方的数据支持，以及体系内其他非干系部门的数据支持，这样就需要进行有效合作。", "合作意图：如果项目需要引入第三方的数据支持，以及体系内其他非干系部门的数据支持，需要充分评估项目风险与合作意图，有效达成合作共识。", "第一阶段的工作完成以后，已经具备了一个大数据项目成功落地的良好基础，接下来就需要按照数据规划阶段的成果继续后续的环节，首先要做的就是要有数据，并且要有高质量的数据，数据到位才能保障项目的有效推进和执行：", "上述的几个环节，必要时需要借助专业的产品工具。", "数据管理：数据管理工作，将影响项目的整个周期，建议采用专业的数据管理产品和工具，或借助有开发能力的供应商量身定做一套数据管理系统。", "第三方数据：可以通过数据资产置换、购买等等方式完成第三方数据的接入。", "在整个第二阶段会形成一系列的标准和流程，这里不一一赘述。", "第一、第二阶段工作完成以后，就将进入最重要的第三阶段工作，在这个阶段中，我们将承前启后的推动大数据项目完成落地工作，真正去形成大数据的应用，带来真实的业务价值：", "场景细分：在这个阶段，对于第一阶段中形成的场景规划，要进行可被实现的场景细分，通过对场景的细分，形成一个个的用例(Use Case)。", "干系组织利益共识：通过场景的细分后的一个个用例(Use Case)，已经可以很好的明确给各干系组织带来的业务价值，在这个时候需要推动各个干系组织形成利益共识，以免由于利益问题导致项目执行的阻碍。", "完成上述Action后，就需要借助供应商的参与和力量继续完成后续的Action。", "技术选型、产品选型以及应用分析模型建立后，就需要进行验证工作了，主要包括场景PoC和商业验证。", "PoC：选取具有典型代表意义的大数据应用场景，进行现场的PoC验证工作，通过PoC，修正和完善每个用例(Use Case)，同时验证技术选型、产品选型的正确性，发现问题及时处理，甚至重新选择技术与产品。", "商业验证：PoC环节完成后，还需要进行商业验证，验证和评估一些关键场景用例(Use Case)的应用效果，评估和预测是否可以达成商业目标，从而推导出达成商业目标可能存在的问题和风险，进行修订与处理，必要调整各个干系部门和干系人之间的利益共识。", "前三个步骤阶段的工作有效得完成后，就进入了第四步骤阶段迭代实施，之所以是迭代实施，也跟大数据类项目的特征有关，就如大数据建立分析模型是一种探索的过程一样，大数据项目的执行也需要进行不断的验证、修正、实施这样的工作，可能需要经过多轮的迭代才能完成项目的建设：", "如果上述的环节发现了重大问题，则针对问题形成改进方案后进入迭代改进环节。", "迭代改进：迭代改进分为小迭代和大迭代，小迭代是在同一期项目中完成的，受到项目上线周期的制约，小迭代可以改进的问题是有限的、小型的。对于影响范围巨大，难度较高的问题需要进入大迭代改进，大迭代一版来说可以规划为项目的二期、三期等等，直到达成预期的战略意图、战略规划和商业目标。", "经过上述的Action环节，一个成功的大数据应用项目终于落地了，这也仅仅是落地的开始，接下来的工作是检验项目成果和真正发挥大数据价值的时刻：", "实施推广：围绕项目的战略意图、规划和商业目标，进行有效的实施推广工作将变得非常重要，良好的实施推广工作可以真正让大数据应用分析项目用起来，让数据“活”起来，源源不断产生价值。推广过程，要巧妙的运用各个干系部门和干系人之间的利益共识。", "数据安全：大数据项目有自己的特点就是一切都围绕数据来展开，说到数据就会涉及到一些隐私数据、高密级数据等等，不管在开发过程中、还是在推广过程中，亦或是在第二阶段的数据治理过程中，都需要严格遵守相关信息安全和数据保密的规划，从技术上和使用上都要保证数据的安全。数据安全是一个大数据项目真正可以成功的重要内容。", "前面四个步骤阶段工作很好的完成后，就是享受大数据应用项目成果的时刻了，相信在前面四个阶段的各个环节中，各个项目参与人员都受到了或多或少的各种折磨，不过这些折磨都是值得的，因此大数据项目真正可以为企业带来不可以预想的巨大价值，只有上马了成功大数据项目的企业才能深深体会到。", "在这个阶段中，企业获得了：", "有效获取了内部商业利益价值、外部商业利益价值，真正去实现了企业建设大数据应用项目的战略意图、战略规划和商业目标。", "如果企业的大数据能力和人员有限，上述路线图中提到的每个步骤阶段，都可以引入供应商来协助企业完成。既可以选择一家供应商负责完成整个项目过程的建设，也可以分步来实施，在不同的阶段选取不同的供应商来完成。", "一般来说，建议后面三个阶段最好选择一家有综合能力的供应商来总包实施，这样可以更好完成项目的预期目标。", "下图大致总结了选择合作伙伴的一点参考，仅供参考：", "大数据项目落地路线图，供应商选择参考", "以上是“大数据项目如何落地?”路线图，是一些项目心得，也可以说这些是实践出来的观点，期望对大家有所帮助。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75247"]},
{"module": ["干货教程"], "note": ["\n文 | 谢佶芸\n1.什么是A/B测试\nA/B 测试，简单来说，就是为同一个目标制定两个方案，让一部分用户使用 A 方案，另一部分用户使用 B 方案，记录下用户的使用情况，看哪个方案的效果更好，以便..."], "title": ["A/B测试系统设计"], "content": ["文 | 谢佶芸", "A/B 测试，简单来说，就是为同一个目标制定两个方案，让一部分用户使用 A 方案，另一部分用户使用 B 方案，记录下用户的使用情况，看哪个方案的效果更好，以便全面推广。", "A/B 测试在有的公司又称为小流量测试或者灰度发布，原因：", "一 是为了统计新功能的效果;", "二 是为了在全流量上线前修复可能出现的BUG。", "虽然在业务上的含义有所差异，但是在系统设计上却是完全相通的。", "A/B测试有三个主要的功能模块：", "一是用户分类", "二是效果统计", "三是流量分配", "用户分类是A/B测试的核心。", "为了满足不同的业务场景，这里简述两种分类方案：", "这种分类方案的使用场景是新功能是针对全站的。而操作方法也很简单：(userId % X) < n。其中X是抽样的比例，例如按照1%抽样，那么X就等于100。而n是抽样的份数，这里有一个流量从小到大的过程，也就是n从1到X的过程。", "这种分类的使用场景是新功能是针对某种特定用户的。比如我们针对大学生人群设计了一个新功能，如果采用全用户抽样，那么不是我们的特定用户的统计效果就会冲淡新功能的真实效果，导致统计结果趋于平淡，无法体现真实效果。这时候我们就需要针对特质人群来做抽样。", "不同的分类方法模块设计也会不同，这里提出一种比较通用的用户分类模块设计方案：用户标签(tag)系统。", "我们把用户的分类用tag来表示，属于某个分类的用户，就给他打上一个对应的tag。怎么找到某个分类的用户需要具体情况具体分析，这里就不深入了。我们需要建立一个用户tag存储模块，这个模块的功能非常简单，就是存取一个用户的tag。这个模块可以仅仅只是一个Redis服务器，也可以是一个数据库，还可以是一个RESTful Service。根据业务需要进行选择。", "有了用户tag存储模块后，我们把我们需要抽样的用户打上一个tag，比如大学生。有时候为了更精确的统计效果，我们在统计的时候不是小流量和全流量对比，而是两个对照组对比，那么两个对照组都需要打上tag，比如大学生A，大学生B。有时候我们需要一个流量从小到大的过程，那么我们就把目标人群分成n份，打上不同的tag，比如大学生1…大学生n。这样在测试过程中，我们可以不断增加测试流量。", "有了tag系统后，效果统计模块就很好设计了。我们在出报表的时候，根据tag的不同，把用户对应的数据统计到不同的报表里，自然就可以出两组对比报表。至于具体需要统计哪些维度的哪些指标，这个就根据业务的需要来选择，此处暂且不表。", "流量分配模块针对不同的业务需要也有很多种设计方案，这里简单提几种：", "这种设计方案最简单。首先把不同的代码部署到两组服务器上面，各自独立，然后在nginx上面加载一个模块，根据用户的tag和流量分配规则，把流量转发到不同的服务器。这种方案的缺点很明显，对于小流量，如果上一台服务器就存在单点问题，上两台服务器又存在资源浪费问题，在两组服务器上面的压力会不均匀。以服务器作为最小分配单元粒度显得太大了。", "这种方案由于对代码不透明，所以开发人员需要完全了解流量分配规则、完全跟进流量从小大到的变化等过程，而且要开发很多相关的代码，在流量完全上线后又要删除这些代码，造成人力资源浪费。", "通过独立的模块进行流量分配控制，在业务代码里面通过注解的方式进行流量分配。", "这种方案是我们推荐的设计方案，接下来会详细介绍这种设计。", "在现代软件开发中，为了避免系统存在单点故障，各种软件都是作为分布式多机部署的。在多机部署的情况下，流量分配模块的数据同步问题就必须考虑。我们推荐采用zookeeper进行基本数据存储，而像用户标签则存储在另外的独立系统中。利用zookeeper的通知机制，我们可以动态的改变流量分配策略。", "SDK依赖这两个存储系统来进行流量分配。Zookeeper里面存储采用何种策略来分配流量，而用户标签系统只存储用户的标签。业务系统集成SDK只需要在配置里面初始化SDK，然后在需要流量分配的方法上面写上注解就可以：", "我们目前支持两种注解级别：", "一是类级别，也就是整个类里面的所有方法的调用都涉及到流量分配;", "二是方法级别，就是只有这个方法的调用涉及到流量分配。", "这里详细说一下方法级别的注解如何使用。", "首先创建一个interface，当然也可以使用已有的interface：", "然后针对两种实现创建这个interface的两个实现类，第一个是默认的实现类：", "默认的实现类上面需要添加@Primary注解，表示这个是默认实现类。", "在需要流量控制的方法上，添加@Switch注解，表示一个动态开关。featureUID表示这个开关的具体策略，alterBean表示开关打开之后需要调用这个bean上面的相应的方法。", "接下来是我们第二个实现类：", "最后是调用这个方法的时候怎么传入环境参数：", "到这里SDK的集成就全部完成了。根据具体的策略和用户的tag，我们会选择正确的方法去执行。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75154"]},
{"module": ["干货教程"], "note": ["\n文 | TalkingData\n对于数据分析，我发现很多运营都有这样一些困惑：\n\n不知道从哪里获取数据;\n不知道用什么样的工具;\n不清楚分析的方法论和框架;\n大部分的数据分析流于形式。\n\n其实，数据分..."], "title": ["零基础搭建运营数据分析知识体系"], "content": ["文 | TalkingData", "对于数据分析，我发现很多运营都有这样一些困惑：", "其实，数据分析并没有大家想象的那么难", "大家一直在说收集数据和数据分析，但是对于两者具体的定义又很难说清楚。很多人都会先入为主，认为数据就是各种表格、各种数字，例如excel报表、各种数据库。其实这是一个错误或者说有偏差的认识，它会使得我们对数据的认识变得很狭隘。", "数据(data)是描述事物的符号记录，是构成信息或者知识的原始材料。这种哲学层次的定义，让数据的范围极大丰富，也符合目前“大数据”发展的需要。", "作为一名运营人，我们接触到的数据可能没有那么复杂，但是也有很多类别。", "从数据的来源来看，可以分为企业外部数据和内部数据。外部数据主要包括宏观经济、新闻舆情、社会人口、和市场调研数据;内部数据包括用户行为数据、服务端日志数据、CRM与交易数据。不同数据的获取途径、分析方法、分析目的都不经相同，不同行业、不同企业在实际分析中也都各有偏好。", "数据分析是指从数据中提取有用的信息，并指导实践。", "但需要注意的是：", "这些信息需要用来指导实践，而不是流于形式;", "需要提取的是有用的信息，而不是自嗨。", "很多人刚接触数据分析的时候，都深感无从下手。所以我们需要宏观的方法论和微观的方法来指导。那么方法论和方法有什么区别?", "方法论是从宏观角度出发，从管理和业务的角度提出的分析框架，指导我们接下来具体分析的方向。", "方法是微观的概念，是指我们在具体分析过程中使用的方法。", "数据分析的方法论很多，这里我给大家介绍一些常见的框架。", "数据分析的方法论很多，没有最好的方法论，只有最合适的。详细介绍一下 AARRR 方法论，对于精益化运营、业务增长的问题，这个方法论非常契合。", "对于互联网产品而言，用户具有明显的生命周期特征，下面我以一个O2O行业的APP为例阐述一下。", "通过各种线上、线下的渠道获取新用户，下载安装APP。", "安装完APP后，通过运营手段激活用户;比如说首单免费、代金券、红包等方式。", "通过一系列的运营使部分用户留存下来，给企业带营收。", "如果用户觉得这个产品不错，可能推荐给身边的人;或者通过红包等激励手段鼓励分享到朋友圈等等。", "需要注意的是以上环节并非完全按照上面顺序来的;运营可以根据业务需要灵活应用。 AARRR的五个环节都可以通过数据指标来衡量与分析，从而实现精益化运营的目的;每个环节的提升都可以有效增长业务。", "借助常见的网站/APP数据分析产品，我们非常快速的完成这7种分析。", "趋势分析是最简单、最基础，也是最常见的数据监测与数据分析方法。通常我们在数据分析产品中建立一张数据指标的线图或者柱状图，然后持续观察，重点关注异常值。", "在这个过程中，我们要选定第一关键指标(OMTM，One Metric That Matter)，而不要被虚荣指标(vanity metrics )所迷惑。", "以社交类APP为例，如果我们将下载量作为第一关键指标，可能就会走偏;因为用户下载APP并不代表他使用了你的产品。在这种情况下，建议将DAU(Daily Active Users，日活跃用户)作为第一关键指标，而且是启动并且执行了某个操作的用户才能算上去;这样的指标才有实际意义，运营人员要核心关注这类指标。", "多维分解是指从业务需求出发，将指标从多个维度进行拆分;这里的维度包括但不限于浏览器、访问来源、操作系统、广告内容等等。", "有时候一个非常笼统或者最终的指标你是看不出什么问题来的，但是进行拆分之后，很多细节问题就会浮现出来。", "举个例子，某网站的跳出率是0.47、平均访问深度是4.39、平均访问时长是0.55分钟。如果你要提升用户的参与度，显然这样的数据会让你无从下手;但是你对这些指标进行拆解之后就会发现很多思路。", "下面展示的是一个产品在不同操作系统下的用户参与度指标数据。", "仔细观察的话，你会发现移动端平台(Android、Windows Phone、IOS)的用户参与度极差，表现在跳出率极高、访问深度和平均访问时长很低。这样的话你就会发现问题，是不是我们的产品在移动端上没有做优化导致用户体验不好?在这样一个移动互联网时代，这是非常重要的一个问题。", "用户分群主要有两种分法：维度和行为组合。第一种根据用户的维度进行分群，比如从地区维度分，有北京、上海、广州、杭州等地的用户;从用户登录平台进行分群，有PC端、平板端和手机移动端用户。第二种根据用户行为组合进行分群，比如说每周在社区签到3次的用户与每周在社区签到少于3次的用户的区别，这个具体的我会在后面的留存分析中介绍。", "正如前面所说的，用户行为数据也是数据的一种，观察用户在你产品内的行为路径是一种非常直观的分析方法。在用户分群的基础上，一般抽取3-5个用户进行细查，即可覆盖分群用户大部分行为规律。", "我们以一个产品的注册流程为例：", "用户经历了如下的操作流程：【访问官网】-【点击注册】-【输入号码】-【获取验证码】。本来是非常流畅的一个环节，但是却发现一个用户连续点击了3次【获取验证码】然后放弃提交。这就奇怪了，用户为什么会多次点击验证码呢?这个时候我建议您去亲自体验一下您的产品，走一遍注册流程。", "你会发现，点击【获取验证码】后，经常迟迟收不到验证码;然后你又会不断点击【获取验证码】，所以就出现了上面的情况。绝大多数产品都或多或少存在一些反人类的设计或者BUG，通过用户细查可以很好地发现产品中存在的问题并且及时解决。", "漏斗是用于衡量转化效率的工具，因为从开始到结束的模型类似一个漏斗，因而得名。", "漏斗分析要注意的两个要点：第一，不但要看总体的转化率，还要关注转化过程每一步的转化率;第二，漏斗分析也需要进行多维度拆解，拆解之后可能会发现不同维度下的转化率也有很大差异。", "某企业的注册流程采用邮箱方式，注册转化率一直很低，才27%;通过漏斗分析发现，主要流失在【提交验证码】的环节。", "经过了解发现，邮箱验证非常容易出现注册邮箱收不到邮件的情况，原因包括邮件代理商被屏蔽、邮件含有敏感字被归入垃圾邮箱、邮件送达时间过长等等。既然这么多不可控因素影响注册转化率，那就换一种验证方式。换成短信验证后，总体转化率提升到了43%，这是非常大的一个增长。", "留存，顾名思义就是新用户留下来持续使用产品的含义。衡量留存的常见指标有：次日留存率、7日留存率、30日留存率等等。我们可以从两个方面去分析留存，一个是新用户的留存率，另一个是产品功能的留存。", "以社区网站为例，“每周签到3次”的用户留存率明显高于“每周签到少于3次”的用户。签到这一功能在无形中提升了社区的用户的粘性和留存率，这也是很多社群或者社区主推这个功能的原因。", "首次注册微博，微博会向你推荐关注10个大V;首次注册LinkedIn，LinkedIn会向你推荐5个同事;申请信用卡时，发卡方会说信用卡消费满4笔即可抽取【无人机】大奖;很多社交产品规定，每周签到5次，用户可以获得双重积分或者虚拟货币。在这里面“关注10个大V”、“关注5个同事”、“消费4笔”、“签到5次”就是我想说的Magic Number，这些数字都是通过长期的数据分析或者机器学习的方式发现的。实践证明，符合这些特征的用户留存度是最高的;运营人员需要不断去push，激励用户达到这个标准，从而提升留存率。", "A/B测试是为了达到一个目标，采取了两套方案，一组用户采用A方案，一组用户采用B方案。通过实验观察两组方案的数据效果，判断两组方案的好坏。", "在A/B测试方面，谷歌是不遗余力地尝试;对于搜索结果的显示，谷歌会制定多种不同的方案(包括文案标题，字体大小，颜色等等)，不断来优化搜索结果中广告的点击率。", "有了具体的分析方法还不够，运营要做好数据分析还需要一个清晰的流程。在这里从宏观、中观和微观三个层次给大家介绍一下。", "风靡硅谷的精益创业，它推崇MVP(最简化可行产品)的理念，通过小步快跑的方式来不断优化产品、增长用户。", "在运营工作中，我们要大胆尝试，将想法转化成产品和运营方法。然后分析其中的数据，衡量产品或者运营的效果。如果好的话保持并大力推广，如果不好的话总结问题及时改进。在“构建-“衡量”-“学习”的不断循环中逐渐优化，这个流程是非常适合运营工作的。", "我们可以试着树立整体的流程：1.明确分析目的和思路 →2.数据收集 →3.数据处理 →4.数据分析 →5.数据展现 →6.报告撰写。", "这个流程只是从“数据”的角度阐述了前后的流程，并未结合业务实际;但值得注意的是数据分析的最终目的是为了指导实践，而不是写一份报告。", "下面介绍的是一个非常详细的分析流程，借助于一定的分析工具，我们可以按照这个思路对网站/APP进行细致入微的分析。", "但这个流程是具有前提的，前提是用数据分析工具做好数据采集和监控工作，把精力集中在业务分析上。这个流程的核心是“MVP”的理念，“发现问题”-“设计实验”-“分析结果”，通过数据来不断优化产品和运营。", "你们喜爱的彩蛋君在公司从事新媒体工作，负责微信的日常运营，阅读量时高时低，总体一般。彩蛋君想办法改进一下微信运营，提高微信的粉丝数和阅读数;", "我们从数据分析的角度对这个很多运营人都面对的问题进行了讨论：", "不清楚自己需要关注哪些核心指标;", "不清楚目标用户的特征(用户属性、用户画像等);", "对自己过往工作缺乏系统分析(数据采集、监测和分析)。从业务增长的角度出发，为配合其内容工作的开展，定制一份分析体系至关重要。", "运营需要明确知道自己的目标或者KPI，然后选择一个核心关键指标(OMTM)进行监测。如果是创业公司，初期可能需要拉新，那么核心指标是注册用户数或者新访问用户数。如果是资讯媒体，注重影响力和覆盖面，那么核心指标应该是微信阅读数或者网页PV。", "无论是哪一种运营岗位，都需要明确知道自己的(目标)用户是那些人?这些人都有哪些特征，他们的关注点和痛点是什么?如果你的用户是产品经理，那么可以尝试爬虫抓取产品经理网站上有关的问题，然后做文本分析：这是定量层面的分析。同时，通过调查访问和问卷调研，获取更加深入的用户特征信息：这是从定性层面的分析。", "借助数据分析工具，对核心关键指标(OMTM)进行持续监测。对于指标异常情况，我们需要及时分析和改进。", "统计和分析过往内容的数据，找出哪些内容、哪些标题、哪些形式、哪些渠道的效果更好，然后朝这方面不断优化。", "电子邮件营销是现在很多企业仍在采用的营销和运营方式，某互联网金融企业通过EDM给新用户(有邮件地址但是未注册用户)发送激活邮件。一直以来注册转化率维持在20%-30%之间，8月18日注册转化率暴跌，之后一直维持在10%左右。", "这是一个非常严重的衰退，需要立即排查原因。EDM渠道注册转化率涉及到太多的因素，需要一个一个排查，我们列举可能的原因：", "技术原因：ETL(数据抽取、转化、载入)出现问题，导致后端数据没有及时呈现在BI报表中;", "宏观原因：季节性因素(节假日等)，其余邮件冲击(其余部门也给用户发邮件稀释了用户的注意力);", "微观原因：邮件的标题、文案、排版设计，注册流程设计。", "一个简单的业务指标，会影响到它的因素可能是多种多样的，所以我们需要对可能涉及到的因素进行精细化衡量才能不断优化。最后发现，产品经理在注册环节添加了『绑定信用卡』，导致注册转化率大幅度下降。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75643"]},
{"module": ["干货教程"], "note": ["\n文 | 毕马威大数据挖掘\n当今社会，“信用”越来越受到人们的重视，不论是个人还是企业，都期望获得尽可能高的信用评分，以享受到更多的授信额度、更优惠的利率等。那么，银行究竟是如何对客户..."], "title": ["银行客户信用评分及实现"], "content": ["文 | 毕马威大数据挖掘", "当今社会，“信用”越来越受到人们的重视，不论是个人还是企业，都期望获得尽可能高的信用评分，以享受到更多的授信额度、更优惠的利率等。那么，银行究竟是如何对客户进行信用评分的呢?就让我们通过本期的文章一起了解一下吧。", "想必很多人多有贷款或申请信用卡的经历，那么“大金主”银行的钱从何而来?其中一小部分是银行的自有资本，其余大部分来源于储蓄、资本市场发行债券或股票等，也就是说，“大金主”是拿着别人的钱放贷，而他自己不过是一个资金中转、流通的“媒介”。如果把“别人”的钱借出去了，借款者却违约，钱收不回来怎么办?银行不仅没有收到贷款利息，反而要“自掏腰包”替借款人补上尚未还清的钱，因此便造成了损失。这就是我们常说的银行所面临的“信用风险”。", "2008年，美国爆发了严重的金融危机，波及实体经济，导致全球经济低迷。早在2006年，那些向信用状况较差的人发放的住房贷款，就已然出现了问题，贷款违约率的不断上升，最终酿成了历史性的悲剧。如何监测、计量信用风险的发生，通过科学的手段度量和有效的管理信用风险，是银行需要面对的长久性问题。", "首先，何为“信用”?俗话“有借有还”从道德上对信用进行了定义，然后银行与其客户之间“借贷”的关系，往往较为复杂。通常，银行需要全方位、多角度地去评价客户，确认客户的“信用”，才能放心地把钱“借”出去。", "我们都熟悉支付宝芝麻信用分，它是通过采集个人用户信息，经过加工、计算得出用户的信用得分，当然，分数越高代表信用越好。", "这几个维度包含了用户个人基本信息、好友互动信息、信用账户情况及履约历史、购物及理财等行为偏好等多项内容，通过大数据技术，最终以分数值的形式，形成对用户信用的准确评价。这就是信用评级。", "随着数学和统计技术在传统金融行业的广泛应用和推广，银行业也采用了“定量”的形式，多角度分析、判断不同客户的不同的信用等级，从而来决定客户可获取的授信额度、首付额度或利率优惠程度等，以科学手段准确地计量客户的“信用”，从而避免因借贷双方信息不一致而引发的信用风险损失。", "现如今，早已不是拨打算盘手工记账的年代，社会上任何活动都拖离不了信息系统，当然，这些信息系统中，也无时不刻地记录着你的所有行踪，这就是所谓的“数据”。对这些数据的存储、清洗、加工，都为银行对客户信用评级提供了健全、丰富的信息来源;基于此，银行以大数据技术进行分析和计算，从而准确地对客户进行信用评价。", "银行进行客户信用评级的数据来源于银行内部系统产生的数据或外部的数据，如图示：", "从客户的第一次开户开始，其与银行的每一次交互都将银行的信息系统留存，例如存款、转账、还信用卡、还贷、销户或购买理财等，每一次活动的时间、方式、地点、账户、金额、交易对象等等，都完整的保存在银行的数据库中。这些积累的数据，是银行非常宝贵的资产。与客户评级相关的数据，通常包括以下几个方面：", "1)客户基本数据：银行通过不同形式、不同时间、不同地点所记录的客户名称、证件编号、联系方式、营收情况、学历、就业情况、客户关联人信息等;", "2)贷款或信用卡账户信息：包括账户号码、余额、开销户时间、额度、额度调整历史等;", "3)交易历史：即贷款放款、还款计划及实际还款、现金提取、信用卡刷卡、还卡、换卡等各类事件的具体时间、地点、方式等详细记录;", "4)担保信息：即贷款抵押物基本信息、估值或评级信息，担保人信息等。", "除上述外，信用卡或贷款产品的营销活动等数据，也与客户评级有关。", "外部数据来源广泛，以人行征信数据为例，其包含了客户基本信息，如姓名、性别、证件编号、婚姻情况、联系方式、住址等等;借款人的信用历史，如逾期情况、贷款尚未结清信息、担保信息、异常交易信息等;还有一些个人非银行信息，如住房公积金信息、社保信息等。", "目前，各家银行都已经建立ODS或数据仓库等数据平台，其包含的信息能满足银行各条线的业务需要，为开展各类管理、经营决策的提供数据基础。然而，客户信用评级数据作为数据平台的一部分，通常混合于其他数据之中，因此，有必要仅针对信用风险管理或信用评级的需要，面向信用风险管理应用开发，单独建立信用风险数据集市。", "数据来源于各类生产、业务系统，经由数据仓库，进入信用风险数据集市中。风险数据集市则按照上层应用的需要，进行数据的整合和存储。一般来说，信用风险相关的数据经过拆分、拼装或重组，以主题的形式存储在信用风险数据集市中。通常，包含以下几个主题：", "数据挖掘是从大量的、有噪音的数据中，发现潜在的规律和价值，以辅助提高管理、决策能力。银行通过对外部数据及信贷等业务中产生的数据进行提炼、分析，开发模型，对客户进行信用评分，以服务于信贷管理，增强风险控制能力。", "银行积累的客户评级相关的数据量极其庞大，出于数据处理速度及模型开发效率的考虑，通常抽取一定量的数据作为样本，开发模型。常用的样本选择方式有两种，随机抽样和分类抽样。随机抽样较为交单，即随机选择样本，认为样本可以代表整体情况。例如，总贷款账户数是5000，不良贷款账户数是100，占比1/50;那么随机抽取100个贷款账户，其中包含2个不良贷款账户。而分类抽样，则需要先分类，确认各类样本的数据量，再分别进行随机抽样。例如上述例子中的账户样本选择，首先据担保情况进行分类，有无担保比例分别为3:2，则再分别随机抽取60个有担保的不良贷款账户和40个无担保的不良贷款记录。", "当然，以上仅为示例，实际情况却往往复杂很多。", "明确因变量和自变量。其中因变量为表现变量，即模型的结果“客户信用情况”;自变量为与之相关的因素，它的预测能力决定于它与因变量之间相关关系和逻辑因果关系。通常，与信用等级相关的因素包含客户的学历、工资、年龄、额度使用情况、现金提取次数、还款时间等。", "模型分组的意义在于区分不同行为模型和数理关系，以提高模型预测的精准度。例如，学生和在职人员的还款能力是有差异的，但是某类自变量和坏账率的表现上，趋势十分相似，所以讲模型分组，将避免相互之间的模型因素的干扰和影响。", "影响模型结果的变量非常复杂，因此需要根据单个变量的实际预测能力进行筛选，剔除没有预测能力的变量，以缩小变量的范围。", "常见的模型算法有线性回归分析、非线性回归分析、逻辑回归模型、神经网络模型、决策树模型等。在实际的模型选择过程中，需根据模型性质、分析人员经验等多方面因素综合考量。", "模型检验，在于衡量开发的信用评分模型能力。常用的检验报告有以下几类：", "其中，前三者表现的效果为：“评分越高，则好账户出现的越多”;而拟合度曲线，则用于对比预测情况与实际情况差异。", "信用评分对银行的经营效益有着重要的作用，信用评分模型应用效果，很大程度上也取决于银行的内部管理及信贷政策。技术和管理相结合，双管齐下，一定是控制客户信用风险的最优方案。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/75665"]},
{"module": ["干货教程"], "note": ["\n“2016中关村大数据日”信息通讯大数据分论坛上，中国电信北京研究院云计算与大数据产品线产品负责人钱兵从运营商大数据商业应用现状与展望方面进行了主题内容分享。介绍了中国移动、中国联通..."], "title": ["36页ppt | 三大运营商大数据发展与展望"], "content": ["“2016中关村大数据日”信息通讯大数据分论坛上，中国电信北京研究院云计算与大数据产品线产品负责人钱兵从运营商大数据商业应用现状与展望方面进行了主题内容分享。介绍了中国移动、中国联通、中国电信在大数据应用领域的典型应用代表，并分析了电信行业大数据发展的问题，对大数据未来发展进行了展望，从智能、平台、行业、融合、商业、生态的方向进行了深入的分析探讨。", " ", " ", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73415"]},
{"module": ["干货教程"], "note": ["\n文 | 陆遥\n产品的数据驱动，我相信有很多产品经理需要面对这件事情。因此，我希望能够分享出我的思考，未必都对，仅仅是我一路走来的心得体会。\n我们的世界，本质上由数学组成，无数数据构..."], "title": ["见微知著：怎么做产品的数据驱动"], "content": ["文 | 陆遥", "产品的数据驱动，我相信有很多产品经理需要面对这件事情。因此，我希望能够分享出我的思考，未必都对，仅仅是我一路走来的心得体会。", "我们的世界，本质上由数学组成，无数数据构建了我们庞大的赖以生存的环境，这就是我们与数据的不解之缘。", "这句话不知道在哪里看到过，用在这里感觉有种提纲挈领的感觉。", "为什么要专门写一篇文章来说数据驱动这件事情?原因是我在这件事情上有深刻的体会。", "产品在设计之初，所有人对它都抱有许多期待，希望它可以实现它作为一个好产品的价值，但是如何去定义这个价值呢?", "一开始，我们可以靠产品直觉，sense是依靠天赋的;可以靠老板明确提出的需求，这取决于你的老板是否英明神武且熟知产品;但在更多的时候，我们依靠的是实打实可见的数据反馈，它们将指导你的决策，推翻你的臆测，逼迫你去追寻数据背后的原因，这就是我所说的数据驱动。我相信有很多产品经理需要面对这件事情，因此我希望能够分享出我的思考，未必都对，仅仅是我一路走来的心得体会。", "刚做产品的时候懵懵懂懂，在网上各种地方寻找资料，企图获取一些灵感，能帮助产品走得更远，走得更好。爬过坑后才知道，做出成功的产品，不是一个随机事件(做人也一样)，企图跟风而没有丝毫见地的产品，会很肤浅，很站不住，这也是我非常推崇通过数据来驱动产品发展的原因。我也相信有很多人会有和我一样的疑惑，产品做完了，我们接着需要做什么?产品迭代。可怎么迭代?", "大家都说，产品经理就是CEO的提前班。我是认可这句话的，因为单单从数据驱动这个角度来描述，可能我们就得站在CEO的角度考虑更多问题。借用以前看过的一个挺有普适性的理论来解这件事情如何来规划吧，这个理论叫做SMART法则。", "S：strategy 策略", "M：model 模型", "A：analysis 分析", "R：result 结果", "T：transform 转变", "举个例子，刚拿到一个产品的设计需求，我们要做的第一件事情是定义产品价值，也是总纲，它常常会写在产品需求文档的第一页，在你需要向所有人一句话解释这个产品的时候，它以简洁明了的方式出现。而如何实现这个价值，就是我们所说的策略，这就像，你想要一个梨子，你可以去买，也可以自己种一颗梨树，也可以通过基因克隆，方式不一而足。产品价值是唯一的，而策略是不同的，一个数据驱动型的产品，往往需要在定义好产品后，完整思考对应的产品策略。", "假设我们希望通过种梨树来得到梨子，在这样的产品策略下，我们所知道的足以支撑这个目标的一切就可以通过市场调研来获取到。我们需要梨的种子、阳光、水、土壤和肥料，还需要在适当的阶段除虫，根与树叶用于支撑营养所需，树干负责传输，它们必须在一个非常健康的状态下才有可能生长出梨子，而这些状态量，就是所谓的模型。", "有一句话我非常喜欢，“在大数据世界中，小就是美”，我们必须明确哪些因素支撑产品达成目标，这样才能在精力和资源有限的情况下，完成数据驱动。当然了，数据模型往往很难在一开始就确定下来，这取决于经验，随着数据观察持续推进，我们就会发现现有的数据量无法支撑起分析所需，那就要求我们继续完善模型。", "假设模型已经日趋完善，足以支撑起决策所需的时候，我们就可以利用这些数据来推进一些事情。好几天没有浇水，导致梨树树叶有些枯萎，这就是联系，实验室里的单一变量法，在分析过程中也可以奏效并说明问题。", "我个人的信条之一是，绝不浪费时间做没有结果的事情，哪怕是不好的结果，也是一种产出和成长。而在开展数据工作的过程中，也肯定需要有一个结果，在预计的时间点，梨树本该开花，但是结果没有，在计划规定的时间内，产品本该达到一个状态，但是结果没有。我们通过结果来反思不足，也用结果来激励团队继续前进，我想这是一个很好的工作方式。", "其实前面已经提到，穷则变，变则通，世界的规律在动态变化。好的计划往往不足以应变一切，我们资深的后端开发同事告诉过我，“我们信奉计划主义，但我们的职责和能力是快速响应变化”，与君共勉。", "思想高度决定流程，活儿好不好看技巧", "了解我的朋友应该知道，我是非常务实的一个人，很多事情，做久了一定会有心得与技巧，而这就决定了你的专业度。只不过有人愿意分享，有人不愿意或者表达不出来，我会尽己所能毫无保留地告诉大家如何把数据驱动这事儿做好，当然离专业还有很大的差距，但应该会有一些帮助。", "前文提到的方法论，其实是我们在实践中的第一步。也就是根据我们各自的实际情况来制定我们各自的数据驱动流程，再思考我们应该监测哪些数据的时候?公司中的每个人都有自己的认知，可能会从各个角度给产品经理一些灵感，归纳总结后，我想大部分公司思考的问题方向也许可以从下图中获取，仅供参考。", "图1-SMART战略板", "大家其实可以看到，产品经理不仅仅需要从客户角度思考问题，而是需要从多维度来考虑问题，从这些问题中我们可以提取出对我们而言最重要的一些数据。", "从客户角度，我们需要考虑日活;", "从运营角度，我们需要考虑转化;", "从资源角度，我们需要考虑投入产出比;", "从竞争与风险角度，我们需要考虑容错率。", "我所说的这些，大家都听说过，而考虑这些问题的过程，其实就是在不断告诉自己这些数据为什么重要，为什么需要观察。这是第二步，即确认数据范围。", "接下来就是一些实践性的工作了，我们需要选择至少一个相对可靠的工具来帮助我们监测数据，建立数据模型。之前的工作中，我们选择的工具是友盟。一般情况下，这些数据足以支撑分析所用，但是也有局限性，因为缺乏与市场上的横向对比，所以如果在资源充裕的情况下，我还是建议向外部采购一些数据作为横向对照的数据补充，这样的话，我们就可以知道自己的梨树和其他人种的相比有何优劣。这就是第三步，建立数据模型。", "当我们做完这三件事情，剩下应该做的，就是不断通过数据的横向纵向对比，来发现自己的不足然后改进;或者发现我们的优势然后把它放大，这个时候我们就可以说，数据确确实实驱动了产品的进化。", "通过漏斗模型来观察支付流程的转化率，通过A/B测试来测试某个功能带来的影响，当这些效果，通过可视化的方式展现在开发、设计和老板面前的时候，我们得到的结论一定比“拍脑门”更有说服力。", "数据驱动尽管看起来是解决一定问题绝好的方法，但话不能说死，数据有时候也会欺骗你。这不是说数据本身有问题，而是解读的人有问题。在经验和认知所限的情况下，我也犯过许多认知错误。正如骑白马的不一定是王子，也可能是唐僧，诸如此类的想当然会干扰我们的判断，这就没有任何捷径可走，只能通过不断地刨根问底和逻辑学习来提升自己的决策准确性。多问多想多调研，这条路无止境。", "至此，我工作中关于数据驱动的思考，实践以及未来前进的目标，都已毫无保留的分享。", "纸上得来终觉浅，绝知此事要躬行。", "如何熟练掌握数据工具做不同的呈现?漏斗模型和A/B测试如何开展?如何将数据结果转化成需求?诸如此类的问题，我相信每个人都有自己最适合的方法来解决。重点是，数据驱动是一件有策略、有模型、有分析、有结论以及有转变的事情;我们知道怎么做的前提，应该是我们知道为什么这么做而非因为大家都这么做或者老板叫我这么做。独立思考的能力，也是这样渐渐培养起来的。", "数据非常迷人，它严谨，科学，合理，我认为是新一代产品经理必须掌握的技能。还有一个产品中经常被提起的名词，产品需求，下一篇文章我希望可以讲一讲我和需求相爱相杀的故事。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74227"]},
{"module": ["干货教程"], "note": ["\n无人机送货、阿法狗下棋、小冰和你谈场恋爱……人工智能领域的成果，一直是企业在大数据运用能力上的主要外在体现，但在亚马逊原首席科学家安德雷斯?韦思岸(Andreas Weigend)看来，大数据能为..."], "title": ["亚马逊前首席科学家:大数据价值体现在AI、BI、CI、DI"], "content": ["无人机送货、阿法狗下棋、小冰和你谈场恋爱……人工智能领域的成果，一直是企业在大数据运用能力上的主要外在体现，但在亚马逊原首席科学家安德雷斯?韦思岸(Andreas Weigend)看来，大数据能为企业做的，还远不止如此。", "以韦思岸之见，大数据对于企业的价值，更全面地来说可以体现在AI、BI、CI和DI，即人工智能(Artificial Intelligence)、商业智能(Business Intelligence)、客户智能(Customer Intelligence)和数据智能(Digital Intelligence)这四个方面。", "在《大数据和我们》(Data for the People)一书中文版发布之际，作者韦思岸接受了记者的采访。", "当前，极力开发人工智能的适用领域，成为了企业界的热门话题，甚至连企业CEO等职位可否被替代，都频频被纳入讨论之中。韦思岸对此的观点则是：“机器应该只负责完成比人类更擅长的任务”。", "人工智能的概念其实早已被提出，而上世纪70年代、90年代的两次“人工智能寒冬”之所以会出现，是因为彼时大数据发展尚不成熟，人工智能难以完成规模化的深度学习。等到近年来大数据行业取得突破性进展，人工智能领域才不断迎来突破。", "所以，人工智能的主要优势，在于记忆与运算大数据，并从大数据中发掘深层次信息。", "《大数据和我们》中有一部分内容就阐述了博世公司的典型案例。", "通过车载摄像头，博世的自动泊车系统可以记录并测量停车位的位置分布、空间大小、距离远近等大数据，并把它们上传至卫星定位系统，通过把海量大数据带入建模得出完成停车的行驶线路。而博世最近公布的、预计于2018年面市的Home Zone Park Assist服务，更是可以为汽车预存最多10条泊车路线，让人工智能在大数据的助力下为用户实现自动泊车。", "也正因如此，能获取可观数据量的公司，往往更可能在这一领域取得成果。韦思岸认为，包括阿里巴巴、腾讯等在内的一些中国公司的优势，正在于此。", "尤其是在近几年，每年有超过6.5亿人登录阿里巴巴旗下网站购物，而微信的用户也突破了8亿，这使得阿里巴巴、腾讯跻身全球拥有最多大数据的几家公司之列，他们也完成了在相应领域的突破。例如，阿里巴巴就在2016年8月的云栖大会上，推出了其首款人工智能机器人ET，而腾讯也于2016年12月26日宣布，将向全球企业提供包括图片标签、名片OCR识别等在内的7项AI云服务。", "与计算机的优势在于记忆、运算相对应，人的优势则在于创新能力与决策能力。", "所以，“数据驱动”这个近年来开始流行的管理理念，在韦思岸看来是有失偏颇的，而“驱动数据”，才应该是让大数据应用到企业决策、管理等领域的理性路径。两者的区别是，前者是以大数据统御人类的决策与判断，而后者是让大数据处于人的掌控之中。事实上，大数据本身是非常粗略的，数据样本有可能非结构化，甚至可能掺杂了欺诈数据。", "把大数据置于其擅长范围以内，用大数据来构建辅助企业决策的预知系统(anticipatory system)，才是其助力企业提升商业智能的更合理选择。韦思岸也以具备“大数据DNA”这一比喻来形容这类企业。", "韦思岸加入亚马逊也与此有关。韦思岸说，亚马逊创始人杰夫?贝佐斯(Jeff Bezos)早年就意识到大数据对于企业的重要性，因此邀请自己来一起为亚马逊制定大数据战略，让公司真正拥有大数据DNA。此后，亚马逊的商品推荐系统、第三方在线销售平台等创新性服务，不仅改善了顾客的购物体验，还让亚马逊自身能更科学地制定经营与市场战略。比如通过对大数据统计的总结，亚马逊就调整了自身的市场划分，把以往几十个细分市场的分类，演变为“将单个顾客一分为十”的全新分类。", "更重要的是，具备大数据DNA的企业并不会惟大数据是从，而是在决策时把它置于辅助性地位。亚马逊计算机中心时区设置的不统一，曾让一些商品从浏览到购买的时间为负值，这也让亚马逊开展了自我纠正，重新基于其全球化经营战略设置了计算机时区等内部系统架构。", "以韦思岸之见，亚马逊与其说是一家电商公司，不如说是一家科技公司。这也正是这家1995年才成立的公司能保持迅猛发展势头，并在2016年《财富》全球500强榜单中排名第44位的重要原因。", "当然，《大数据和我们》一书也向我们展示，大数据DNA并不一定是科技公司的专利。在书中，全球规模最大基金公司桥水基金就于近期宣布，公司未来的员工招聘、绩效评定、日常管理等领域，将引入大数据辅助决策，桥水基金创始人雷伊?达里奥(Ray Dalio)也表示，相信这样能“有助于提高认识、改进决策过程并实现更好的成果”。", "“我们应当让人去做人擅长的事，让计算机去做计算机擅长的事”，正如韦思岸所言，人与计算机的深度协同，才是企业具备大数据DNA、能以商业智能取得领先的象征。", "既然大数据是为人们决策提供辅助信息，那么，哪些信息相对不重要，哪些信息则相对更重要?“我喜欢听到用户们通过大数据来发声”，韦思岸把来源于经营第一线的用户数据，看作是更重要的大数据。", "《大数据和我们》记录了韦思岸于20世纪90年代初在施乐公司的帕克研究中心工作时的情形。虽然当时已经诞生了超级计算机，但囿于数据量的缺乏，这位斯坦福大学博士仍只能通过大量假设来进行研究。", "而现在，通过来源于客户的大数据，企业就能倾听到客户真实的心声，并以源自于客户方面的客户智能作为自身在行业内保持领先的基础。在韦思岸看来，其曾经提供过咨询服务的多家企业，如阿里巴巴、汉莎航空、摩根大通、GE等，都已非常注重客户数据对企业发展的意义。比如据韦思岸回忆，在与自己共同出席一次会议时，马云就曾表示“call me Data Ma”。", "能真切听到客户通过大数据发出的心声，这只是第一步。客户数据能否转化为客户智能，关键还在于，企业经营决策的制定能否真正以客户需求为中心。", "事实上，这也是体现企业领导者能否真正在企业内贯彻大数据DNA的一个分水岭。", "例如，在当今的移动互联网时代，用户的一大诉求是希望企业能注重保护自己的数据隐私。但各类大规模用户资料泄露事件就表明，这一客户心声并非业内许多公司的真正关注点。苹果公司则是极少数的特例之一。2016年2月，蒂姆?库克(Tim Cook)能拒绝FBI关于在苹果设备预留后门的要求，让苹果注重客户隐私保护的声誉进一步提升，而一部分用户是非常愿意为此支付相应价值的。", "此外，如果企业目前暂无能力获取海量客户数据，那么，通过数据服务商或中间商来查阅大数据，也是企业提升自身客户智能的合理选择。", "比如，包括宝马、福特、MapQuest等在内的企业已经与微软旗下的Inrix开展了合作，后者是日均可分析超过1亿部智能手机地理位置数据的公司。再比如，餐饮、院线等行业内的企业可以通过Yelp、大众点评等网站获悉各类客户数据，以改善企业的服务与产品线。", "如果一家企业能够在行业变革趋势洞察，企业内部决策制定，以及企业外部客户分析上充分运用大数据，那么，这就是企业在数据智能方面的体现。对此，韦思岸用了“ABC客户行为模式”(ABC of Consumers’ Behavior)来予以概括。", "A代表认可(Approval)，表明企业在获取和运用大数据助力企业发展时，需尽量获得用户的肯定。", "亚马逊在建立用户点击与购买量的数据库时，并不保存每位客户的身份信息，而是只关注客户从一件产品到另一件产品的跳转轨迹。韦思岸认为这是亚马逊能获得不少客户认可的重要因素。", "B代表归属感(Belongingness)，这说明，具备数据智能竞争力的企业，都善于提升其用户黏性。《大数据和我们》中就列举了南非保险商探索健康公司的例子。探索健康公司与超市、商店共同推出的“活力”促销计划，为客户提供金钱奖励。若客户的支付记录显示他们购买了健康食物，他们就能凭此获得返现或保费折扣。", "C则代表多重含义，比如对话(conversation)，比如交流(Communication)，又比如社区(Community)。", "韦思岸也对界面新闻记者表示，他在与好友、2002年诺贝尔经济学奖得主丹尼尔?卡内曼(Daniel Kahneman)探讨后，两人都认为C最重要的含义是连接(Connection)。在行将来临的由人工智能、云计算、物联网主导的时代，平台化、生态化必然将成为企业的转型方向。由此，企业与平台合作各方的连接，企业与产业链各端的连接，都将深刻且持续地改变企业的发展轨迹。显然，这一任务的最好承担者正是大数据。", "能够像Facebook、Uber一样以大数据连接企业内外各方的企业，才是数据智能的真正代表。“那些敞开大门，让数据和信息畅通无阻连接的企业，将赢得更长远的未来”，韦思岸说。", "韦思岸曾在联合国的一次演讲中，把大数据比喻为新时代的石油，他认为其是21世纪最重要的原材料。韦思岸对界面新闻记者说，在未来，企业运用大数据优化战略制定与日常管理，会是一件如同企业要用电一样常态化、普遍化的事情。", "大数据能为企业带来的AI、BI、CI和DI之增长动能，将是身处万物互联化、万物智能化时代的企业所必须具备的DNA。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74092"]},
{"module": ["干货教程"], "note": ["\n本文是2016年中国（上海）大数据产业创新峰会的演讲内容。\n引言：\n对于移动运营商来说，作为数据管道，本身拥有众多的数据资源，具有天然的优势，更应该利用大数据挖掘，为移动运营带来更多..."], "title": ["背靠优质数据源 中国电信大数据应用实践（PPT）"], "content": ["本文是2016年中国（上海）大数据产业创新峰会的演讲内容。", "引言：", "对于移动运营商来说，作为数据管道，本身拥有众多的数据资源，具有天然的优势，更应该利用大数据挖掘，为移动运营带来更多的竞争优势。大数据可以助力移动运营为用户提供更好的体验；降低运营成本提升运营效率；带来新的增长点以及更多的业务机会。", "在大数据生态建设上，中国电信保持了身段柔软、低姿态的姿势。提升内部效率是运营商大数据项目的基础。当前运营商自身的业务和服务面临新的挑战，大数据在改善内部语音通信和数据增值方面可以发挥作用，比如精准营销、客户维系、业务创新等。在对外变现上，零售领域的基于商圈人流特征、消费特征和客户标签的精准营销；改善交通效率；提高政府社会治理水平都可以被列为大数据的盈利生态圈。", "1、电信用户基本数据", "号码", "\n身份信息", "\n缴费信息", "\n消费信息", "2、移动位置数据", "基站信息", "\n移动信息", "\n滞留信息", "3、信令数据", "主叫信息", "\n被叫信息", "\nIP信息", "\n位置信息", "4、网络日志数据", "网源地址", "\n目的地IP", "\n目的地URL", "\n……", "5、终端数据", "Mac地址", "\n终端ESN", "\nAPP信息", "\n……", "\n", "\n5、用户触点数据", "10000", "\n网厅", "\n……", "下面是详细的PPT内容", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "End.", " ", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73566"]},
{"module": ["干货教程"], "note": ["\n作者：小数 GrowingIO\n随着精益化运营的概念不断深入人心，数据分析已经成为了互联网人的必修课。相比于高深的概率统计、算法模型，简单、直观的图表工具得到了更为广泛的应用。那么图表都..."], "title": ["从入门到精通：如何用图表做好数据分析?"], "content": ["作者：小数 GrowingIO", "随着精益化运营的概念不断深入人心，数据分析已经成为了互联网人的必修课。相比于高深的概率统计、算法模型，简单、直观的图表工具得到了更为广泛的应用。那么图表都有哪些类型?不同类型的图表又该怎么用?来，让我们深入浅出，看图说话。", "初阶的图表简单易懂，能满足简单的数据分析需求，具体包括趋势、频数、比重、表格等类型。图表数据分析的前提就是将自己需要呈现的指标，以一定的维度拆分，在坐标系中以可视化的方式呈现出来。", "趋势分析是最基础的图表分析，包括线图、柱状图、堆积图等多种形式。", "线图可以观察一个或者多个数据指标连续变化的趋势，也可以根据需要与之前的周期进行同比分析。柱状图可以观察某一事件的变化趋势;如果将整体拆分可以做成堆积图，同时观察到部分所占比重及变化趋势。", "周期对比线图", "(堆积)柱状图", "产品经理和运营人员通过趋势图分析流量的实时走向，如每日PV、UV、DAU等基本数量指标以及停留时长、平均访问页面数等质量指标，可以及时把握产品的变化趋势。一旦趋势周期对比发生异常(异常高和异常低)，我们需要及时介入排查原因、解决问题。", "根据业务需求对指标按照一定维度拆分，对比不同组别的频数，便于分清轻重缓急。", "条形图", "条形图清晰展示了用户在不同类别上的频数，并且按照数量从大到小排序。上图展示的是某产品用户使用浏览器的频数分布，在资源有限的情况下产品可以先适配Chrome和IE浏览器以提升绝大部分用户体验。", "双向条形图", "上面的双向条形图展示了某B端产品的客户平均停留时长极端情况(非常高和非常低)，企业1-5非常活跃，可以让运营人员促进客户增购、续约，而企业6-10活跃度非常低，即将流失，需要运营人员立刻介入干预。", "比重分析主要是用来了解不同部分占总体的比例。横向比较，扇形图、环形图可以满足这类需求;纵向比较，百分比堆积图可以显示不同部分所占比例的趋势变化。", "访问用户来源环形图", "上方的「环形图」显示了某节点访问用户来源渠道比例，而下方的「百分比堆积图」则动态显示了不同渠道比例的变化趋势，市场或者运营人员可以据此动态优化我们的资源投放。", "百分比堆积图", "表格信息密集，可以同时分析多维度、多指标数据，适合对数据敏感的人群使用。虽然表格能看到具体的数值，但是不能直观看到趋势、比重。", "表格提供三十多个维度供指标拆解", "通过表格不难发现，移动端访问用户占了非常大的比例，但是跳出率非常高。这样的表格数据启示我们有必要优化移动端产品，提升整体访问深度。", "下面介绍的是气泡图，气泡图用来展示一个事件与多个维度之间的关系，如分析B端产品客户成单周期与客户活跃度、登录账号数量之间的关系。", "客户温度-健康度”气泡图", "除了上述常见的图表，还有散点图、箱线图、股价图、雷达图等图表，在此不一一赘述。", "正如前面所言，初阶图表能满足简单的业务需求。但要想深入洞察用户行为，还需要紧密结合业务实践，用更加专业的图表辅助数据分析。在这里，我和大家分享三个实用的工具：漏斗图、留存图和热(力)图。", "漏斗图主要用于转化过程，例如注册流程、商品购买流程，分析用户在不同阶段的转化或者流失情况。", "漏斗图", "产品运营应该关注重点转化路径的转化率，对于转化率非常低的环节、或者转化率突然下降的情况，都需要及时排查原因。", "留存是指用户首次访问你的网站，多少天后又重新回访的情况。利用留存曲线可以对留存进行深入分析。", "留存曲线", "某问答社区通过留存曲线发现，EDM获取的新用户(红色)留存度和活跃度远远高于一般新用户(绿色)，这启示社区运营者：EDM可能成为社区的下一个增长点。", "热图，又称热力图，显示的是用户在你产品页面上的点击、停留偏好。借助热图产品经理可以优化产品页面布局，运营可以优化内容，确实是一个好工具。", "热图", "随着数据可视化技术的不断发展，图表的类型越来越丰富，我们不可能在一篇文章中将其穷尽。但是图表数据分析的本质不会变，其最终目还是要辅助人们的决策。", "人们的工作在不断细分，需要分析和决策的内容也不太一样。同样都是市场部门的同事，负责内容营销的与负责SEM的需要关注的数据差异很大，而这就需要搭建属于自己的数据看板。", "数据看板", "例如SEM主管根据工作需要搭建数据看板，将广告投放(表格)、访客来源(百分比堆积图)、访问用户量(线图)、登录用户量(柱状图)和注册转化率(漏斗)等重要数据集中在一个看板中。数据看板能帮助我们以合适的方式展示数据，集中精力做好业务决策。", "用图表做好数据分析并非易事，它绝非一朝一日之功，但也并不是无规律可循。", "在实践中用数据驱动增长", "首先是对业务的理解，能洞察数字背后的商业意义。其次是灵活选择维度拆分指标，在图表坐标系中以合适的形式进行可视化展示。最后一定要从图表数据分析中发现问题，并指导业务决策。在这样不断反复的过程中，不断优化我们的图表数据分析过程，用数据来驱动业务增长。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/66985"]},
{"module": ["干货教程"], "note": ["\n文 | 吴邢一夫\n本文阐述整个推荐体系从0搭建的全流程，也是最近以来一直深入研究的成果展现，因原文太长，故此切分成3部分发送，每天发送1篇，全文结构为：上篇：第零章概述，第一章标签体..."], "title": ["从零搭建推荐体系： 推荐体系和评估体系(下)"], "content": ["文 | 吴邢一夫", "本文阐述整个推荐体系从0搭建的全流程，也是最近以来一直深入研究的成果展现，因原文太长，故此切分成3部分发送，每天发送1篇，全文结构为：上篇：第零章概述，第一章标签体系搭建;中篇：第三章用户体系，第四章项目体系，下篇：第五章推荐体系，第六章评估体系，第七章全文总结，第八章参考资料。", "很明显，推荐方法和推荐算法是整个推荐系统中最核心、最关键的部分，很大程度上决定了推荐系统性能的优劣。目前，主要的推荐方法包括：基于内容推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐和组合推荐。详细的方法介绍也放置在附录4当中以供参考，下面梳理出各方法的优劣：", "在除去场景外，也要结合性能因素来进行考量，不同数据量级的情况下不同方法配合不同算法产生的性能压力也是不同的，需要结合公司自身承受情况进行选择。可以看出，以上方法均有不同程度的优势和劣势，所以目前主流推荐方法也几乎均采用混合推荐的方法，利用两种或多种方法之间的优势，规避劣势从而达成尽量完美的方法，这其中也一定是基于不同的使用场景和产品具体情况具体分析了。", "这是整个推荐系统的核心区域，之前做的许多的工作其实都是在给推荐算法提供所谓的相关系数条件，当系数越多的时候，计算出的结果一定是更准确的。", "从数学角度来说是计算用户与内容之间的相似度和距离，相似度越高，距离越近的，自然越容易达成转化，所以常见算法也就是向量里面的夹角余弦算法、皮尔逊系数，从距离来说会有欧几里得空间距离算法、曼哈顿距离算法等等，包括还有许多新进研究的算法例如基于图摘要和内容相似混合聚类的推荐算法GCCR。", "简单介绍下GCCR，该算法可以极端稀疏的数据集上具有较高的准确度，同时在冷启动的场景下能够提供多样性的推荐结果，从而避免推荐结果收敛过快的问题。", "首先，选取用户节点中关注数量较高的节点，从而抽取出稀疏数据中的一个密集子集，利用图摘要的方法，对此密集子集形成关注兴趣相似的核心聚类。", "然后，提取种子聚类的内容特征和整个数据集中其它用户的内容特征，基于内容相似度对整个用户群进行聚类，最后将聚类结果用于主题推荐。通过对密集数据子集和全数据集的两阶段聚类过程，提高对极端稀疏数据集的聚类效果。同时，由于图摘要聚类中的类模糊性，可以在对用户兴趣聚类的过程中保留一定的多样性，从而避免冷启动时收敛过快。", "所以当前算法非常的多，结合不同场景和产品选择最优算法，才是最好的，在附录5当中也列举了常规的一些距离算法以供参考。", "当运用于实际情况时，一定要结合产品自身情况考量，例如产品冷启动期间数据过少，用哪种方法，在数据量级充分上来的时候，减轻计算压力应该用哪种，长期需要修正的时候需要用哪种，都是需要我们综合考量的，下面也将自己梳理的整体推荐思路进行分享。", "在产品上线初期，无论使用人数，还是内容，都相对较少，还未有足够数据支撑用户相关行为以及趋势，所以在此阶段，以收集用户行为、属性为最高目的，先达成最粗略的推荐行为，也就是判断哪些用户是疑似某一细化方向的目标用户，仅此即可。应该分为两个方向来考虑这个问题，新用户和老用户，对于新用户只能从环境熟悉和可能的物理属性进行判断，老用户可以全方位多维度判断，详见第二章，这里不多做叙述。", "所以在当前阶段，主要目标就是收集用户行为，一切行为均不能遗漏，这也就是前文所说的，先围绕每个人建立一套粗略喜好标签模型，此阶段希望的是实时调整，根据用户使用频次和动作来决定，一定要快，因为刚刚上线，用户随时有可能离开。在用户随手点击内容以后回到首页的时候发现已经有较为感兴趣的内容了，那种好感度是不一样的。", "在这个阶段，已然有之前的用户行为的基础数据作为支持了，所以我们首先要做的就是将用户分组，将有相似喜好的用户找到，方法就是用最经典的向量算法里的夹角余弦，每个用户直接都要分别计算，不过好再现阶段用户量级不多，可以大量计算。计算依据也就是根据之前用户的相关操作行为，给用户打上的相关标签，按照标签相似度来给用户进行聚类。", "所以在聚类完成后，一定会获得离别内某种同样的特征值，所以这也就完成了第二阶段的工作，每个类别内的用户进行相同的内容展示。", "而且我们已经知道了喜爱不同项目之间的用户特征属性，这时候再进来的用户，我们也就可以相应的放在疑似库里了，等到收集到相应的新用户行为，也就能确定这个新用户的相关喜好方向了，成本会减小很多。所以在这个阶段，要尽量收集全，时间可以控制在2周左右，为下一步更加精准的推荐做准备。", "从这里开始，伴随着的一定是大批量计算。所以这里我们就是通过每个人的行为，猜测每个人的未知喜好，进行最大化的推荐匹配，我们需要设定动作权重系数，例如：有效打开=5, 分享=4, 收藏=3, 互动=2 , 其它跳转=1, 无效打开=-2 , 点×=-5。", "设定完毕后，我们可以看到当用户A、B、C在第二步时，在同一类组，看到的内容均相同，但是行为可能完全不同。我们就能得到以下的近似值，设任意三篇内容为x、y、z轴，那么对于用户A来说，那就是(3,-1,-1)，B君是(5,1,-5)，C君是(-5,3,3)。用夹角余弦=向量点积/ (向量长度的叉积)，所以 A君B君夹角的余弦是0.81，A君C君夹角的余弦是-0.97。", "根据余弦定则，等于1是0°，就是完全重合，-1是180°完全相反，所以越靠近1两者越相似，所以AB相似，所以看AB用户之间的差异，比如A看了x的新闻，B只看了y的新闻，就可以进行交叉推荐了。", "所以当内容(x、y、z)越多时，计算越准确，直接的结果那就是由于数据量的不断增大以及用户量级的不断增大，每一次计算也是不现实的，用户也多，次数就多，内容也多，点就多，所以考虑到这些，我们在第四步达成最终的目的。", "我们通过之前的数据积累，大量收集用户之前的相关行为，在这里要通过协同过滤矩阵及拆解来解决问题，矩阵拆解的核心其实是为了得到潜在因子，所以我们需要怎么做。", "将内容与用户的行为结合，变为这一张表格，能相应减少许多计算量同时达到不错的效果，如下表：", "设定有效打开=5, 分享=4, 收藏=3, 互动=2 , 其它跳转=1, 无效打开=-2 , 点×=-5。", "将表格利用协同过滤中的矩阵拆解进行计算，我们就可以得到如下两张表：", "这两个矩阵相乘就可以得到估计的得分矩阵：", "将用户已经看过的内容剔除后，选择分数最高内容的推荐给用户即可(红体字)。所以在这里，我们其实已经就完全可以精准化的推荐了，结果与计算量达到了相应的平衡。", "已经到这一步了，这时候每个人的展示信息已然不尽相同，所以要结合人的之前打开行为，一定要结合时间、场景、内容载体考虑之后的推荐，并且适当加入长尾内容，用户感兴趣的大方向，但是细化方向没有临幸过的。", "抽样技术在数据挖掘中主要用在两个地方：一是在数据预处理和后处理阶段，为了避免计算规模过大;二是在数据挖掘阶段，通常会对训练出来的模型进行交叉验证，需要抽样将所有样本划分为训练集和测试集。", "通常所说的抽样都是随机抽样，主要用于所有样本点都可以认为没有区分时适用。还有一种分层抽样，在样本需要显著的分为不同的子集时，针对每个子集分别进行抽样。", "当样本的维度增加的时候，待学习的模型的复杂性是随着维度呈指数增长的，这种现象通常称为“维灾难”。这也就意味着，如果我们想在高维空间中学到和在低维空间中精度一样高的模型，所需要的样本数是呈指数增长的。", "维度规约通常是用来处理维灾难问题的。通常维度规约有两种思路，一是从高维数据中选出最能表达数据的一些维度，并用这些维度来代表数据，称为特征选择;另一种是将高维数据通过某种技巧变换映射到低维空间，称为特征构造。", "主成分分析是最主要的一种特征选择方式，它通过特征分解能够得到每一个维度对于整个数据的最小均方差的贡献程度，从而定量判断每一维对于数据所包含信息的贡献度。然后保留最主要的一些维度，抛弃一些不显著的维度，对数据进行降维。", "奇异值分解是主要的特征构造方式，它通过矩阵分解的方式，将数据从高维空间映射到低维空间，对数据进行降维。", "展示阶段，我们应该以用户为唯一维度来进行思考，从第三章也能看出分为外因和内因，通过外因确定大方向，通过本次的操作行为确定内因，两者结合，时时修正，达到贴合用户的目的。所以并不一定是用户历史行为某类型内容打开较多、较高就要通通放在前面，在自身使用时候还有所谓的厌倦，与惊喜内容相结合，并且是先弱后强还是先强后弱还是两头强中间弱，都是我们应该通过用户时时改变的，强弱为通过历史行为分析出的内容对用户吸引力的量词。", "当前所有的做法都是在不惜代价的尽量精准的完成推送，转化最大化，可是这样真的好吗?不禁从另一个角度思考，如果用户收到的都是符合喜好的内容，就是最好的选择了吗?其实不然，这则很有可能陷入更危险境地——信息茧房。", "美国学者凯斯·R·桑斯坦指出信息茧房以“个人日报”的形式呈现：“伴随着网络技术的发达、信息的剧增，人们可以随意选择想关注的话题，可依据喜好定制报纸、杂志，每个人都可为自己量身打造一份“个人日报”。当个人被禁锢在自我建构的信息脉络中，生活必然变得程序化、定式化，信息茧房就像是“回音室”，人们设定了话题、观点，听到的是自己的回音，每个人将都闭塞在自己的空间之内。”", "在面对个性化新闻推荐如何走出“信息茧房”，遏制其产生的“回音壁”倾向，桑斯坦提出应该构建网络的“人行道”模式。他认为无论使物理空间的街道、公园、还是报纸、广播、电视等大众媒体，都属于公共领域，都应该像“人行道”一样，可能会遇到种种非计划和不想要的情景，不同的人群会体认到新鲜体验;那些未经实现筛选而遭遇的情状，会引发人们的言行互动。", "按照桑斯坦的理论，媒体应该添加“公共论坛”，通过提供用户“非计划”和“不想要”的信息，让他们有机会接触到不同领域的信息。当前新闻类客户端常规做法一般都是添加“热点”类的频道栏目，并且在每个不同的频道下的信息流中用带“热”字的红色小标提示热点信息以示区别。让用户接触到不同阶层的观点，不同类别的信息。", "但是在个性化新闻推荐方面，应该弱化用户相关性推荐的原则，拓宽用户的关注领域。个性化推荐除了基于用户自身的基本信息，还有就是基于协同过滤的，是根据社交关系中的好友的兴趣进行关联推荐。这种推荐原则会使用户聚集一批与自身在兴趣、文化等方面相类似的朋友，关注对象的同质化会让用户接收的信息也趋向于同质化。", "所以如何根据当前的兴趣建立模型，分析依据当前兴趣推测之后的兴趣偏移点，甚至偏移速度，在适时的时候完成长尾化内容推荐，可能才是未来推荐系统真正的价值。", "随着算法出炉之后，是需要经过长期的修正以及实时调整的，在这里面的样例，依然将用户分了组处理。我相信像头条、网易新闻这种体量的APP来说，应该是以每个人就是一个维度，每个人为单位计算和文章内容的相似度，才是终极目的。因为人有自己的用户标签体系，内容同样也有，先确定能影响用户权重的最大标签做粗略筛选，之后精细化每篇内容和每个人进行向量计算逐一得到结果，逐一进行推荐。", "在每天够后一定要有相应的review，评估之前策略的效果，结合浏览时间、打开比例、互动反馈、卸载情况等综合考虑策略的优劣，快速调整，下面就要相应介绍评估体系的建立。", "我们应该获得的是根据用户对推荐的显性或隐性反馈改进、优化原有用户模型，以确保模型能够匹配用户的最新偏好，从而提高模型精度和推荐质量。", "精确度的衡量最典型的算法是平均绝对误差(MAE)、平均平方误差(MSE)以及标准平均误差(NMSE)。平均绝对误差是所有单个观测值与算术平均值的偏差的绝对值的平均，用来衡量一组数自身的离散程度。有两个优点，第一是计算方法简单，易于理解，第二是每个系统的平均绝对误差唯一，从而能够区分两个系统平均绝对误差的差异，能更好地反映预测值误差的实际情况。", "在有些系统中，用户只在意推荐列表前端的预测误差，而对系统的整体误差并不是很在意，这时也不适合采用预测准确度进行评估，后两种更适合用分类准确度度量系统的推荐质量，来衡量观测值同真值之间的偏差，说明样本的离散程度，可作为衡量测量精度的一种数值指标。", "集合准确率、召回率和精确率也可以用来衡量推荐的准确度，准确率表示用户对一个被推荐内容感兴趣的可能性，召回率定义为推荐列表中用户喜欢的内容与系统中用户喜欢的所有内容的比率，精确率定义为推荐列表中用户浏览的内容与全部推荐内容的比率。", "其中还有像多样性(平均海明距离)、分类准确度(ROC曲线)、排序准确度(平均排序分)、以及半衰期(半衰参数)的因素，之前也描述过用户的兴趣还好是在随着时间的不断推移而增强或减弱的。除此之外与用户直接相关的比如惊喜度、意外程度、覆盖率、新鲜感、用户满意度等指标仍可说明。", "这些评估的获得手段一般从用户反馈、数据量化、长期观察和体验数据来获得，最终均要转化为量化指标来进行评估，例如使用时长、使用深度、打开比例等等，之后再进行拆解，某一指标与哪些行为相关，进行相关的行为追查，直接定位。", "从而达到评估体系的核心意义，到底是好还是不好，如果不好问题在哪儿，定位后进行修正。这部分产品同学只要大概了解有什么方法，即可，具体的原理有算法团队来做，要都搞清楚太难了，已经涉及太多的数学内容，有兴趣的课余时间自己学习即可。", "目前采用的用户模型更新技术主要可以分成三类：一类是从用户反馈中抽取新信息添加到用户模型中的信息增补技术;一类是根据生态系统的优胜劣汰法则来优化用户模型的自然进化技术;还有一类是通过调整网络连接权重来自适应更新的神经网络技术。", "这是目前为止使用最多的一类用户模型更新技术。它又包括了直接的信息增补以及涉及权重调整的信息增补两种类型。前者将获取的用户对推荐的反馈信息直接添加到用户模型中，典型系统如GroupLens、Ringo、Video　Recommender、PC Findert、WEBSELL等。这种更新只是简单地添加了新信息，并没有删除或削减无效旧信息在用户模型中的作用，因此很可能导致推荐阶段假阳性错误的出现，即将用户不喜欢的项目推荐给了用户。", "而且随时间的推移，模型规模不断扩大，这带来了存储空间占用和模型维护问题。", "相比之下，采用后者作为更新技术的系统，如LetiziatL、Personal Web Watcher、Webmate、Krakatoa Chronicle和WebCobral ，不仅将用户新的反馈信息(如新关键词)增加到用户模型中，而且还会调整用户模型中新、旧信息的权重，使反映用户最新偏好的新信息在推荐中起到更为重要的作用，同时无效的旧信息将随其权重的不断减小而最终从模型中被删除。", "因此，含权重调整的信息增补技术从一定程度上缓解了直接信息填补存在的问题，但这类技术的性能很容易受到新信息选择方法和被增加的新信息数量的影响。", "遗传算法是一种基于自然选择和遗传机理的迭代搜索优化技术，由适应度函数、染色体种群以及选择、交叉和变异三个主要操作算子组成。每一代种群包含了若干个个体(被称为染色体)。依据每个个体的适应度函数值，种群经过选择，交叉和变异操作一代代向更优良、更适应环境的方向进化，从而逐渐逼近最优解。使用遗传算法作为模型更新技术的系统。", "通常将用户模型编码成一个染色体并随机产生其他染色体作为初始种群。当初始种群进化迭代到满足终止条件时，解码适应度最高的染色体来取代系统目前的刚户模型即可实现更新。还有一部分系统也使用遗传算法作为模型的更新机制，但这部分系统通过对信息收集代理而不是用户模型本身的优胜劣汰来间接实现模型更新。", "当用户兴趣发生变化时，通过遗传进化，低性能的旧代理被消除，能直接满足用户新必趣或对用户及其他代理有用的代理得到繁衍。", "基于遗传算法的更新技术从多个初始点(群体)而不是单点出发持续搜索可能的用户模型构成空间，因此是一种高效且能够最优化用户模型的更新算法。但是基于遗传算法的更新技术，其适应度函数和染色体编码方法要求针对每个问题专门设计。", "神经网络是一种自适应的更新技术。当用户偏好随时问发生变化时，神经网络将自适应地调整网络连接权重，更新网络输出的识别结果来跟踪这种变化。有的更新只在原有类别的基础上对新旧偏好信息进行分类调整，有的则建立新的识别类，并剪除代表过时知识的识别类来对应于用户新兴趣的兴起和旧兴趣的衰亡。", "在这种情况下，神经网络的网络结构发生了变化，网络可能需要被重新训练来识别和记忆变化后的用户偏好。由于神经网络的更新依赖于前期神经网络的学习，因此通常只有以神经网络作为学习技术的系统才会用其作为更新技术。", "与模型学习技术相比，用户模型更新技术更关牲just-in-time型的学习而不是模型的建立和执行，因此算法需要具有更强的学习效率和对动态变化的适应能力。但是也有相应问题，目前的模型更新技术通常只按照固定频率对模型进行更新，这使得系统无法及时跟踪和捕捉用户兴趣的变化，从而造成了推荐结果和用户实际兴趣的差异。", "本节阐述评估体系的建立，列举相关维度和相关动作等，并且列举相关的修正手段和简单介绍，以供在长期不断修正模型和算法，达到更好的效果，更全面优质的服务用户。", "整篇文章阐述了搭建推荐体系的全流程，从思路表述、标签体系、用户体系、项目体系、推荐环节和评估体系的建立和相关细节及当前的模式，技术手段等。尽量完整和周密的阐述了全部流程以供梳理和参考，在正文后还有附录部分，阐述详细的算法和摘录的相关方法以共参考。", "凯文凯利在《失控》一书中提出“共同进化”的观点：", "在当前，即使使用推荐系统，也并没有根本上解决人们如何有效获得信息的难题。所以仍可以尝试探索更加智能的信息获取模式，以及更加自然的人机交互接口。", "犹如“大白”一样的贴心，处处想在用户前面，在合适的时候将想看的信息展示出来，随情绪而变，毕竟人类心理活动是十分微妙的，尽量通过规律摸清人类的喜好迁移，洞察微妙的行为变化，是未来努力的方向。", "同系列之  ", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73340"]},
{"module": ["干货教程"], "note": ["\n文 | 朱江\n传统应用使用本地事务和分布式事务保证数据一致性，但是在微服务架构中数据都是服务私有的，需要通过服务提供的API来访问，所以分布式事务不再适用微服务架构。那么微服务架构又..."], "title": ["为什么说传统分布式事务不再适用于微服务架构?"], "content": ["文 | 朱江", "传统应用使用本地事务和分布式事务保证数据一致性，但是在微服务架构中数据都是服务私有的，需要通过服务提供的API来访问，所以分布式事务不再适用微服务架构。那么微服务架构又该如何保证数据一致性呢?本文就来谈谈这个话题。", "我们先来看下第一部分，传统使用本地事务和分布式事务保证一致性。", "传统单机应用一般都会使用一个关系型数据库，好处是应用可以使用ACID。为保证一致性我们只需要：开始一个事务，改变(插入，删除，更新)很多行，然后提交事务(如果有异常时回滚事务)。更进一步，借助开发平台中的数据访问技术和框架(如 Spring)，我们需要做的事情更少，只需要关注数据本身的改变。", "随着组织规模不断扩大，业务量不断增长，单机应用和数据库已经不足以支持庞大的业务量和数据量，这个时候需要对应用和数据库进行拆分，这就出现了一个应用需要同时访问两个或两个以上的数据库情况。开始我们用分布式事务来保证一致性，也就是我们常说的两阶段提交协议(2PC)。", "本地事务和分布式事务现在已经非常成熟，相关介绍很丰富，此处不再讨论。我们下面来谈谈为什么分布式事务不适用于微服务架构。", "首先，对于微服务架构来说，数据访问变得更加复杂，这是因为数据都是微服务私有的，唯一可访问的方式就是通过 API。这种打包数据访问方式使得微服务之间松耦合，并且彼此之间独立，更容易进行性能扩展。", "其次，不同的微服务经常使用不同的数据库。应用会产生各种不同类型的数据，关系型数据库并不一定是最佳选择。", "例如，某个产生和查询字符串的应用采用 Elasticsearch 的字符搜索引擎;某个产生社交图片数据的应用可以采用图数据库，例如Neo4j。", "基于微服务的应用一般都使用 SQL 和 NoSQL 结合的模式。但是这些非关系型数据大多数并不支持 2PC。", "可见在微服务架构中已经不能选择分布式事务了。", "依据 CAP 理论，必须在可用性(availability)和一致性(consistency)之间做出选择。如果选择提供一致性需要付出在满足一致性之前阻塞其他并发访问的代价。这可能持续一个不确定的时间，尤其是在系统已经表现出高延迟时或者网络故障导致失去连接时。", "依据目前的成功经验，可用性一般是更好的选择，但是在服务和数据库之间维护数据一致性是非常根本的需求，微服务架构中应选择满足最终一致性。", "最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。", "当然选择了最终一致性，就要保证到最终的这段时间要在用户可接受的范围之内。那么我们怎么实现最终一致性呢?", "从一致性的本质来看，是要保证在一个业务逻辑中包含的服务要么都成功，要么都失败。那我们怎么选择方向呢?保证成功还是保证失败呢?", "我们说业务模式决定了我们的选择。实现最终一致性有三种模式：可靠事件模式、业务补偿模式、TCC 模式。", "可靠事件模式属于事件驱动架构，当某件重要事情发生时，例如更新一个业务实体，微服务会向消息代理发布一个事件。消息代理会向订阅事件的微服务推送事件，当订阅这些事件的微服务接收此事件时，就可以完成自己的业务，也可能会引发更多的事件发布。", "1. 如订单服务创建一个待支付的订单，发布一个“创建订单”的事件。", "2. 支付服务消费“创建订单”事件，支付完成后发布一个“支付完成”事件。", "3. 订单服务消费“支付完成”事件，订单状态更新为待出库。", "从而就实现了完成的业务流程。但是这并不是一个完美的流程。", "这个过程可能导致出现不一致的地方在于：某个微服务在更新了业务实体后发布事件却失败;虽然微服务发布事件成功，但是消息代理未能正确推送事件到订阅的微服务;接受事件的微服务重复消费了事件。", "可靠事件模式在于保证可靠事件投递和避免重复消费，可靠事件投递定义为：", "避免重复消费要求服务实现幂等性，如支付服务不能因为重复收到事件而多次支付。", "因为现在流行的消息队列都实现了事件的持久化和 at least once 的投递模式，『消息代理确保事件投递至少一次』已经满足，今天不做展开。", "下面分享的内容主要从可靠事件投递和实现幂等性两方面来讨论，我们先来看可靠事件投递。", "首先我们来看一个实现的代码片段，这是从某生产系统上截取下来的。", "根据上述代码及注释，初看可能出现 3 种情况：", "从上面分析的几种情况来看，貌似没有问题。但是仔细分析不难发现缺陷所在，在上面的处理过程中存在一段隐患时间窗口。", "微服务 A 投递事件的时候可能消息代理已经处理成功，但是返回响应的时候网络异常，导致 append 操作抛出异常。最终结果是事件被投递，数据库却被回滚。", "在投递完成后到数据库 commit 操作之间如果微服务 A 宕机也将造成数据库操作因为连接异常关闭而被回滚。最终结果还是事件被投递，数据库却被回滚。这个实现往往运行很长时间都没有出过问题，但是一旦出现了将会让人感觉莫名，很难发现问题所在。", "下面给出两种可靠事件投递的实现方式。", "本地事件表方法将事件和业务数据保存在同一个数据库中，使用一个额外的“事件恢复”服务来恢复事件，由本地事务保证更新业务和发布事件的原子性。考虑到事件恢复可能会有一定的延时，服务在完成本地事务后可立即向消息代理发布一个事件。", "微服务在同一个本地事务中记录业务数据和事件。", "微服务实时发布一个事件立即通知关联的业务服务，如果事件发布成功立即删除记录的事件。", "事件恢复服务定时从事件表中恢复未发布成功的事件，重新发布，重新发布成功才删除记录的事件。", "其中第Ⅱ条的操作主要是为了增加发布事件的实时性，由第三条保证事件一定被发布。", "本地事件表方式业务系统和事件系统耦合比较紧密，额外的事件数据库操作也会给数据库带来额外的压力，可能成为瓶颈。", "外部事件表方法将事件持久化到外部的事件系统，事件系统需提供实时事件服务以接受微服务发布事件，同时事件系统还需要提供事件恢复服务来确认和恢复事件。", "该方式将业务系统和事件系统独立解耦，都可以独立伸缩。但是这种方式需要一次额外的发送操作，并且需要发布者提供额外的查询接口。", "介绍完了可靠事件投递再来说一说幂等性的实现，有些事件本身是幂等的，有些事件却不是。", "如果事件本身描述的是某个时间点的固定值(如账户余额为 100)，而不是描述一条转换指令(如余额增加 10)，那么这个事件是幂等的。", "我们要意识到事件可能出现的次数和顺序是不可预测的，需要保证幂等事件的顺序执行，否则结果往往不是我们想要的。", "如果我们先后收到两条事件，(1)账户余额更新为100，(2)账户余额更新为120。", "1. 微服务收到事件(1)", "2. 微服务收到事件(2)", "3. 微服务再次收到事件(1)", "显然结果是错误的，所以我们需要保证事件(2)一旦执行事件(1)就不能再处理，否则账户余额仍不是我们想要的结果。", "为保证事件的顺序一个简单的做法是在事件中添加时间戳，微服务记录每类型的事件最后处理的时间戳，如果收到的事件的时间戳早于我们记录的，丢弃该事件。如果事件不是在同一个服务器上发出的，那么服务器之间的时间同步是个难题，更稳妥的做法是使用一个全局递增序列号替换时间戳。", "对于本身不具有幂等性的操作，主要思想是为每条事件存储执行结果，当收到一条事件时我们需要根据事件的 ID 查询该事件是否已经执行过，如果执行过直接返回上一次的执行结果，否则调度执行事件。", "在这个思想下我们需要考虑重复执行一条事件和查询存储结果的开销。", "如果重复处理一条事件开销很小，或者可预见只有非常少的事件会被重复接收，可以选择重复处理一次事件，在将事件数据持久化时由数据库抛出唯一性约束异常。", "如果重复处理一条事件的开销相比额外一次查询的开销要高很多，使用一个过滤服务来过滤重复的事件，过滤服务使用事件存储存储已经处理过的事件和结果。", "当收到一条事件时，过滤服务首先查询事件存储，确定该条事件是否已经被处理过，如果事件已经被处理过，直接返回存储的结果;否则调度业务服务执行处理，并将处理完的结果存储到事件存储中。", "一般情况下上面的方法能够运行得很好，如果我们的微服务是 RPC 类的服务我们需要更加小心，可能出现的问题在于，(1)过滤服务在业务处理完成后才将事件结果存储到事件存储中，但是在业务处理完成前有可能就已经收到重复事件，由于是 RPC 服务也不能依赖数据库的唯一性约束;(2)业务服务的处理结果可能出现位置状态，一般出现在正常提交请求但是没有收到响应的时候。", "对于问题(1)可以按步骤记录事件处理过程，比如事件的记录事件的处理过程为“接收”、“发送请求”、“收到应答”、“处理完成”。好处是过滤服务能及时的发现重复事件，进一步还能根据事件状态作不同的处理。", "对于问题(2)可以通过一次额外的查询请求来确定事件的实际处理状态，要注意额外的查询会带来更长时间的延时，更进一步可能某些 RPC 服务根本不提供查询接口。此时只能选择接收暂时的不一致，时候采用对账和人工接入的方式来保证一致性。", "为了描述方便，这里先定义两个概念：", "补偿模式使用一个额外的协调服务来协调各个需要保证一致性的微服务，协调服务按顺序调用各个微服务，如果某个微服务调用异常(包括业务异常和技术异常)就取消之前所有已经调用成功的微服务。", "补偿模式建议仅用于不能避免出现业务异常的情况，如果有可能应该优化业务模式，以避免要求补偿事务。如账户余额不足的业务异常可通过预先冻结金额的方式避免，商品库存不足可要求商家准备额外的库存等。", "我们通过一个实例来说明补偿模式，一家旅行公司提供预订行程的业务，可以通过公司的网站提前预订飞机票、火车票、酒店等。", "上海-北京6月19日9点的某某航班。", "某某酒店住宿3晚。", "北京-上海6月22日17点火车。", "在客户提交行程后，旅行公司的预订行程业务按顺序串行的调用航班预订服务、酒店预订服务、火车预订服务。最后的火车预订服务成功后整个预订业务才算完成。", "如果火车票预订服务没有调用成功，那么之前预订的航班、酒店都得取消。取消之前预订的酒店、航班即为补偿过程。", "为了降低开发的复杂性和提高效率，协调服务实现为一个通用的补偿框架。补偿框架提供服务编排和自动完成补偿的能力。", "要实现补偿过程，我们需要做到两点：", "首先要确定失败的步骤和状态，从而确定需要补偿的范围。", "在上面的例子中我们不仅要知道第 3 个步骤(预订火车)失败，还要知道失败的原因。如果是因为预订火车服务返回无票，那么补偿过程只需要取消前两个步骤就可以了;但是如果失败的原因是因为网络超时，那么补偿过程除前两个步骤之外还需要包括第 3 个步骤。", "比如一个支付微服务的补偿操作要求参数包括支付时的业务流水 id、账号和金额。理论上说实际完成补偿操作可以根据唯一的业务流水 id 就可以，但是提供更多的要素有益于微服务的健壮性，微服务在收到补偿操作的时候可以做业务的检查，比如检查账户是否相等，金额是否一致等等。", "实现补偿模式的关键在于业务流水的记录", "做到上面两点的办法是记录完整的业务流水，可以通过业务流水的状态来确定需要补偿的步骤，同时业务流水为补偿操作提供需要的业务数据。", "当客户的一个预订请求达到时，协调服务(补偿框架)为请求生成一个全局唯一的业务流水号，并在调用各个工作服务的同时记录完整的状态。", "当调用某个服务出现异常时，比如第 3 步骤(预订火车)异常。", "协调服务(补偿框架)同样会记录第 3 步的状态，同时会另外记录一条事件，说明业务出现了异常。然后就是执行补偿过程了，可以从业务流水的状态中知道补偿的范围，补偿过程中需要的业务数据从记录的业务流水中获取。", "对于一个通用的补偿框架来说，预先知道微服务需要记录的业务要素是不可能的。那么就需要一种方法来保证业务流水的可扩展性，这里介绍两种方法：大表和关联表。", "大表顾明思议就是设计时除必须的字段外，还需要预留大量的备用字段，框架可以提供辅助工具来帮助将业务数据映射到备用字段中。", "关联表，分为框架表和业务表，技术表中保存为实现补偿操作所需要的技术数据，业务表保存业务数据，通过在技术表中增加业务表名和业务表主键来建立和业务数据的关联。", "大表对于框架层实现起来简单，但是也有一些难点，比如预留多少字段合适，每个字段又需要预留多少长度。另外一个难点是如果向从数据层面来查询数据，很难看出备用字段的业务含义，维护过程不友好。", "关联表在业务要素上更灵活，能支持不同的业务类型记录不同的业务要素;但是对于框架实现上难度更高，另外每次查询都需要复杂的关联动作，性能方面会受影响。", "有了上面的完整的流水记录，协调服务就可以根据工作服务的状态在异常时完成补偿过程。但是补偿由于网络等原因，补偿操作并不一定能保证 100%成功，这时候我们还要做更多一点。", "补偿过程作为一个服务调用过程同样存在调用不成功的情况，这个时候需要通过重试的机制来保证补偿的成功率。当然这也就要求补偿操作本身具备幂等性。", "关于幂等性的实现在前面做过讨论。", "如果只是一味的失败就立即重试会给工作服务造成不必要的压力，我们要根据服务执行失败的原因来选择不同的重试策略。", "1) 如果失败的原因不是暂时性的，由于业务因素导致(如业务要素检查失败)的业务错误，这类错误是不会重发就能自动恢复的，那么应该立即终止重试。", "2) 如果错误的原因是一些罕见的异常，比如因为网络传输过程出现数据丢失或者错误，应该立即再次重试，因为类似的错误一般很少会再次发生。", "3) 如果错误的原因是系统繁忙(比如 HTTP 协议返回的 500 或者另外约定的返回码)或者超时，这个时候需要等待一些时间再重试。", "重试操作一般会指定重试次数上线，如果重试次数达到了上限就不再进行重试了。这个时候应该通过一种手段通知相关人员进行处理。", "对于等待重试的策略如果重试时仍然错误，可逐渐增加等待的时间，直到达到一个上限后，以上限作为等待时间。", "如果某个时刻聚集了大量需要重试的操作，补偿框架需要控制请求的流量，以防止对工作服务造成过大的压力。", "另外关于补偿模式还有几点补充说明。", "微服务实现补偿操作不是简单的回退到业务发生时的状态，因为可能还有其他的并发的请求同时更改了状态。一般都使用逆操作的方式完成补偿。", "补偿过程不需要严格按照与业务发生的相反顺序执行，可以依据工作服务的重用程度优先执行，甚至是可以并发的执行。", "有些服务的补偿过程是有依赖关系的，被依赖服务的补偿操作没有成功就要及时终止补偿过程。", "如果在一个业务中包含的工作服务不是都提供了补偿操作，那我们编排服务时应该把提供补偿操作的服务放在前面，这样当后面的工作服务错误时还有机会补偿。", "设计工作服务的补偿接口时应该以协调服务请求的业务要素作为条件，不要以工作服务的应答要素作为条件。因为还存在超时需要补偿的情况，这时补偿框架就没法提供补偿需要的业务要素。", "一个完整的 TCC 业务由一个主业务服务和若干个从业务服务组成，主业务服务发起并完成整个业务活动，TCC 模式要求从服务提供三个接口：Try、Confirm、Cancel。", "完成所有业务检查", "预留必须业务资源", "真正执行业务", "不作任何业务检查", "只使用 Try 阶段预留的业务资源", "Confirm 操作满足幂等性", "释放 Try 阶段预留的业务资源", "Cancel 操作满足幂等性", "整个 TCC 业务分成两个阶段完成。", "第一阶段：主业务服务分别调用所有从业务的 try 操作，并在活动管理器中登记所有从业务服务。当所有从业务服务的 try 操作都调用成功或者某个从业务服务的 try 操作失败，进入第二阶段。", "第二阶段：活动管理器根据第一阶段的执行结果来执行 confirm 或 cancel 操作。", "如果第一阶段所有 try 操作都成功，则活动管理器调用所有从业务活动的 confirm操作。否则调用所有从业务服务的 cancel 操作。", "需要注意的是第二阶段 confirm 或 cancel 操作本身也是满足最终一致性的过程，在调用 confirm 或 cancel 的时候也可能因为某种原因(比如网络)导致调用失败，所以需要活动管理支持重试的能力，同时这也就要求 confirm 和 cancel 操作具有幂等性。", "在补偿模式中一个比较明显的缺陷是，没有隔离性。从第一个工作服务步骤开始一直到所有工作服务完成(或者补偿过程完成)，不一致是对其他服务可见的。另外最终一致性的保证还充分的依赖了协调服务的健壮性，如果协调服务异常，就没法达到一致性。", "TCC模式在一定程度上弥补了上述的缺陷，在TCC模式中直到明确的confirm动作，所有的业务操作都是隔离的(由业务层面保证)。另外工作服务可以通过指定 try 操作的超时时间，主动的 cancel 预留的业务资源，从而实现自治的微服务。", "TCC模式和补偿模式一样需要需要有协调服务和工作服务，协调服务也可以作为通用服务一般实现为框架。与补偿模式不同的是 TCC 服务框架不需要记录详细的业务流水，完成 confirm 和 cancel 操作的业务要素由业务服务提供。", "在第4步确认预订之前，订单只是pending状态，只有等到明确的confirm之后订单才生效。", "如果3个服务中某个服务try操作失败，那么可以向TCC服务框架提交cancel，或者什么也不做由工作服务自己超时处理。", "TCC 模式也不能百分百保证一致性，如果业务服务向 TCC 服务框架提交 confirm后，TCC 服务框架向某个工作服务提交 confirm 失败(比如网络故障)，那么就会出现不一致，一般称为 heuristic exception。", "需要说明的是为保证业务成功率，业务服务向 TCC 服务框架提交 confirm 以及TCC 服务框架向工作服务提交 confirm/cancel 时都要支持重试，这也就要confirm/cancel 的实现必须具有幂等性。如果业务服务向 TCC 服务框架提交confirm/cancel 失败，不会导致不一致，因为服务最后都会超时而取消。", "另外 heuristic exception 是不可杜绝的，但是可以通过设置合适的超时时间，以及重试频率和监控措施使得出现这个异常的可能性降低到很小。如果出现了heuristic exception 是可以通过人工的手段补救的。", "如果有些业务由于瞬时的网络故障或调用超时等问题，通过上文所讲的 3 种模式一般都能得到很好的解决。但是在当今云计算环境下，很多服务是依赖于外部系统的可用性情况，在一些重要的业务场景下还需要周期性的对账来保证真实的一致性。比如支付系统和银行之间每天日终是都会有对账过程。", "作者介绍", "朱江，普元解决方案中心总经理。2003 年毕业于河海大学，2007 年加入普元信息技术股份有限公司，长期致力于金融软件平台建设实践。主持完成多个金融行业平台产品及解决方案的研发工作，包括业务集成平台、集中监控平台、统一渠道接入平台等。对金融行业技术架构有较深刻理解，曾主持中国工商银行、建设银行、中信银行、国开银行等多家大型企业的技术平台规划与落地，目前负责普元解决方案中心，负责基于普元标准产品打造行业解决方案。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/74029"]},
{"module": ["干货教程"], "note": ["文 | 秦路\n前一篇文章列举了常用的分析图表，今天主要围绕Excel常用的图表技巧，以及图表的设计规范展开。相信大家练习后，图表设计能力会从下图：\n\n进化到：\n\n数据分析师应该会设计图表和报..."], "title": ["新手入门 | 打造高端的数据报表"], "content": ["前一篇文章列举了常用的分析图表，今天主要围绕Excel常用的图表技巧，以及图表的设计规范展开。相信大家练习后，图表设计能力会从下图：", "进化到：", "数据分析师应该会设计图表和报表。这里并不是要求大家像设计师一样运用Photoshop等软件绘制，而是知道数据元素如何用图表更好的表达，将数据蕴含的信息展现出来。理解数据是分析师的工作，展示数据也是分析师的工作。", "对于非数据分析师的岗位，诸如运营、产品和市场，每天也会接触大量的报表，日报周报项目报，用好图表，让枯燥的数据变成丰富的视觉，也是一种帮助。", "好的数据可视化，应该设计和数据并存。数据分析师拿到很多数据，把它们像食材般加工成美味佳肴，但是菜色卖相不好，就让人下不了筷子。可视化就是数据的色香味，也是沟通和汇报的工具。图表给自己看，再难看也无所谓，如果需要汇报给领导和同事，美观和易读至少需要吧，不然大家怎么了解你的分析成果。", "今天的内容倾向于报表化图表的制作，好的图表应该有以下几个要素：", "图表的首要功能是解释，而不是设计，尤其大部分图表都会落入到过度设计的陷阱。", "图表设计，首先应该是没有设计。", "上图是Excel老版本的默认模板，连直男也无法忍受的酷炫3D渐变阴影风。想当年小鲜肉的我也为此目眩神迷，给翠花完成PPT作业都是用得这类设计……然后就没有然后了。", "当我们使用这类图表，反问一句，这些设计有必要么?我们只需要用到柱形图的对比，那么阴影用来干什么?渐变用来干什么?背景黑色用来干什么?", "这些元素对我们解读数据没有任何帮助，甚至会阻碍读者阅读数据。", "其次，好的图表要能解释数据，不同图表有不同的使用方法，如果我要观察销售额的增长，我就不应该用饼图。如果数据量过少，就不应该使用散点图。图表如何使用可以看我的上一篇文章。", "为了使数据的解读不失真，数据应要求精确到小数位。12.5比12好，尤其在对比数据的情况下。数据若再近一步精确，对解读的意义就不大，反而对解读者进行干扰。", "另外一种情况是单位换算造成的数据失真。例如3145米换算成3.1公里等，虽然是小细节，但不要出现某饼图的各比例相加不等于100%的案例。", "数据的解读因为每个人的观点和视角不同，可以呈现诸多的结果。这也是我们常说统计学会撒谎的原因。", "有经验的数据分析师甚至能够引导分析的结果。", "下图是一张销售额柱形图，看来销售额没有啥特大变化嘛。", "我们换另外一种图表展示。就看到了变化的增长趋势。", "实际上两张图表的数据没有任何差异，为什么呢?区别只在坐标轴。第一张图的Y坐标轴起始为0，第二张图起始是2.45。第二张是截取了部分的柱形图。", "只是随便动动手脚，数据表达就那么大差异，看来数据分析师也可以是阴险角色啊。其他方法还有销售额细分。看一下不同地区的增长，如果某几个地区的增长好，就单独拎出来作图，美其名曰抽样。", "统一是规范和约束图表，当图表过多时，一个统一标准不会让人眼花撩乱。", "如果图表整体颜色是冷色调，那么就不要再加入暖色。", "如果图表文字是雅黑，就不要再加入宋体。", "如果某地区数据，用了柱形图对比，其他地区也遵循柱形图样式。", "如果某图表，女性使用红色，男性使用蓝色，那么这一规范应该在所有图表体现。除了颜色，其他设计元素同理。", "如果有多张图表，图表元素应该统一，如标题、坐标轴刻度、坐标轴位置等。", "统一性是为了可读性服务的，如果图表样式混乱，解读者会非常困惑。你做一份男女差异的数据报告，前面男性数据都用蓝色表示，突然换个粉色谁会受得了?", "后文的教程，都会遵循以上要素进行设计。", "好的配色决定了图表整体美观上限。一般只选取两到三种颜色。过多的颜色无法聚焦于图表本身，会让图表变得像玛丽苏。", "配色属于设计领域的技能，分析师就不要学怎么搭配辅色对比色了。直接借助Adobe Color CC的色彩主题选取颜色。", "第一个配色方案不错，高贵冷艳的商务风。后续图表我们均用此配色方案，下面是十六制颜色编号：", "404B4F", "82DFFF", "CEF2FF", "416F7F", "A5C2CC", "利用rand( ) 函数随机生成示范数据，生成一张默认的柱形图。", "比起酷炫3D渐变阴影风，Excel2016的默认图表更易让人接受，这张图表在设计方面其实已经合格。不过男同胞若要为姑娘们的PPT和Excel排忧解难，我们得再深入设计。", "首先将数据和图表分离在两张sheet，方便设计。", "新sheet背景涂成灰色，记住是灰色。灰色是一种中立的颜色，它和任何颜色结合都不会显得突兀。", "接下来就是砍砍砍时间，我们将图表上所有与数据表达无关的元素全部删除：销售额标题移除、背景色设置为透明，柱形图颜色调整为配色方案。", "很多人会疑问，为什么连标题、背景色都要删除。它和word的原理近似：设计样式和内容分离。图表只需要聚焦于数据表达本身。标题可以通过Excel的单元格添加，别用图表的元素。", "对第一列单元格颜色填充，拉高，开头空两格，输入配色文字。", "此时标题的设计比图表自带标题不知道好看到哪里去了。旁边的单元格可以配上文字注释，辅助阅读者理解。设计的核心思路是通过单元格完善图表，图表只专注数据元素。", "更优秀的报表，会加入重点数据，引入更多图表，用颜色划分区块，每一区块对应相应的数据内容，使之具有Dashboard的雏形。", "因为时间有限，就不更多展开了(上海我直接复制的)。熟练后制作这样的报表大约只要5分钟，但是对数据内容产生的隐性价值是不可估量。多补充一点，如果设计过程中图表大小位置因为单元格一直变化。可以在选项中选择固定大小和固定位置，方便设计。", "PPT和Excel是通用的，以上设计方法也适用在PPT中，相信大家已经了解怎么设计出商务范的报表。大家可以参考Excel2016的默认模板，都是出彩的设计。", "我希望大家看到这里，能了解Excel图表和报表的设计原理，但日后不要陷入一味追求美观的道路。图表是最终结果的呈现，作为报告让它美观无可厚非，但是在分析过程中，马马虎虎也就得了，毕竟自己看。在我的工作场景中，除了有需要，我也不会专门设计这类报表。", "另外悄悄告诉大家，不要展示太牛逼的图表设计技巧，不然以后老板都会让你设计的……", "ok，了解完图表和报表设计，接下来讲解中高级技巧。", "复合图表和次坐标轴堪称图表届的vlookup。它能给图表添加更丰富的视觉表达。", "前文中我增加了利润数据，现在我们需要计算利润的变化趋势。因为利润和销售额不是同一个维度，再用柱形图不合适，此时可以用折线图表达。", "点“选择数据”，添加利润，Y轴选取利润所在数据区域。", "这时候橙色就是添加上的利润。我们点击橙色柱形图，右键更改图表类型为折线图。", "更改配色，虽然橙色也挺好看的。线条也改为平滑。", "利润和销售额的数值在同一个度量范围，可以共用坐标轴。如果利润数值过小，比如零点几，或者30%这种比例，那么在图表上会近似一条平缓的直线，视觉表达欠佳。本图的数据类型我不建议用次坐标轴。", "点击折线图，选择坐标轴系列选项，点次坐标轴。可以看到图表两边都出现了坐标刻度。左边是利润刻度，右边是销售额刻度，但在不做说明的情况下阅读者根本不会了解。为了可读性，需要额外的增加元素说明。这又无形中增加了设计复杂度。所以图表的元素取舍有多方面的考虑因素。", "如果复合图表及次坐标轴堪称图表届的vlookup，那么辅助列就是图表的数据透视表。", "在我之前的文章(这篇文章在微信公众号，感兴趣可以看，以后抽空补上)，里面甘特图的绘制借助了辅助列。", "这里讲解如何利用辅助列绘制漏斗图。", "下图是牛逼逼有限公司电商APP的销售流程转化，每一行代表用户操作类型和对应达成的用户数。", "漏斗图的本质是堆积条形图。我们增加两个辅助列，计算方式为(最大用户数-当前用户数)/2。", "将列顺序更改为辅助列1，用户数，辅助列2的排序。然后新建图表为横向堆积条形图。相信在这里大家已经看出辅助列的作用了。", "形状出来了，接下来我们还需要几步操作。点击Y轴，选择设置坐标轴格式，逆序类别上打上勾。", "图表成功翻转，将左右两边的条形图颜色设置为无。对图表元素砍砍砍，只保留核心元素。", "接下来我们将图表放到报表上，进行适当的装饰，搞定", "辅助线和辅助列不一样，辅助线是图表附加的元素。辅助线能绘制高级图表。", "甘特图在项目管理中用来统筹时间进度。如果我需要在现有的图表基础上增加原计划实际计划，应该怎么做?下图是牛逼逼公司在其APP上准备的双十二促销活动计划表。如何直观的用甘特图表示活动进度?", "先增加一列辅助列。该列辅助列用于后续定位。数值等值增加即可。", "选择计划开始时间和辅助列，插入图表，带折线的散点图(只选数字即可，不要选自段名)。", "现在看不出这是甘特图，不要紧，对Y轴进行逆序排列。点击图表的添加元素，增加误差线-标准误差，误差线就是我们的辅助线。", "误差线是高级图表经常用到的功能，箱线图、标靶图/子弹图都能用误差线做出来。下图十字状的线条就是误差线。", "选择横向的水平误差线，点击设置格式，方向正误差，无线端，误差值自定义，选择计划使用时间这一列。", "删除垂直误差线，对误差线线条加粗，设置颜色。现在有一点甘特图的样子了。", "点击图表选择数据，新增加一个系列，数据源选择实际开始时间和辅助列(在最开始建立散点图时，其实可以一并选择的)。", "点击橙色这条线，重复误差线的建立。在自定义值时，不要选择计划使用时间，而是实际完成时间。设置线条颜色的时候，用另外一种颜色和样式，以便区分。", "之后将橙色和蓝色填充为无，移除无用的设计元素。我们把甘特图放到报表中。", "因为时间关系，甘特图还是缺漏部分说明元素的。大家有兴趣可以自己尝试添加。很多咨询公司图表就是这样画的。", "误差线是一种高级用法，在高级的可视化分析中我们会利用误差线进行一系列的分析，在后续的章节内容中我们会再次学习。", "最终这幅报表就是成果，不知道大家有没有掌握。其实Excel还有更强大的功能，比如切片，比如动态图表，比如各插件，通过一系列的组合可以做出半自动报表，像财务报表、咨询公司报表，都有各自的风格，大家可以网上查询学习。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数 据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程 、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73924"]},
{"module": ["干货教程"], "note": ["\n文 | Xiaobing Li & Ankur Bansal  译者 | 薛命灯\n随着Uber业务规模不断增长，我们的系统也在持续不断地产生更多的事件、服务间的消息和日志。这些数据在得到处理之前需要经过Kafka。那..."], "title": ["开源“Chaperone”：Uber是如何对Kafka进行端到端审计的"], "content": ["文 | Xiaobing Li & Ankur Bansal  译者 | 薛命灯", "随着Uber业务规模不断增长，我们的系统也在持续不断地产生更多的事件、服务间的消息和日志。这些数据在得到处理之前需要经过Kafka。那么我们的平台是如何实时地对这些数据进行审计的呢?", "为了监控Kafka数据管道的健康状况并对流经Kafka的每个消息进行审计，我们完全依赖我们的审计系统Chaperone。Chaperone自2016年1月成为Uber的跨数据中心基础设施以来，每天处理万亿的消息量。下面我们会介绍它的工作原理，并说明我们为什么会构建Chaperone。", "Uber的Kafka数据管道概览Uber的服务以双活的模式运行在多个数据中心。Apache Kafka和uReplicator是连接Uber生态系统各个部分的消息总线。", "截止2016年11月份，Uber的Kafka数据管道概览。数据从两个数据中心聚合到一个Kafka集群上。", "要让Uber的Kafka对下游的消费者做出即时响应是很困难的。为了保证吞吐量，我们尽可能地使用批次，并严重依赖异步处理。服务使用自家的客户端把消息发布到Kafka代理，代理把这些消息分批转发到本地的Kafka集群上。有些Kafka的主题会被本地集群直接消费，而剩下的大部分会跟来自其他数据中心的数据一起被组合到一个聚合Kafka集群上，我们使用uReplicator来完成这种面向大规模流或批处理的工作。", "Uber的Kafka数据管道可以分为四层，它们跨越了多个数据中心。Kafka代理和它的客户端分别是第二层和第一层。它们被作为消息进入第三层的网关，也就是每个数据中心的本地Kafka集群。本地集群的部分数据会被复制到聚合集群，也就是数据管道的最后一层。", "Kafka数据管道的数据都会经过分批和确认(发送确认)：", "Kafka数据管道的数据流经的路径概览。", "Uber的数据从代理客户端流向Kafka需要经过几个阶段：", "我们为了让Kafka支持高吞吐量，做出了一些权衡。数以千计的微服务使用Kafka来处理成百上千的并发业务流量(而且还在持续增长)会带来潜在的问题。Chaperone的目标是在数据流经数据管道的每个阶段，能够抓住每个消息，统计一定时间段内的数据量，并尽早准确地检测出数据的丢失、延迟和重复情况。", "Chaperone概览Chaperone由四个组件组成：AuditLibrary、ChaperoneService、ChaperoneCollector和WebService。", "Chaperone架构：AuditLibrary、ChaperoneService、ChaperoneCollector和WebService，它们会收集数据，并进行相关计算，自动检测出丢失和延迟的数据，并展示审计结果。", "AuditLibrary实现了审计算法，它会定时收集并打印统计时间窗。这个库被其它三个组件所依赖。它的输出模块是可插拔的(可以使用Kafka、HTTP等)。在代理客户端，审计度量指标被发送到Kafka代理。而在其它层，度量指标直接被发送到专门的Kafka主题上。", "审计算法是AuditLibrary的核心，Chaperone使用10分钟的滚动时间窗来持续不断地从每个主题收集消息。消息里的事件时间戳被用来决定该消息应该被放到哪个时间窗里。对于同一个时间窗内的消息，Chaperone会计算它们的数量和p99延迟。Chaperone会定时把每个时间窗的统计信息包装成审计消息发送到可插拔的后端，它们可能是Kafka代理或者之前提到的Kafka服务器。", "Chaperone根据消息的事件时间戳把消息聚合到滚动时间窗内。", "审计消息里的tier字段很重要，通过它可以知道审计是在哪里发生的，也可以知道消息是否到达了某一个地方。通过比较一定时间段内不同层之间的消息数量，我们可以知道这段时间内所生成的消息是否被成功送达。", "ChaperoneService是工作负载最高的一个组件，而且总是处在饥饿的状态。它消费Kafka的每一个消息并记录时间戳。ChaperoneService是基于uReplicator的HelixKafkaConsumer构建的，这个消费者组件已经被证明比Kafka自带的消费者组件(Kafka 0.8.2)更可靠，也更好用。ChaperoneService通过定时向特定的Kafka主题生成审计消息来记录状态。", "ChaperoneCollector监听特定的Kafka主题，并获取所有的审计消息，然后把它们存到数据库。同时，它还会生产多个仪表盘：", "Chaperone创建的仪表盘，从上面我们看出数据的丢失情况。", "从上图可以看出每个层的主题消息总量，它们是通过聚合所有数据中心的消息得出的。如果没有数据丢失，所有的线会完美地重合起来。如果层之间有数据丢失，那么线与线之间会出现裂缝。例如，从下图可以看出，Kafka代理丢掉了一些消息，不过在之后的层里没有消息丢失。从仪表盘可以很容易地看出数据丢失的时间窗，从而可以采取相应的行动。", "从仪表盘上还能看出消息的延迟情况，借此我们就能够知道消息的及时性以及它们是否在某些层发生了传输延迟。用户可以直接从这一个仪表盘上看出主题的健康状况，而无需去查看Kafka服务器或uReplicator的仪表盘：", " ", "Chaperone提供一站式的仪表盘来查看每个数据中心的主题状态。", "最后，WebService提供了REST接口来查询Chaperone收集到的度量指标。通过这些接口，我们可以准确地计算出数据丢失的数量。在知道了数据丢失的时间窗后，我们可以从Chaperone查到确切的数量：", "Chaperone的两个设计目标在设计Chaperone时，为了能够做到准确的审计，我们把注意力集中在两个必须完成的任务上：", "为了确保每个消息只被审计一次，ChaperoneService使用了预写式日志(WAL)。ChaperoneService每次在触发Kafka审计消息时，会往审计消息里添加一个UUID。这个带有相关偏移量的消息在发送到Kafka之前被保存在WAL里。在得到Kafka的确认之后，WAL里的消息被标记为已完成。", "如果ChaperoneService崩溃，在重启后它可以重新发送WAL里未被标记的审计消息，并定位到最近一次的审计偏移量，然后继续消费。WAL确保了每个Kafka消息只被审计一次，而且每个审计消息至少会被发送一次。", "接下来，ChaperoneCollector使用ChaperoneService之前添加过的UUID来移除重复消息。有了UUID和WAL，我们可以确保审计的一次性。在代理客户端和服务器端难以实现一次性保证，因为这样会给它们带来额外的开销。我们依赖它们的优雅关闭操作，这样它们的状态才会被冲刷出去。", "因为Chaperone可以在多个层里看到相同的Kafka消息，所以为消息内嵌时间戳是很有必要的。如果没有这些时间戳，在计数时会发生时间错位。在Uber，大部分发送到Kafka的数据要么使用avro风格的schema编码，要么使用JSON格式。对于使用schema编码的消息，可以直接获取时间戳。而对于JSON格式的消息，需要对JSON数据进行解码才能拿到时间戳。", "为了加快这个过程，我们实现了一个基于流的JSON消息解析器，这个解析器无需预先解码整个消息就可以扫描到时间戳。这个解析器用在ChaperoneService里是很高效的，不过对代理客户端和服务器来说仍然需要付出很高代价。", "所以在这两个层里，我们使用的是消息的处理时间戳。因为时间戳的不一致造成的层间计数差异可能会触发错误的数据丢失警告。我们正在着手解决时间戳不一致问题，之后也会把解决方案公布出来。", "在Chaperone之前，数据丢失的第一个征兆来自数据消费者，他们会出来抱怨数据的丢失情况。但是等他们出来抱怨已经为时已晚，而且我们无法知道是数据管道的哪一部分出现了问题。有了Chaperone之后，我们创建了一个用于检测丢失数据的作业，它会定时地从Chaperone拉取度量指标，并在层间的消息数量出现不一致时发出告警。", "告警包含了Kafka数据管道端到端的信息，从中可以看出那些管道组件的度量指标无法告诉我们的问题。检测作业会自动地发现新主题，并且你可以根据数据的重要性配置不同的告警规则和阈值。数据丢失的通知会通过多种通道发送出去，比如页式调度系统、企业聊天系统或者邮件系统，总之会很快地通知到你。", "我们生产环境的大部分集群仍然在使用Kafka 0.8.x，这一版本的Kafka对从时间戳到偏移量的索引没有提供原生支持。于是我们在Chaperone里自己构建了这样的索引。这种索引可以用来做基于时间区间的查询，所以我们不仅限于使用Kafka的偏移量来读取数据，我们可以使用Chaperone提供的时间戳来读取数据。", "Kafka对数据的保留是有期限的，不过我们对消息进行了备份，并把消息的偏移量也原封不动地保存起来。借助Chaperone提供的索引，用户可以基于时间区间读取这些备份数据，而不是仅仅局限于Kafka现存的数据，而且使用的访问接口跟Kafka是一样的。", "有了这个特性，Kafka用户可以通过检查任意时间段里的消息来对他们的服务进行问题诊断，在必要时可以回填消息。当下游系统的审计结果跟Chaperone出现不一致，我们可以把一些特定的消息导出来进行比较，以便定位问题的根源。", "总结我们构建了Chaperone来解决以下问题：", "Chaperone不仅仅告诉我们系统的健康情况，它还告诉我们是否有数据丢失。例如，在Kafka服务器返回非预期的错误时，uReplicator会出现死循环，而此时uReplicator和Kafka都不会触发任何告警，不过我们的检测作业会很快地把问题暴露出来。", "如果你想更多地了解Chaperone，可以自己去探究。我们已经把Chaperone开源，它的源代码放在Github上。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73997"]},
{"module": ["干货教程"], "note": ["\n文 | 高颜 海航生态科技技术研究院大数据开发工程师\n文章介绍了海航生态科技舆情大数据平台的容器化改造经验，包括初期技术架构、应用容器化、架构迁移、持续发布与部署。\n海航舆情监控系统..."], "title": ["海航生态科技舆情大数据平台容器化改造"], "content": ["文 | 高颜 海航生态科技技术研究院大数据开发工程师", "文章介绍了海航生态科技舆情大数据平台的容器化改造经验，包括初期技术架构、应用容器化、架构迁移、持续发布与部署。", "海航舆情监控系统能够为海航集团内部提供监控网络舆情信息，对负面信息、重大舆情及时预警，研判具体舆情或者某一舆情专题事件的发展变化趋势，生成图标报告和各种统计数据，提高舆情工作效率和辅助领导决策。然而，随着项目的持续运行，许多问题逐渐暴露出来，为了解决这些难题，对整个项目重新规划设计，迁移到Hadoop、Spark大数据平台，引进持续化Docker容器部署和发布，开发和运营效率得到显著提升。", "舆情平台项目的初衷是为了加强海航集团及其下属各成员企业的品牌效应，并且减少关键信息传播的成本，及时洞悉客户评价和舆论走向，以及指导舆论引导工作，加快对紧急事件的响应速度。", "需要完成工作包括分析及预测敏感内容在互联网、社交网络等载体的传播状况，包括数据采集， 情感分析，爆发预测，敏感预警等", "目前的规模：", "通过设置微博种子账户(一部分通过搜索，一部分是公司微博账号)，挖掘粉丝的粉丝深层次挖掘，爬取数据每天信息条目目前有20w 左右，逐渐会加入更多 的种子账户，也在沟通购买新浪的开放API;", "其他如微信公众号、自媒体类，同行业票价网站等，一共300多家站点，数据维度达到30多个，每天数据量达150w条，数据量接近10G;", "一些展示效果如下所示：", "加入项目的时候，项目架构比较简单，作为一个验证阶段，就是一个传统的Web应用，采用的 Spring Web MVC + MySQL，再加上数据采集功能爬虫系统+文本分析模型(CNN)，代码审查使用Git + GitLab。", "Java语言实现，基于WebMagic框架二次开发。由于各个网站的页面布局没有一个统一的格式，所以开发人员需要针对每个网站单独写一个爬虫程序用来做页面数据解析。爬虫在部署的时候是，手动进行编译，并按照运行计划打多个可执行jar包，分别部署到多个节点上执行，数据存入MySQL数据库(用一个专门的节点来部署)。支持最初的30几个网站和微博的数据，数据量每天大概有不到20w。", "Python实现，使用结巴分词工具和CNN(卷积神经网络)模型，支持矩阵批量运算。运行方式是Python web(用框架是Tornado)提供API，由爬虫调用调用，并回填结果，增加情感倾向、热度、关键词等字段，后存入数据库。", "典型的Spring MVC应用，采用Spring MVC + MyBatis + MySQL，前端使用ECharts生成图形和报表;统计数据是提前计算好，存入MySQL数据库中，并通过Quartz调度运算作业和数据更新。", "很显然，MySQL无法应对数据的大量增长，这个平台对于数据的增长和扩张是无法适应的， 应用的接口响应时间从开始的几秒甚至延长到几分钟，无法令人接受。", "总结一下，这个框架有多个显而易见的弊端(也算是初期作为验证使用，另一方面也是因为开始资源不足)：", "为了解决上述问题，我们就尝试去做首先确定的是需要迁往大数据平台。在这同时，我们做了一些容器化的工作。做这些工作的目的是，方", "便部署和迁移，容易进行伸缩控制，能够借助工具向着自动化的方向进行。", "采用Gradle构建工具，使用了Gretty插件，去除代码依赖 jar包，依赖代码化，配置一键调试和运行;采用Jenkins持续构建工具，给每一个模块搭建了一条流水线代码测试、打包和部署，目前部署是shell脚本实现。", "爬虫代码中每个站点的数据抓取是一条流水线，每条流水线有着相同的流程，我们把配置部分代码抽出来，改写启动入口接收配置参数，由配置来决定启动哪些站点的流水线;修改Spring Web改为前后端分离;", "首先是MySQL数据库容器化，把默认的/var/lib/mysql数据目录和配置文件目录挂载到了本地，把之前的数据做了迁移;接着是Web服务，使用Tomcat镜像，挂载了WebApps目录，Gradle打war包复制到本地挂载目录;", "然后是文本分析模型，由于文本分析模型需要安装大量依赖文件(pip)，我们重新构建了镜像提交到本地Registry;周期执行的计算任务打成jar包，运行时启动新的镜像实例运行。", "容器编排我们使用的是Rancher平台，使用默认Cattle编排引擎。我们大概有40多个长时运行的实例，分为3类：", "爬虫实例，接近40个实例调度到20多个宿主节点上。我们数据放在在CDH平台上，这些容器间并不发生通信，只与文本分析模型进行通信，最后数据发送到CDH集群的Kafka，对这些实例只进行代码替换、更新及运维工作;", "目前部署了3个文本分析模型的实例，由爬虫根据名字随机请求。", "批处理任务类，使用Rancher提供的crontab工具，周期性的运行。", "现在可以做到自动的代码更新和部署，时间大概不到一个小时，之前部署一次至少半天。", "Rancher提供了Registry管理功能，可以很方便地管理Registry。为了加速下载，我们在本地部署了一个Registry，方便镜像更新和应用迁移。", "随着爬虫爬取的数据逐日增加，现在这个系统肯定是支撑不了的。 我们经过讨论，确定了基本架构。使用HBase + ElasticSearch作为数据存储，Kafka作消息队列，由HBase负责保存爬虫数据，ES则负责建立索引(我们的一致性目前要求不高)。由Rancher管理分布式爬虫将爬取的数据送往Kakfa集群，在这之前向文本分析模型(容器中)发送http请求，回填相应字段。然后再由两个Kafka Consumer将数据分别传输到HBase和ES中完成数据保存。", "爬虫现在经过容器化，由Rancher进行管理。", "统计工作交由Spark SQL读写HBase完成，目前还没有做到实时的。我们的做法是按天统计存到表中，服务请求时根据请求条件选择计算范围进行实时计算。这个算是离实时性前进了一步，接下来会继续改造成实时的。", "这里有一个细节，由于我们的数据是有时间要求的，有根据时间排序的需求，而且我们处理的数据也主要是在近期范围的(最近一天/周/月/年)，所以我们希望HBase能根据记录的发布时间来排倒序，于是我们将时间戳作为HBase的rowkey拼接的第一段，但这样又引入了新的问题，记录在HBase集群上会“扎堆”，于是为了缓解这个问题，我们把发布时间的小时拿出来放在这个时间戳之前，这样局部还是根据时间排序的，暂时也不会影响到HBase节点的伸缩。", "后端使用Spring Data (ES + HBase)操作数据，暂时未加入缓存机制;前端还是用AngularJS，但是做了前后端分离。现在总数据量已经达到之前的数十倍，数据请求基本在1S以内，检索查询由ES提供数据，请求基本在300ms至1s。离线批处理作业执行时间由先前的8min缩减到平均2.5分钟。", "目前大数据平台未实现容器化，运行在一套CDH集群上，集群配置了高可用。Kafka和ES使用的是开源版(Spring Data的版本原因)，通过使用Supervisord提高其服务的可靠性。", "在这一块儿，我们下一步的目标是将大数据平台的计算部分如spark、模型算法这一块儿分离出来实现容器化，方便我们实现计算能力根据计算量进行弹性自动伸缩，我们有一套基于Mesos管理Docker镜像的测试集群，包括Spark应用和分布式的机器学习算法，这一部分正在测试中。", "这一块使用GitLab + Gradle + Jenkins(Docker)+ Shell脚本", "Gitlab中设置提交触发，Jenkins设置接收触发执行Pipeline，Jenkins执行构建，调用Gradle和Shell命令执行构建;由于已做了代码和配置文件分开映射到本地，部署时复制打包代码到部署节点替换代码文件，重启容器实例完成服务部署。", " ", " ", "Q：Spark直接运行在Mesos不是很方便么，容器化优势是否明显?主要考虑点在哪?", "A：容器化主要考虑两点:一 解决海量数据计算的资源编排问题 ，未来也会基于CaaS云提供服务 , 二 研发体系的敏捷化与标准化问题。我们正在考虑根据计算需要实现弹性伸缩，容器化是一个助力。", "Q：请问为什么Elasticsearch，而没有选择Solr呢?", "A：在有索引下，ES性能会要好一些，而且它原生分布式支持，安装配置也简单。", "Q：代码没有打包进镜像中是出于什么原因?", "A：这样部署运行会更灵活，我可以代码放到本地，也可以上传到实例中。代码提交比较频繁，执行环境变化却不大，只替换变换的部分部署会更快速。主要是为了适应目前这种部署方式。", "Q：爬虫容器如何调度，是分布式吗?", "A：是分布式的，这个是按时间定时运行的，Rancher提供的crontab，爬虫程序提供执行入口。", "Q：HBase主键设计依然没有解决热点问题?", "A：确实未完全解决，基于时间序列的暂时未找到更好的rowkey设计方法;把他分成24小段，加入时间，单独对每段来说，它是按时间排序的，也算是一种折中。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72774"]},
{"module": ["干货教程"], "note": ["文 | 秦路\n我们上一篇《新手入门 | 打造高端的数据报表》教大家如何制作清晰美观的报表以及相应技巧，但是报表是结果的呈现，并不是数据分析的过程。\n数据分析师更多用到的报表是BI。\nBI全称..."], "title": ["数据可视化：深入浅出谈BI"], "content": ["我们上一篇《新手入门 | 打造高端的数据报表》教大家如何制作清晰美观的报表以及相应技巧，但是报表是结果的呈现，并不是数据分析的过程。", "数据分析师更多用到的报表是BI。", "BI全称商业智能(Business Intelligence)，在传统企业中，它是一套完整的解决方案。将企业的数据有效整合，快速制作出报表以作出决策。涉及数据仓库，ETL，OLAP，权限控制等模块。", "今天的学习我们着重于数据分析过程，使用Power BI打造数据分析师Dashboard报表。为了更好的学习和实践，我们依旧会使用《Excel实战篇》的数据进行操作。这是做出的简单作品。", "，提取密码 6x2v 。", "Power BI在微软官网有下载(注不注册无所谓的)，Power BI | 互动数据可视化 BI 工具，只需要下载Desktop桌面操作版，大小约120MB。暂时只推出Win版本。", "大家如果在下载过程中出现CAB文件损坏错误，大概是某软哪方面又出错了，可以换浏览器下载，也可以下载中文繁体版。Microsoft Power BI Desktop。反正我是一直报错。", "BI工具主要有两种用途。一种是利用BI制作自动化报表，数据类工作每天都会接触大量数据，并且需要整理汇总，这是一块很大的工作量。这部分工作可以交给BI自动化完成，从数据规整、建模到下载。", "另外一种是使用其可视化功能进行分析，BI的优点在于它提供比Excel更丰富的可视化功能，操作简单上手，而且美观，如果大家每天作图需要两小时，BI会缩短一半时间。", "BI作为企业级应用，可以通过它连接公司数据库，实现企业级报表的制作。这块涉及数据架构，就不深入讲了。", "Power BI是微软家的。如果大家熟悉Excel，应该会知道微软推出的Power Query、Power Pivot、Power View和Power Map，是Excel上非常强大的四个插件。Power BI则是微软将它们作为集合推出。", "Power Query是用于数据提取、整合、搜索的插件。它偏向数据模型的建立，而不是单元格的使用。", "Power Pivot是数据透视表的高级应用，使用DAX能进行大量的科学计算。性能方面，比Excel函数要快两个量级，百万级的处理不成问题。", "Power View是图表的高级应用，实现了过滤、联动、拖拽等功能。", "Power Map是可视化地图。", "如果大家熟练掌握以上四个插件，那么在Excel上也能实现部分BI。毕竟Excel是企业中人手一款的工具，和BI相比有轻量级的好处，虽然数据分析师需要掌握的工具更多。", "市面上有很多丰富的BI工具，Tableau，QlikView，BDP等，各有侧重，也各有价格。但是操作过程都是相似的，大体分为五个步骤：数据源读取、数据清洗、数据关联、图表制作、Dashboard整合。熟悉了其中一个，再学会另外的就不难。", "因为我工作用的BI是私有化部署到服务器，直接连接生产环境的，演示不方便。所以才用Power BI演示，实际我也说不上熟练。", "我们打开Power BI，它会让我们登录，不用管它。", "界面和Office软件比较接近。上面是操作工具项，左侧栏是导航栏。", "Power BI 的左侧导航栏对应三个模块：仪表板、报表和数据集。仪表板或报表需要数据才能操作，我们先读取数据集。", "点击工具栏的取得资料(奇怪的翻译)。", "Power BI支持各类丰富数据源(市面上绝大部分BI都支持，只是读取方式略有差异)，除了Excel和CSV 文件，它还支持Acess、SQL数据库、Hadoop/HDFS、Spark、第三方API等。", "这是新手教程，连接CSV即可，选择载入练习数据DataAnalyst。", "这里可以针对数据编辑，先略过，选择载入。", "自动跳转到数据报表页，数据报表(Report)是数据规整和清洗过程。", "大家还记得实战篇中演示的数据清洗吗?之前我们体验了一遍Excel函数清洗的过程。这次需要用BI再进行一遍清洗。数据清洗是分析师最蛋疼且耗时持久的工作，没有之一。", "Power BI有一个高级功能叫DAX(Data Analysis Expressions)，它是整个 Power BI 使用的公式语言。", "DAX近似Excel函数(大多数第三方BI，函数均接近Excel)，故它针对新手非常友好。如果大家已经熟悉Excel函数，上手速度会很快。基本上函数名字都一样，如果不熟悉，可以查阅官网提供的文档。", "我们先清洗报表中的薪水salery，和实战篇过程一样，需要将其拆分成两个新列，并且计算平均值。", "点击模型项的新建资料行(这里的翻译应该不对，应是column列，后文我都用列表示)，此时新增加的列没有任何内容。我们需要做的操作就是以salery生成两列。", "这里需要用到DAX。当成函数使用它就行，不过Excel是单元格级别的引用，而DAX中的任何引用、计算、汇总等，都是以列为单位的。", "=’Table Name'[ColumnName]", "这是最简单的引用，Table Name是我们这张报表的名字，我载入的csv叫DataAnalyst，那么报表就叫做DataAnalyst，ColumnName是我们需要引用的列，名字叫做salary。下图公式就是范例。", "如果表名中有空格，需要加引号，如果没有则不需要。如果是跨表引用，TableName是必须的，否则只需要ColumnName。DAX支持自动填充，可以通过模糊输入+回车快速输入。", "我说过它近似Excel，那么Excel加减乘除的习惯可以直接使用在上面。", "=’Table Name'[ColumnName1]+’Table Name'[ColumnName2]*3", "接下来继续清洗步骤，我们查找k所在的字符串位置。", "=search(“k”, DataAnalyst[salary],1)", "利用left函数截取工资下限。", "=left(DataAnalyst[salary],search(“k”, DataAnalyst[salary],1)-1)", "搞定。资料行重命名为bottomSalery", "接下来是工资上限topSalery，使用”-“截取的时候报错了。", "=search(“-“, DataAnalyst[salary],1)", "检查一下发现原来是有“10K以上”这类字符串。DAX查找不到“-”，这时需要返回一个出错时表达的值。因为10k以上的描述无法确定工资上限，那么我们就把返回的值限定为bottomSalery。", "在这里请记住，DAX的容错性比Excel低，只要DAX中有一行返回Error，那么整列都是Error。我们需要用Iferror函数保证容错性。", "这里给出topSalary的计算，比较繁琐。", "topSalery = IFERROR(mid(DataAnalyst[salary],SEARCH(“-“,DataAnalyst[salary],1)+1,LEN(DataAnalyst[salary])-SEARCH(“-“,DataAnalyst[salary],1)-1),DataAnalyst[bottomSalery])", "之后新建一列使用(DataAnalyst[bottomSalery]+DataAnalyst[topSalery])/2 计算该岗位的平均工资。", "大家看到这里，是不是觉得DAX公式非常长?新手可以多增加辅助列来进行计算。", "Excel中有比较方便的分列功能，那么Power BI中是否拥有呢?答案是肯定的，右键点击列，选择编辑查询选项。", "这里依旧吐槽翻译。分割资料行就是我们熟悉的分列功能。选择自定义，用“-”即可完成分列(原始数据会被拆分，所以建议先复制一列)。", "实战篇提到过，我们的北京数据是有重复值的，那么我们通过positionId这职位的唯一标示，来删除重复项。右键点击移除重复项目即可。", "我们再看一下查询编辑的其他功能。", "可以选择多个字段进行分组。对结果进行求和、计数等操作", "如果是订单、用户行为、用户资料等大量数据，一般会以分组形式进行计算。不同分组字段，会生成不同的维度，像范例中的城市、工作年限，教育背景都是维度，也是图表的基础。如果生成的维度足够多，我们能利用维度组成数据模型，这是OLAP的概念。", "除此以外，也能利用过滤直接筛选数据。我们选择出含有数据分析、分析的数据。排除掉大数据工程师等干扰职位。", "这里支持多条件复杂逻辑筛选。", "到这里，我们已经完成实战篇中的清洗过程中，我这次简单化了。以上步骤都能通过右侧的套用步骤还原和撤销。这里不会出现bottomSalery这类列。", "之后选择工具栏的关闭并套用，报表数据就会更新。最后数据2300多行。", "通过数据查询和报表DAX公式，我们就能完成数据清洗和规整的步骤。主要思路是：移除重复值、过滤目标数据、清洗脏数据、数据格式转换。", "我们工作中会用到很多数据，不可能依靠一张表走天下。若是在Excel中，我们经常用Vlookup函数将多张表关联汇总。Power BI则用拖拽关联数据，更方便。一般是先关联再清洗。", "因为我的数据只有一张表，用不到关联，以官网截图为例。", "很简单，用拖拽将Product的manufactureId和Manufacturer的manufactureId关联，我们可以理解成做了vlookup引用，也可以想成SQL的Join。", "分析会涉及到很多复杂因素，这些因素相关的数据不会安安静静给你呆在一张表里，而是不同的表，所以需要用到数据关联。", "数据关联在学习到SQL后会更加清晰，这是SQL的核心概念之一。", "进入图表设计阶段，点击侧边栏第一个项。BI比Excel好的地方在于，它只要拖拽就能设计和生成。", "点击任一图表，画布上会自动生成图形，要切换图表类型直接点击其他即可。我们把城市和平均工资拖拽到视觉效果下的栏目，它会自动生成图表。不同图表需要的维度、轴都不一样，具体按提示进行。", "视觉效果下有设计选项，可以将图表调整的更美观，这里不详细介绍了。", "点击新增视觉效果(继续吐槽翻译)，可以继续在画布上增加图表。绝大部分BI，都是支持联动的，所谓联动，通俗讲，就是点击图表上的维度元素，其他数据也会按此维度相应变化。", "上图就是一个很好的联动例子，点击城市维度的北京，其他图表都变了，平均工资由14.23变成15.23。而学历则变成突出显示，显示出了北京的博士们薪水远高于平均水准。", "图表联动带来更好的数据洞察，将不同数据维度的组合和选取，为分析师带来决策能力的提升。当然我最喜欢的特点之一是省时间。", "通过不断的图表组合，就能生成数据分析师自己的分析画布。这块画布叫作Dashboard。当然图表好看与否，就取决于分析师的设计品味了(这个我教不了你们，哈哈)。", "如果维度过多，大家可以插入视觉选项中的交叉筛选器，添加过滤功能，常用于精细化的分析，例如时间维度。图表右上角按钮，还能选择导出数据，导出一份该图表的csv数据。", "我们也能将其发布到网上，作为同事和企业间协作，或者手机端浏览数据用。当然这里需要注册账号，就看大家意愿了。", "更多功能留待大家学习，到这里，Power BI的新手教程就结束了。我列举了常用的功能，不知道大家有没有从Excel图表水平跃升到一个新阶段，大家可以自己拿数据做图表报告作为分析师行业的敲门砖。如果还有疑问，就借助官网文档学习，BI作为一个领域，它值得数据分析师深入。", "以下是一些补充：", "因为时间的关系，我没有讲解更多的样式设计内容。大家可以去官网下载范例，含有原始数据练习。主要是学习他人的报表汇制思路。", "另外Power BI的图表偏少，类似标靶图、箱线图都没有。不过官网有各类图表下载。搜索pbiviz即可，没有中文。", "Power BI在它内部已经集成了R语言，没错，就是统计学中的R语言。如果你觉得视图功能还不够强大，那么我们可以利用R来绘制图表，甚至借助R做回归分析等。当然R是第七周的内容。这里只以官网截图为例。", "BI很重要的一个功能是数据更新，它是报表自动化的基础，它通常和SQL关联。我们使用CSV，只能往里面黏贴数据更新，还是繁琐了些，只属于半自动化。这将在学会SQL后解决。", "36大数据(www.36dsj.com)成立于2013年5月，是中国访问量最大的大数据网站。36大数 据(微信号:dashuju36)以独立第三方的角度，为大数据产业生态图谱上的需求商 、应用商、服务商、技术解决商等相关公司及从业人员提供全球资讯、商机、案例、技术教程 、项目对接、创业投资及专访报道等服务。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73989"]},
{"module": ["干货教程"], "note": ["\n优秀的API设计话题，在很多团队涌现，这些团队正在努力完善他们的API策略。\n在之前发布的博客上，我简要的讨论了API设计的重要性。一个设计良好的API应该包含那些好处：你的API应该能提高开..."], "title": ["API 设计的最佳实践"], "content": ["优秀的API设计话题，在很多团队涌现，这些团队正在努力完善他们的API策略。", "在之前发布的博客上，我简要的讨论了", "。一个设计良好的API应该包含那些好处：你的API应该能提高开发者经验、方便的快捷文档和高可用性。", "但优秀的API设计究竟应该怎么做？在这个博客中，我将详细的介绍一些RESTful  API的最佳设计。", "一般来说，一个有效的API设计遇有以下特点：", "为说明下面列出的概念， 我会以一个照片分享的应用程序举例。 该应用程序允许用户上传照片， 以拍摄地点和心情标签描述照片特征。", "资源是REST概念的基础。 一条资源是一个重要对象，它本身可以被引用。 一条资源含有数据，与其他资源的关系，以及操作方法，以允许访问和处理相关信息。", "一组资源被称为一个集合。 集合和资源的内容取决于你的组织和消费者的需求。 例如，如果你认为，市场获得了你的产品用户群的基本信息会受益， 那么，你就可以将此作为集合或资源。", "统一资源定位器（URL）确定了资源的在线位置。 URL指向你API资源存在的位置。 基URL是这个位置的一致部分。", "在照片分享应用一例中， 我们可以公开用户数据，只要用户通过集合和资源使用该应用程序， 经由适当的URL访问。", "URL应该是整洁、优雅和简单的，这样开发人员更易在他们的web程序中使用你的产品。 很长且难以理解的URL不仅看起来很糟糕，而且在记录时容易出现错误。所以使用名词应该是很好的。", "没有规定让资源名词使用单数或复数，但是在如果是集合的话使用复数无疑很好的。 具有相似的资源和集合分别保持它们的一致性是较好的做法。", "保持这些名词的自解释性，有助于开发人员了解从URL描述的资源, 这最终会使他们使用你的API。", "回到照片分享应用程序，它拥有返回集合的公共API /users 、/photos。注意到它们都是复数名词了吗? 它们也是自描述的，我们可以推断出 /users 和/photos分别是获取产品注册的用户信息和分享的照片。", "所有资源都有一组方法，可以对它们进行操作由该接口暴露的数据。", "REStful APIs包含主要的HTTP方法，其良好的定义和独立的功能能够应对所有资源。这里是RESTful API里常用HTTP方法列表，这些方法定义CRUD如何操作资源和集合。", "(如果你想了解PUT和PATCH的不同，", ".)", "虽然在URL中使用动词也不错，但是GET, PUT, POST, DELETE操作已经用于描述了资源的操作，因此在URL中使用动词代替名词会显得比较混乱。", "在照片分享app中，/users 和/photos 是一个端点，API的终端消费者可以更直观的使用RESTful CRUD 进行上述的操作。", "在他们使用你的产品时向开发人员提供良好的反馈，对于提升使用率和用户维持率是很好的。每一个客户端的请求和服务端的响应都可以看做是一个消息，在理想化的RESTful 生态系统中，这些消息必须是自描述的。", "良好的反馈对于实现的验证是积极的，错误实现产生的消息可以帮助用户调试和纠正使用产品的不正确方式。对于API, 错误信息是使用API上下文的良好方式之一。", "调整你的错误与标准HTTP代码一致。客户端引发的错误应该使用400类型错误，如果是服务端的错误应该使用500类型来响应。 一个对资源操作成功后应该返回一个200类型的响应。", "有很多的响应代码。想了解更多, ", ".", "一般来说，使用你的API时有三种可能就结果：", "成功解决客户使用你的API时遇到的问题将大大改善开发人员使用体验和防止滥用API。 详细描述这些错误，同时保持简明和整洁。在错误消息中提供足够的信息有助于用户解决它们的问题，如果你觉得需要更加详细的信息，最好另写文档并在此处提供链接。", "一个良好设计的API应该有响应示例，以明白是否成功调用了一个URL。响应示例应该简单、简洁和易于理解的。 一个好的经验法则是：帮助开发人员在5秒内理解一个成功的响应。回到我们的照片分享应用，我们定义了 /users 和 /photos URL。 /users 应该易数组的形式提供所有注册的用户，并且包含用户名称和加入日期属性。", "你可以在Swagger（OpenAPI）中使用 ", " 来定义你的API，详述如下：", "注意数据类型和实例的每个响应项注释，最终用户期望的是一个成功的GET调用。成功响应后最终用户将接收的JSON看起来如下：", "如果用户调用了这个方法，就应该获得包含上述数据和一个说明正确调用了的200响应状态码。 同样的,，一个不正确的调用应该产生适当的400 或500 响应状态码和其它相关信息，以帮助用户更好地操作。", "你试图开放的数据包含大量的有利于用户的属性，这些属性描述的基本资源和隔离特定信息，可以通过适当的方法来操作。", "一个API应该努力提供完整的信息、数据和资源来帮助开发者以无缝的方式与之整合。", "然而， 完整意味着对你的API考虑通用使用情况。可能会有许多这样的关系和属性,单独为他们定义资源不是好的做法。", "资源暴露的数据量也应考虑。如果你暴露得太多，这会对服务器产生消极的影响，特别是对于负载和性能。", "上述情况和关系是在设计的API时需要重点考虑的因素，可以使用适当的参数来处理。你可以扫描查询参数中limit参数来限制响应数据量，或者让客户端使用路径参数来隔离数据。", "以我们的照片分享应用作为示例。", "它可以用于开发者获取在特定位置和特定主题标签中共享的所有照片的信息。 您可能还想要通过将每个 API 调用的结果数限制为 10，以减轻服务器负载。 如果最终用户想要找到波士顿的所有照片，并使用 winter 标签，则请求将是：", "注意现在的复杂性如何被简化为一个与查询参数的简单关联。如果您想根据客户的请求提供特定用户的信息，则调用：", "这里的 kesh92 是一个指定的用户名，它会返回该用户的地点和加入日期。", "这只是朝着API完备性进行参数设计的几种方式，可以帮助用户更直观的使用你的API。", "最后， 有疑问时，暂时离开它。如果你重新思考了某个资源或集合的功能，应该把它放入下一个迭代中。开发和维护API是一个持续的过程，等待正确的用户反馈可以构建一个健壮的API，以帮助用户以创造性的方式集成和开发应用程序。", "没有一个API设计方法能够一劳永逸解决所有组织的问题。上述的建议仅仅是参考意见和建议，能否采用取决于你的用户情况和需求。", "一个API设计至关重要的主要原因是你的API能帮助到终端用户，他们的需求就是贯穿你整个API设计的指明灯。", "原作者：Keshav Vasudevan ，译者： luke , 小吕 , 卞卞 , abel533", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72940"]},
{"module": ["干货教程"], "note": ["\n文 | 张云松\n最近几年这波在资本撬动的互联网金融火爆的浪潮中，大数据和数据科学在互联网金融行业中极大的提升了应用价值。数据分析师不再是苦逼的跑数的、出报表的，摇身一变成了风控模型..."], "title": ["互联网金融中的数据科学"], "content": ["文 | 张云松", "最近几年这波在资本撬动的互联网金融火爆的浪潮中，大数据和数据科学在互联网金融行业中极大的提升了应用价值。数据分析师不再是苦逼的跑数的、出报表的，摇身一变成了风控模型专家、数据科学家。尤其是大数据风控、大数据征信领域，已及现在大热的金融科技概念(Fintech)，一片火热的场景，数据挖掘机器学习相关专业同学的薪资也翻番上涨，越来越多的计算机和统计领域同学加入互联网金融行业。", "面试中发现很多同学的dream job都是我要做机器学习相关工作、我要做算法、我要做模型……但其实以一个从业者角度看，我们大量的时间还是在做数据理解、数据处理、大量的特征工程、重复验证特征、不停的调参、不停的在做实验，我对modeler的定义基本就是半个蓝领。不过在这个学习和工作的过程中，倒逼我们将一些学术论文中的方法和各种五花八门的算法可以真正有机会应用到商业领域并且产生价值。", "本文分享一些互联网金融从业者日常工作中实际用到与数据科学相关的内容。由于日常工作中涉及数据和策略非常敏感，本文中不会透露具体业务中的产品策略，只会对一些思考和方法进行介绍。下面涉及的模型和算法原型内容我们大部分是用R和python实现的，同时为了方便日常工作也开发了自用的R包解决数据处理、特征工程、可视化、自动化模型报告等繁琐的工作。", "由于互联网金融产品形态非常多，下文主要介绍目前很热的小额微贷(cash loan)的在线授信贷款产品中数据科学的应用，在大量用到了Fintech中可能涉及的各种技术及架构，包括传统数据挖掘和机器学习算法、自然语言处理(NLP)、卷积神经网络(cnn)等。在线授信贷款产品也是互金产品中路径最长，最复杂的产品。类似产品形态为蚂蚁借呗、京东白条以及各种手机上的贷款app，如拍拍贷、宜人贷、原子贷、手机贷等。", "在线小额微贷产品有别于传统个人信贷产品，用户不用提交一堆纸质材料，也不用上门面签，也不用等几天甚至十几天才能知道审批结果，用户只需要在手机app或微信上完成贷款申请，十几分钟甚至数分钟就能获得贷款额度，很快就能放款成功，极致的互联网贷款体验。用户整个产品流程大体为，线上寻找贷款à比较各类贷款产品à申请贷款à审批à放款à还款，整个过程完全线上化，只要你有一台智能手机。", "在线贷款行业是一个大蛋糕，尤其对骗子来说，不管是线下的贷款中介代办包装还是线上的盗号刷单等黑产从业者，都盯上了快速不需面签的在线贷款产品，各种身份伪冒、伪造数据、篡改手机信息等欺诈方式及技术随之而生，甚至有些技术公司和黑客专门为欺诈提供技术服务。可以说反欺诈能否做好是在线贷款行业的一条生死线。", "简单介绍下欺诈模式，可以分为一方欺诈和三方欺诈，一方欺诈通常指骗子来申请贷款后没有还款意愿造成违约，三方欺诈指欺诈分子借用冒用他人身份或协助他人伪造申请信息进行骗贷。下面重点介绍下数据科学在识别三方欺诈中的一个应用。", "我们存在一个基本的假设，骗子的紧密关联人是骗子概率大，正常还款的用户的朋友正常还款的概率大。同时我们要对不良的贷款中介进行识别，因为中介骗子会帮很多还款意愿不强的人通过提供虚假、伪造、包装申请信息的方式进行骗贷，甚至还会教申请人如何应对贷款审查。通过中介(或者本身就是团伙)进行集中贷款申请的风险非常高，是一种常见的欺诈类型，分析发现社交网络分析和其他交叉检验方法能有效识别上述欺诈模式。", "我们定义用户和用户之间如果共用某些核心信息那么他们之间就存在紧密联系，这些核心信息可以是手机设备、电话号码、身份证、银行卡号、邮箱等。以这些信息作为点，信息之间的关系作为边就可以构造出类似下面的图网络。", "如上图，图中有两个用户通过手机申请贷款，一个放款成功，一个放款失败，通过用户申请中提供的信息，将其核心信息构建成一个网络图，可以看到两个用户一共关联到3个手机号，3部手机设备，两个用户是通过一个公用的手机设备联系起来的。上图的真实业务场景是尾号979的用户来申请时，发现与其强相关的用户已经成功放款，并且通过图上的关系已经申请调查出尾号979的用户是之前放款用户的配偶，若批准尾号979的贷款申请则将增加两人整体负债，所以最终审判拒绝掉了这笔贷款申请。", "上面是一个简单例子，真实业务中欺诈与反欺诈是道高一尺魔高一丈的博弈过程，简单的反欺诈策略很容易被真正的欺诈分子发现并规避，简单策略的效果会不断下降。事实上欺诈很难被完全解决，反欺诈的一个重要思路就是不断的提高欺诈分子作弊的成本，并且保证反欺诈策略准确性情况下使反欺诈策略能召回更多的欺诈行为并且使整个反欺诈体系更智能。", "这种思路下同样是通过社交网络反欺诈，我们需要更全面的描述每个用户之间的关系，用户之间关系的定义也不止是上述这些强关系，还包括很多弱关系，比如用户间打过电话，用户间是同一单位，用户家庭住址在同一区域，用户之前是QQ好友等，这些更多的关系关系的叠加很容易出现下面类似的用户间非常复杂的关联网络。", "构建图的同时，对每个点还可以赋予不同的属性，这些属性可以用于后续的特征工程提取。举个例子，对图中用户身份证类型的顶点，可以设置多个属性，如是否黑名单、用户资产、是否有房。后续特征工程中就可以根据顶点属性衍生出具体的特征，如一度关联的身份证是黑名单的顶点个数。", "用户关系网络图构建的最终目的是提升欺诈团伙的识别准确度以及实现自动化的反欺诈决策，即提升欺诈识别的效果和效率。我们希望通过社交网络挖掘出用户更多的特征用于反欺诈模型和策略的训练，所以对于这种复杂的用户关系网络图，接下来我们要进行两件事，其一，用户图特征提取;其二，点属性标签的补全。", "每个用户的可以通过手机、身份证等关键信息点，获取单个用户网络的连通图(事实上对10亿级节点的图的查询效率已经非常低了，在实时决策的场景下需要通过算法优化来解决响应时间的问题，比如图入库的锁问题，异常点的查询超时)。对每个点计算其在图中常用的属性特征，比如度、接近中心度、page rank中心度、betweenness。这个过程可以看做是对给定用户，通过图数据进行特征工程。大部分策略和模型的效果往往由特征工程的质量决定，甚至特征工程方法也成了各家公司不可泄露的核心内容。", "举几个例子，除了上面简单的点中心度相关的直接特征还能直接想到非常多的特征，比如用户n度关联点的关联手机号数、用户关联到的设备号占所有关联点的比例、用户关联的黑名单身份证号数等等。细心的同学可以看出上述举例的特征计算大部分可以实现标准化，通过开发单独特征工程模型实现特征的自动计算，这样能极大提升模型开发的效率。", "在策略分析和特征计算中，我们需要很多点的标签属性，比如对某一身份证的属性，是否是黑名单，是否有房，是否信用卡额度超过3万等等。但是实际业务中往往大部分用户标签属性是缺失的，比如用户申请到一半就流失了，用户最终放弃了，我们都没法准确收集这些标签。怎么办?我们通过图相关的社群发现算法进行标签补全，比如最常用标签传播算法LPA(Lable Propagation Algorithm)，还有类似的算法，比如SLPA、HANP、DCLP等等。", "虽然通过LPA可以快速补全标签属性，但很快我们就发现补全后的效果不一定理想。其中有两个业务产生的数据问题。其一，噪音数据造成很多奇异点使非相关的用户关联到一起，造成数据失真。一个常用的场景就是很多用户都会拨打10086，很多人都会被同一400的骚扰电话骚扰，那么这就导致现实生活中本没有紧密关联的用户被关联到一起;其二，由于用户的贷款是较独立的事情，所以每个用户的图规模较小，没有足够的已标记数据对其进行标签传播，造成最终标签传播的覆盖率降低。类似下图中的情况。", "小额微贷在线贷款产品的核心竞争力就是数据化决策的效果和效率，整个业务中策略和模型应用非常普遍，基本上在整个用户生命周期中策略和模型都可以体现并发挥价值。如下图：", "由于近两年大数据和风控技术的方法论已逐渐普及，这里不对常用的技术进行介绍。我们还是从一个从业者角度分析下工业界模型开发中最关心的几个问题：", "业务上线前没有信贷表现样本，所以授信决策通常由信贷专家的经验决定。但如果一直依赖人工判断，人为判断的不确定性和人工依赖将严重影响业务的扩展，也无法通过互联网快速复制扩展业务。", "所以我们希望业务初期能将人为经验进行一定量化总结，业内常用层次分析法(AHP)这种定性加定量的方式建立风控评分。层次分析法通过将人工授信决策的复杂思考抽象成一个个两辆决策比较，最终将授信决策形成一个量化的评分表。在传统信贷产品中AHP方法在业务初期很长一段时间并不比定量模型差很多。", "在业务发现中期，有了一定放款用户作为模型训练的样本，可以使用真实违约用户作为模型的正样本，但对于信贷业务中违约率通常在5%以下，建模时通常面临不平衡样本(imbalance)问题。实际建模中通常可以用过采样(oversampling)或欠采样(undersampling)处理样本。具体而言，即对违约样本进行重抽样，或从非违约样本中进行抽样。", "在信贷业务中，不止正负比例问题，还有由于总放款量不足，违约样本过少的问题，这种情况经常被称作低违约资产组合 (LowDefaultPortfolio，LDP)。过采样的方法在违约样本过少情况下对模型效果的提升还是非常有限。除了bootstrap的采样方法还可以尝试SMOTE进行采样，不过我实际工作中尝试在信贷风控模型中SMOTE，发现其表现并不稳定。", "实际建模中我们会通过其他替代方法增加正样本，在金融领域可以通过经验丰富的信贷专家对拒绝用户进行人工标注加入模型正样本，或通过用户信息相似度聚类方法对表现期不充分的用户进行标签重置;", "被信贷专家拒绝的申请虽然没法完全证明放款后一定会违约，但其风险通常比被信贷专家通过的高很多，在建模中可以把这些拒绝样本放入模型中作为正样本;另外也可以在建模时损失函数定义中对正负样本赋予不同的权重，在真正业务中很少采用，基本信贷领域的模型还是依赖特征来增强模型效果，对算法的改进往往收效较少。", "虽然这些方法的真实效果不如真实用违约样本进行建模效果好，但在细节模型调优中都是常用的尝试方法。", "特征工程做的好坏是所有模型工作者保持饭碗的有效手段。在线授信领域对特征的挑剔更上一层，由于大部分预测模型都是用历史数据去预测未来几个月甚至1年后发生的违约事件，所以对特征的效果、特征稳定性、可解释性中需要不断平衡。", "每个模型都需要做大量的数据特征处理，为了提升效率最好把常用的特征工程自动化，并开发响应算法自动生成特征。举个特征工程的例子", "上图中是用户提供的信用卡消费账单数据有用户id、交易金额、交易时间、交易描述。上述只有4个字段的用户数据如何进行特征工程?如下几方面入手考虑：", "对于有用户重复交易的数据，RFM是最有效的特征衍生方法。通过recency, frequency, monetary三个维度衍生出非常多的特征，比如最近1周交易金额次数、最近3个月交易金额平均值等等特征。", "上述交易描述(description)字段中是非结构化的文本，每笔交易可以被赋予一个或多个标签分类，比如通过交易描述中的内容，可以将交易分类为分期交易、取现交易、消费交易。通过交易描述可以产出不止一类的分类体系，比如还可以将交易分成线上交易(支付宝、财付通等)、汽车交易、生活消费交易等等。这些NLP分出的标签可以作为一个分类，然后应用于上述RFM的特征衍生环节，比如最近1周取现交易次数、最近3个月线上交易平均值等。", "对交易描述进行的文本分类会用到一些常用的自然语言处理，比如词聚类(word2vector)、主题词聚类(LDA)、朴素贝叶斯分类等。同时也会用非算法的分析方法，比如通过对分词后的词统计分析的方法给出非常实用的分类。", "无论采用哪种文本分类方法，关键在于这类基础标签的效果如何评估。由于我们最终需要用这些分类标签用于分析用户的违约概率，所以我们可以采用一些先验的方法，使分类标签可以对用户违约风险有比较显著的区分力。", "在实际业务中优化文本分类产生的特征能非常有效的提升，这点很容易理解，文本中挖掘的特征其实是用户真实行为习惯的描述，一旦数据足够充分这种方法衍生的特征就是在刻画一个自然人的金融及生活模式。", "RFM模型中通常只对交易金额进行简单的算术和统计计算，事实上对用户每个用户按时间排序的交易数据，可以看做是一个时间序列。对于交易金额组成的一个时间序列，我们抽取每个用户组成的时间序列中季节性系数、白噪声、时间序列中自回归模型的模型系数。这些值都可以认为是描述一个用户金融行为的特征!", "由于互联网金融业务场景中获取用户强相关的数据相对有限，对于建模来说很多时间就是在不断的无脑挖特征，验证特征，再重复……", "从事数据相关行业十余年来，看到的其实数据科学真正能用充分在工业领域发挥决定性作用的少之又少。从互联网金融到金融科技，越来越多企业在研究将数据科学和实际业务结合起来充分发挥数据的价值进行金融创新。数据化决策未来将会影响整个互联网金融行业，其价值不只是反欺诈和授信模型这些应用点，还将决定整个金融产品的设计和营收模式。希望更多的数据科学家将数据在实际商业场景中发挥出最大的价值。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72660"]},
{"module": ["干货教程"], "note": ["\n大家好，今天带来的是企业数据质量管理实战系列之一的《数据质量管理的核心要素和技术原则》。\n主要大纲：\n\n数据质量管理的三个要素\n数据质量管理的技术关键点\n数据质量管理实战\n\n“十三五”..."], "title": ["企业数据质量管理核心要素和技术原则"], "content": ["大家好，今天带来的是企业数据质量管理实战系列之一的《数据质量管理的核心要素和技术原则》。", "主要大纲：", "“十三五”，规划提出了国家的大数据战略，指出了企业实现以数字化驱动业务发展，实现数据开放共享，创新业务发展的新思路。现阶段大中型企业已经开始了数据化运营的实践。", "在这个环境和趋势中，数据是得以实现整个规划布局的前提。", "在将数据作为资产的前提下，企业的运营需要准确的、完整的、及时的、高价值、高质量的数据。", "数据是企业数字化转型的核心要素，企业的决策者根据数据背后所反映出来的现象或趋势。分析并洞察出其背后有价值的信息，进而在决策和行动中，赢得先机，做出正确的判断。", "企业产品生产过程中数字化能力已经逐步取代传统的产品制作模式，以生产男式西装为主红领集团为例，用户在手机APP上下单之后，测量师会到你家里做定式测量，然后在版型库里做自动设计，自动排产之后就能生产使用了。整个过程都是基于高质量的数据驱动的，数据质量成为业务的生命线。", "在企业数字化转型的进程中，数据的质量成为了一个制约因素。", "数据能发挥价值的大小依赖于其数据的质量的高低，高质量的数据是企业业务能力的基础。", "但劣质的数据还不如没有数据依据经验的进行决策，通过错误的数据分析出的结果会带来灾难。", "数据质量问题产生的原因有很多方面，比如在技术、管理、流程方面都会碰到。企业要在把这些方面的数据质量问题都管控到，监控好，才能从整体上提高质量水平。", "今天我们先谈一下在技术领域中该注意哪些。", "在技术问题域中要提高数据质量水平，数据的梳理、数据规范以及数据生命周期是三个关键的要素。", "我们先来看数据梳理，数据梳理有两个目的：", "在项目实践中，对数据梳理核心的一环是对企业内的元数据梳理，对元数据的梳理能解决理清企业数据现状和明晰核心基础数据，是数据梳理的核心工作内容，被一些企业形象的比喻成摸家底工程。", "如上图所示，我们对企业交易数据进行剖析发现，交易数据的组成其核心是由主数据、参考数据和元数据组成。如果每一类数据的元数据在名称和格式上存在差异的话，那么数据的质量必然不高。", "例如：在交易数据里面有产品名称和描述这两个元数据，而在主数据中表述这两个对象的名称是产品名称和产品描述，若两边的字段类型长度也存在不一致，那数据有质量问题那是必然的。", "下面我们介绍数据规范，这里的规范主要指数据标准和数据模型，我们分开介绍。", "如电力行业的CIM模型，电信行业的ETOM模型，指导这电力和电信信息系统的建设，以确保数据质量从开始就能得到保证，还可以为现有应用软件的数据质量水平提供衡量标准。", "它促进企业数据模型落地，也担负着对企业存量系统中关键数据进行标准化的任务。", "企业数据模型在落地的过程中，各部门看待问题的角度不一样，加之有些外购产品的限制，不能很好的按企业数据模型的规范进行落实，导致数据在集成，互联互通的时候碰到数据不一致的问题，数据质量问题随处可见。", "如果系统(项目)模型设计的时候,有一套能遵循的规范，那么就能解决这一问题。数据标准就扮演了这么一个角色，它从企业数据模型中剥离出重要的业务实体，在系统(项目)模型设计的时候，实现了对模型设计是否符合企业数据模型规范的监控和评审，并从管理和认责的角度切入进去，很好的解决了上述问题。", "数据和货币一样，流通起来的价值远远大于它静态的价值。因此我们要对数据生命周期的每个环节进行监控把关，知道在每个环节数据发生了什么变化，才能采取相应的手段来处理质量问题。", "比如在规划阶段，我们要参考企业数据模型进行系统模型的设计，并且要遵循数据标准的规范要求。在获取阶段要重点关注数据的及时性问题，数据传输的问题;", "在存储和共享过程中要重点关注数据的整合问题，数据一致性，完整性问题。", "数据生命周期我们在实践中总结了几个技术原则，在第二部分的分享中给大家做介绍。", "这里，我们针对数据生命周期再介绍一种分析数据质量问题的应用场景，它对我们实施数据质量项目，是一个不错的参考。", "上图是客户信息产生，维护和使用的一个样例场景。", "在图中每个节点是企业中的一个部门，客户信息从左至右流动，在这个场景中我们发现销售部、市场部、客户信息管理部都有维护客户信息的权利，且所使用的系统不统一。", "利用数据生命周期的分析思路，若能在管理和流程上明确数据维护的责任主体，在源头处切入质量监控节点，对关键元数据进行统一，将能有效的解决这些问题，数据质量也能大幅度的提升。", "在技术实践上，如何利用技术能力，帮助企业更容易的实现数据质量的提升，我们在项目中提炼了四项技术原则。", "数据质量一直是企业的痛点，数据质量不高，分析和挖掘无从谈起，但是企业容易出现的问题是只对已经产生的数据做检查，然后再将错误数据剔除，这种方法治标不治本，不能从根本上解决问题。", "要想真正解决数据质量问题，应该从需求开始，将数据质量的服务集成到需求分析人员、模型设计人员与开发人员的工作环境中，让大家在日常的工作环境中自动控制数据质量，在数据的全生命周期中控制数据质量。", "在企业大数据治理过程中，对于大数据生产线中的每个集成点，都需要做数据质量的检查，严格控制输入数据的质量。比如在数据采集过程，集成过程，分析过程等等都需要做检查。", "但在大数据环境中，每个集成点都会有海量数据量流过，把数据逐条检查这种传统方式是行不通的，应该采用抽样的方式，对一批数据做数据质量的检查，来确定这批数据是否满足一定的质量区间，再决定是否需要对这批数据做详细的检查。", "目前企业内的数据主要分为外部数据和内部数据，大数据时代到来让各企业广泛采购第三方数据，第三方数据的质量逐渐成为决定企业数据质量的关键因素。", "对于企业的内部数据，可以通过业务梳理直接获得质量检核规则。但是对于外部第三方数据，需要先对这些数据进行采样，并应用关联算法自动发现其中的质量检核规则，并将这些检核规则持续积累，形成外部数据的检核规则库。", "企业的数据管理需要对整个企业大数据的质量有评判机制，需要能够自动化的对企业数据评分，促进整改。通过控制防止问题数据进入大数据平台。特别对于第三方数据，第三方数据的质量需要建立一定的评估模型，对于第三方数据的质量做一定的控制，从而能够保护企业的投资，使企业购买的数据真正有价值和意义。", "上面介绍了数据质量管理的核心三要素，以及基于三要素在实战中抽取出来的四项技术原则。下面我们看一个完整案例。", "这是一个新建系统从需求到投产的场景案例，数据质量管控和切入共包含五个步骤。", "基于系统建设的业务需求，分析数据标准规范，建立需求和标准的映射。从企业角度通过映射标准了解到需求是解决那个数据主题域的业务问题。数据标准的映射也统一了业务术语，实现了源头出控制质量的目的。", "参考企业数据模型，依据本次业务需求设计出系统(项目)模型，系统模型的核心数据模型来源于企业数据模型。企业数据模型的参考从模型层面避免了，字段类型，字段长度，字段命名不统一，不一致的问题。", "上述两步骤完成了数据生命周期的规划阶段，也按数据管理要素二思路落实企业数据模型。步骤三实现了本次系统建设的元数据管理，附加实现了核查系统元数据是否符合企业数据模型规范，是否符合数据标准规范的工作。", "对涉及数据集成整合的场景，要对数据生命周期中数据传输的几个环节进行数据质量监控和检核，也就是在集成点处进行监控。在此阶段设计到数据质量规则制定的工作内容。实现传输过程中数据质量的把关。", "若本次系统建设导致其他系统发生变更时，需要协同变更，是否发生变更时基于数据梳理实现元数据自动化管理所带来的直接保证。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/73715"]},
{"module": ["干货教程"], "note": ["这是一个来自百度内部培训关于数据分析的、阅读类的PPT，文字说明非常充分，适合刚入门数据分析的朋友进行学习。\n\n主要内容：\n1、什么是数据分析（道）\n1.1 数据分析是什么？\n1.2 什么是做好..."], "title": ["百度内部培训资料PPT：数据分析的道与术"], "content": ["这是一个来自百度内部培训关于数据分析的、阅读类的PPT，文字说明非常充分，适合刚入门数据分析的朋友进行学习。", "1、什么是数据分析（道）", "1.1 数据分析是什么？", "\n1.2 什么是做好数据分析的关键？", "\n1.3 分析要思考业务，尤其是接地气", "\n1.4 分析要言之有物，行之有效", "2、数据分析方法（术）", "3、常见的统计陷阱", "4、PPT蕴含的人生哲理", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72903"]},
{"module": ["干货教程"], "note": ["\n文 | 颜国平  腾讯云-天御系统研发负责人。\n一、背景介绍\n最近1~2年电商行业飞速发展，各种创业公司犹如雨后春笋大量涌现，商家通过各种活动形式的补贴来获取用户、培养用户的消费习惯。\n但..."], "title": ["腾讯防刷负责人：基于用户画像的腾讯大数据防刷架构"], "content": ["文 | 颜国平  腾讯云-天御系统研发负责人。", "最近1~2年电商行业飞速发展，各种创业公司犹如雨后春笋大量涌现，商家通过各种活动形式的补贴来获取用户、培养用户的消费习惯。", "但任何一件事情都具有两面性，高额的补贴、优惠同时了也催生了“羊毛党”。", "“羊毛党”的行为距离欺诈只有一步之遥，他们的存在严重破环了活动的目的，侵占了活动的资源，使得正常的用户享受不到活动的直接好处。", "今天主要分享下腾讯自己是如何通过大数据、用户画像、建模来防止被刷、恶意撞库的。", "“羊毛党”一般先利用自动机注册大量的目标网站的账号，当目标网站搞促销、优惠等活动的时候，利用这些账号参与活动刷取较多的优惠，最后通过淘宝等电商平台转卖获益。", "他们内部有着明确的分工，形成了几大团伙，全国在20万人左右：", "专门制作各种自动、半自动的黑产工具，比如注册自动机、刷单自动机等;他们主要靠出售各种黑产工具、提供升级服务等形式来获利。", "实现手机短信的自动收发，其实一些平台亦正亦邪，不但提供给正常的商家使用，一些黑产也会购买相关的服务。", "他们主要是大量注册各种账号，通过转卖账号来获利;该团伙与刷单团伙往往属于同一团伙。", "到各种电商平台刷单，获取优惠，并且通过第三方的电商平台出售优惠，实现套现。", "这些黑产团队，有三个特点：", "专业化：专业团队、人员、机器来做。", "团伙化：黑产已经形成一定规模的团伙，而且分工明确;从刷单软件制作、短信代收发平台、电商刷单到变卖套现等环节，已经形成完整的刷单团伙。", "地域化：黑产刷单团伙基本分布在沿海的一些经济发达城市，比如，北京、上海、广东等城市，这或许跟发达城市更加容易接触到新事物、新观念有关。", "对抗刷单，一般来讲主要从三个环节入手：", "识别虚假注册、减少“羊毛党”能够使用的账号量。在注册环节识别虚假注册的账号，并进行拦截和打击。", "提高虚假账号登录门槛，从而减少能够到达活动环节的虚假账号量。比如，登录环节通过验证码、短信验证码等手段来降低自动机的登录效率，从而达到减少虚假账号登录量、减轻活动现场安全压力的目的。", "这个是防刷单对抗的主战场，也是减少“羊毛党”获利的直接战场;这里的对抗措施，一般有两个方面：", "1)通过验证码(短信、语音)降低黑产刷单的效率。", "2)大幅度降低异常账号的优惠力度。", "风险学习引擎：效率问题。由于主要的工作都是线下进行，所以线上系统不存在学习的效率问题。线上采用的都是C++实现的DBScan等针对大数据的快速聚类算法，基本不用考虑性能问题。", "风险学习引擎：采用了黑/白双分类器风险判定机制。之所以采用黑/白双分类器的原因就在于减少对正常用户的误伤。", "例如，某个IP是恶意的IP，那么该IP上可能会有一些正常的用户，比如大网关IP。", "再比如，黑产通过ADSL拨号上网，那么就会造成恶意与正常用户共用一个IP的情况。", "黑分类器：根据特征、机器学习算法、规则/经验模型，来判断本次请求异常的概率。", "白分类器：判断属于正常请求的概率。", "我们以黑分类器为例来剖析下分类器的整个逻辑框架。", "总的来讲我们采用了矩阵式的逻辑框架，最开始的黑分类器我们也是一把抓，随意的建立一个个针对黑产的检测规则、模型。", "结果发现不是这个逻辑漏过了，而是那个逻辑误伤量大，要对那一类的账号加强安全打击力度，改动起来也非常麻烦。", "因此我们就设计了这个一个矩阵式的框架来解决上述问题。", "矩阵的横向采用了Adaboost方法，该方法是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些分类器集合起来，构成一个最终的分类器。", "而我们这里每一个弱分类器都只能解决一种帐号类型的安全风险判断，集中起来才能解决所有账户的风险检测。", "那么在工程实践上带来三个好处：", "比如某平台虚假账号集中在邮箱账号，策略就可以加大对邮箱账号的打击力度，影响范围也局限在邮箱帐号，而不是该平台所有的账号。", "模型训练最大的难度在于样本的均衡性问题，拆分成子问题，就不需要考虑不同账号类型之间的数据配比、均衡性问题，大大降低了模型训练时正负样本比率的问题。", "某一个分类器的训练出现了问题，受影响的范围不至于扩展到全局。", "矩阵纵向采用了Bagging方法，该方法是一种用来提高学习算法准确度的方法，该方法在同一个训练集合上构造预测函数系列，然后以一定的方法将他们组合成一个预测函数，从而来提高预测结果的准确性。", "上面讲的部分东西，理解起来会比较艰涩，这里大家先理解框架，后续再理解实现细节。", "大数据一直在安全对抗领域发挥着重要的作用，从我们的对抗经验来看，大数据不仅仅是数据规模很大，而且还包括两个方面：", "数据广度：要有丰富的数据类型。比如，不仅仅要有社交领域的数据、还要有游戏、支付、自媒体等领域的数据，这样就提供了一个广阔的视野让我们来看待黑产的行为特点。", "数据深度：黑产的对抗。我们一直强调纵深防御，我们不仅仅要有注册数据，还要有登录，以及账号的使用的数据，这样我们才能更好的识别恶意。", "所以想要做风控和大数据的团队，一定要注意在自己的产品上多埋点，拿到足够多的数据，先沉淀下来。", "我们的团队研发了一个叫魔方的大数据处理和分析的平台，底层我们集成了MySQL、MongoDB，Spark、Hadoop等技术，在用户层面我们只需要写一些简单的SQL语句、完成一些配置就可以实现例行分析。", "这里我们收集了社交、电商、支付、游戏等场景的数据，针对这些数据我们建立一些模型，发现哪些是恶意的数据，并且将数据沉淀下来。", "沉淀下来的对安全有意义的数据，一方面就存储在魔方平台上，供线下审计做模型使用;另一方面会做成实时的服务，提供给线上的系统查询使用。", "画像，本质上就是给账号、设备等打标签。", "我们这里主要从安全的角度出发来打标签，比如IP画像，我们会标注IP是不是代理IP，这些对我们做策略是有帮助的。", "以QQ的画像为例，比如，一个QQ只登录IM、不登录其他腾讯的业务、不聊天、频繁的加好友、被好友删除、QQ空间要么没开通、要么开通了QQ空间但是评论多但回复少，这种号码我们一般会标注QQ养号(色情、营销)，类似的我们也会给QQ打上其他标签。", "标签的类别和明细，需要做风控的人自己去设定，比如：地理位置，按省份标记。性别，安男女标记。其他细致规则以此规律自己去设定。", "我们看看腾讯的IP画像，沉淀的逻辑如下图：", "一般的业务都有针对IP的频率、次数限制的策略，那么黑产为了对抗，必然会大量采用代理IP来绕过限制。", "既然代理IP的识别如此重要，那我们就以代理IP为例来谈下腾讯识别代理IP的过程。", "识别一个IP是不是代理IP，技术不外乎就是如下四种：", "扫描IP是不是开通了80,8080等代理服务器经常开通的端口，显然一个普通的用户IP不太可能开通如上的端口。", "开通了HTTP代理的IP可以通过此法来识别是不是代理IP;如果带有XFF信息，该IP是代理IP无疑。", "如果带有Proxy-Connection的Keep-alive报文，该IP毫无疑问是代理IP。", "如果一个IP有的端口大于10000，那么该IP大多也存在问题，普通的家庭IP开这么大的端口几乎是不可能的。", "以上代理IP检测的方法几乎都是公开的，但是盲目去扫描全网的IP，被拦截不说，效率也是一个很大的问题。", "因此，我们的除了利用网络爬虫爬取代理IP外，还利用如下办法来加快代理IP的收集：通过业务建模，收集恶意IP(黑产使用代理IP的可能性比较大)然后再通过协议扫描的方式来判断这些IP是不是代理IP。每天腾讯都能发现千万级别的恶意IP，其中大部分还是代理IP。", "实时系统使用C/C++开发实现，所有的数据通过共享内存的方式进行存储，相比其他的系统，安全系统更有他自己特殊的情况，因此这里我们可以使用“有损”的思路来实现，大大降低了开发成本和难度。", "数据一致性，多台机器，使用共享内存，如何保障数据一致性?", "其实，安全策略不需要做到强数据一致性。", "从安全本身的角度看，风险本身就是一个概率值，不确定，所以有一点数据不一致，不影响全局。", "但是安全系统也有自己的特点，安全系统一般突发流量比较大，我们这里就需要设置各种应急开关，而且需要微信号、短信等方式方便快速切换，避免将影响扩散到后端系统。", "适应的场景包括：", " ", " ", "风险学习引擎包括两个部分，线上和线下两部分：", "线上：自己利用c/c++来实现。", "线下：涉及利用python开源库来做的，主要是一些通用算法的训练和调优。", "我们做了部分改造，主要是DB的引擎方面。", "白分类器主要用来识别正常用户，黑分类器识别虚假用户。", "先通过正负样本进行训练，并且做参数显著性检查;然后，人工会抽查一些参数的权重，看看跟经验是否相符。", "相比安全，风控的外延更丰富，更注重宏观全局;针对一个公司来讲，风控是包括安全、法务、公关、媒体、客服等在内一整套应急处理预案。", "如果识别错了正常用户不会被误伤，但是会导致体验多加了一个环节，如弹出验证码、或者人工客服核对等。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72618"]},
{"module": ["干货教程"], "note": ["\n文 | 王洪磊， 朱晟君\n随着互联网与移动互联网的兴起， 海量终端设备带来数据量的大幅增加，对于存储的需求也随之增长。 传统存储的集中式架构在性能和扩展性上存在着天然的缺陷， 难以适应..."], "title": ["为何说国内企业级存储市场将迎来快速增长？| 上"], "content": ["文 | 王洪磊， 朱晟君", "随着互联网与移动互联网的兴起， 海量终端设备带来数据量的大幅增加，对于存储的需求也随之增长。 传统存储的集中式架构在性能和扩展性上存在着天然的缺陷， 难以适应下游客户海量存储的需求场景。在互联网化浪潮推动下， 软件架构的升级与硬件介质更新推动存储行业迈入新的阶段：", "a）虚拟化。随着数据量指数级增长， 互联网化思维要求数据中心能够很好地解决传统存储架构存在的数据孤岛问题，虚拟化应运而生，进而推动传统存储产业发展。", "b）软件定义存储。 传统存储架构难以满足互联网业务高并发、 高扩展性的需求，软件定义存储很好地解决了互联网厂商的痛点，未来随着数据量增加、互联网业务并行需求增加，软件定义存储架构占比将持续提升，有望成为存储产业的主流架构。", "。 随着数据增加、业务处理量提升，读写频率及读写速度需求均陡然增加，传统磁盘阵列虽能通过并行架构提升读写速率，但仍难以满移动设备、社交网络、大数据分析等高频高速数据响应需求。而全闪存阵列非常好的契合了 IO 密集型场景，且随着价格逐年走低，未来全闪存阵列占比有望持续提升。", "国内厂商充分受益于国产替代， 迎巨大历史发展机遇我们认为， 在数据大爆炸的时代，数据量的指数级增长必将引爆对存储的需求，且在国产化和去“ IOE”的大背景下，国产厂商将获得历史性发展机遇。云计算的发展、虚拟化和软件定义存储等新技术的加速发展使国内厂商有望弯道超车，大举占领国内市场， 进而实现国际化。", "存储系统是 IT 系统中的核心。首先， 存储系统的稳定性将影响到整个业务系统的正常运营，存储系统如出现数据丢失、宕机将对业务系统连续性产生致命影响；其次，存储系统的性能将影响到整个业务系统的效率， 存储系统的读取速度将决定数据存储与提取效率，直接影响业务系统的效率；再次， 存储系统的扩展能力将决定整个业务系统的扩展性， 随着大数据时代到来，数据量爆发也对存储系统的可扩展性提出了更高要求， 扩展能力将成为存储设备能够支撑数据生产型业务的必要指标之一。", "从分类上看，外挂存储按控制层可分为 DAS、 NAS、 SAN 为三种常见的方式。", "DAS（直接连接存储）：存储设备是通过电缆（通常是 SCSI 接口电缆）直接连到服务器的， I/O 请求直接发送到存储设备。", "DAS 主要应用于网络规模较小、数据存储量小、组网简单的小型网络中。 DAS架构连接简单，集成在服务器内部， 为点到点的连接，具有安装技术要求低， 成本较低等特点。", "DAS 的劣势较为明显。首先， DAS 架构的可扩展性较差， SCSI 总线支持的距离最大为 25 米， 支持的设备数量最多为 15 个，直接制约了其可扩展性；其次， 直连存储无法共享，因此经常出现的情况是某台服务器的存储空间不足，而其他一些服务器却有大量的存储空间处于闲臵状态却无法利用；再次， DAS 结构下的数据保护流程复杂，如果做网络备份，那么每台服务器都必须单独进行备份，而且所有的数据流都要通过网络传输。如果不做网络备份，那么就要为每台服务器都配一套备份软件和磁带设备，备份流程的复杂度和备份成本会大大增加。", "NAS（网络连接存储）： NAS 方式则全面改进了以前低效的 DAS 存储方式。它采用独立于服务器，使用网络连接存储专用文件服务器来连接存储设备，自形成一个网络。这样数据存储就不再是服务器的附属，而是作为独立网络节点而存在于网络之中，可由所有的网络用户共享。", "NAS 是独立的存储节点存在于网络之中，与用户的操作系统平台无关，可以做到真正的即插即用， 同时 NAS 还具有资源易于共享、 部署简单且扩展性较好等优势。 但 NAS 也存在存储性能较低，可靠性差等劣势。", "1991 年， IBM 公司在 S/390 服务器中推出了 ESCON 技术。它是基于光纤介质，最大传输速率达 17MB/s 的服务器访问存储器的一种连接方式。在此基础上，进一步推出了功能更强的 ESCON Director(FC SWitch)，构建了一套最原始的 SAN系统。", "SAN 是一个用在服务器和存储资源之间的、专用的、高性能的网络体系。它为实现大量原始数据的传输而进行了专门的优化。", "用光纤通道构建的 SAN 由以下三个部分组成：", "1）存储和备份设备：包括磁带、磁盘和光盘库等；", "2）光纤通道网络连接部件：包括主机总线适配卡、驱动程序、光缆、集线器、交换机、光纤通道和 SCSI 间的桥接器；", "3）应用和管理软件：包括备份软件、存储资源管理软件和存储设备管理软件。", "1）网络部署容易；", "2）易于存储和备份；", "3）高性能：因为 SAN采用了光纤通道技术，所以它具有更高的存储带宽，存储性能明显提高；", "4） 非常好的扩展性：由于 SAN 采用了网络结构，扩展能力更强。光纤接口提供了 10公里的连接距离，这使得实现物理上分离、 不在本地机房的存储变得非常容易。", "传统的网络存储系统采用集中的存储服务器存放所有数据，存储服务器成为系统性能的瓶颈，也是可靠性和安全性的焦点，不能满足大规模存储应用的需要。", "随着互联网的兴起， 数据创造的主体由企业逐渐转向个人用户，而个人所产生的绝大部分数据均为图片、文档、视频等非结构化数据，企业办公流程更多通过网络实现，表单、票据等都实现了以非结构化为主的数字化存档。结构化与非结构化数据量加速爆发， 云计算、 大数据时代的到来对存储空间的需求呈指数级上升，而传统集中式存储在成本、可扩展性等方面存在劣势，以互联网厂商为主导的分布式存储逐步兴起，广泛应用于泛互联网化应用领域。", "分布式存储系统，是将数据分散存储在多台独立的设备上。分布式网络存储系统采用可扩展的系统结构，利用多台存储服务器分担存储负荷，利用位臵服务器定位存储信息，它不但提高了系统的可靠性、可用性和存取效率，还易于扩展。", "据 IDC 全球企业存储系统季度追踪报告， 2016 年第二季度全球企业存储系统销售收入同比持平，为 88 亿美元。该季度总体出货容量同比增长 12.9%达到34.7EB。直接向超大规模数据中心进行售卖的原始设计制造商的收入增幅有所下滑，这部分市场同比减少 21.5%至 7.947 亿美元。该季度基于服务器的存储销售额增长 9.8%，占有近 24 亿美元的收入。外部存储系统仍然是最大的细分市场，市场规模同比持平，为 57 亿美元。", "该季度 EMC 和 HPE 在整个全球企业存储系统市场位列前两位，所占份额分别为18.1%和 17.6%。据 IDC 分析报告， HPE 的同比增幅受到 HPE 从 2016 年 5 月开始在中国与 H3C 合作的影响， HPE 设计的部分存储系统贴牌后在中国市场销售，未来这部分并不计入 HPE 的市场数据中心。戴尔以 11.5%的市场份额位列第三， IBM 和 NetApp 的全球份额分别为 6.8%和 6.7%。作为单一的群组，由原始设计制造商直接售卖给超大规模数据中心客户的存储系统在该季度占全球开支的 9%。", "我们认为，全球整体存储需求趋于稳定，传统存储占比会逐渐减小，以闪存、分布式存储为代表的新型存储正在快速被市场接受。随着传统存储厂商纷纷布局存储研发创新，这一趋势将会延续。", "据 IDC 公布的中国外部磁盘存储市场调查数据， 2016 年第二季度，中国存储市场增长表现稳健，整体销售额超 35.3 亿元，同比增长 7%，总体存储容量 1.84EB。业务数据量的增长是存储市场增长的主要驱动力，存储设备需要在性能、容量、高级功能上持续提升，以匹配复杂业务。", "国产厂商的存储销售高速增长，华为以 7.4亿销售额名列榜首，是位列第二的 EMC销售额的 2.18 倍。海康威视以 43.3%的高增长率领涨，而国外老牌存储巨头 EMC和 IBM 销售额下滑明显。我们认为，得益于国家信息化安全需要和国产化趋势的延续，外企市场占比的逐步减少给国内存储厂商的大发展提供了良好的机遇。", "SAN 存储在金融、能源、电信、财税等重点大型行业广泛应用，已经在关键业务的数据管理方面发挥了巨大作用，占整体存储市场的 57.3%，仍然是企业级存储的主流市场和增长主力。", "451 Research 基于全球 700 位以上 IT 专业人士，并结合了 451 对 25000 位资深IT 买家和企业技术高管的响应作出分析，结果表明，随着内部部署存储开支下降17%，公有云存储开支将在两年内翻 1 倍。", "根据 Gartner 的数据显示，全球公有云服务市场规模将从 2015 年的 1750 亿美元增长到 2016 年的 2040 亿美元，增长率为 16.5%。其中增长最快的子市场是基础设施即服务（IaaS），将在 2016 年增长 38.4%，市场规模将达到 224 亿美元。IaaS 层主要包括服务器、存储和网络，按照一半的占比进行估计，公有云服务中存储的市场规模在 2016 年预计为 112 亿美元。", "“十三五”规划明确提出国家“互联网+”战略，从信息化基础设施建设来看，“十三五”信息基础设施建设模式和使用模式都发生了根本性转变。经过“十二五”期间的发展，以云计算、云存储、云共享和云安全为代表的云服务模式日趋成熟，随着公有云服务被政府和企业广泛采纳，基于云服务的 IT 基础设施建设模式也越来越受到广泛关注和认可。随着云服务的进一步深化，未来人们可以有望像购水一样方便快捷地购买云服务。因此，可以预见，“十三五”期间政府和企业会减少单独构建合同和平台的投资，转而购买和使用云服务进行信息化建设。", "我们预计，随着信息化基础设施建设的推进，公有云将持续高速发展，而作为公有云的核心组件之一的存储设备，未来市场空间将进一步打开。", "物联网和可穿戴设备正在加速普及，标准的明确、 新技术的应用也将促进物联网产业发展。 我们认为，物联网的存储方式可以大致分为三种，首先是本地存储，物联网本地会有一些记忆体，但物联网个体节点的最主要功能是资料回传和交互，因此本地存储能力会很有限；其次是私有云存储，也就是企业或者其他组织通过建设私有云基础设施，将物联网节点上的数据回传汇总到私有云上，以用于查询和调用；最后是公有云存储，把物联网节点上的所有数据汇集到公有云上，一方面便于管理，另一方面海量数据可以实现大数据的分析、预测，实现数据价值。", "物联网设备所产生的庞大数据将带来巨大收益：据 IDC 预测，到 2020 年，将有超过两千一百亿设备连接至物联网，到 2025 年，将带来约 62 千亿美元的收入。我们认为，物联网带来的海量的数据将会直接带来存储的海量需求，为存储未来的市场空间提供了保障。", "据易观国际统计，2015 年我国大数据市场规模达 102 亿元，2017 年有望达到 170亿元。这看似百亿级别的市场，背后却能撬动数万亿元的相关市场规模。进入 DT时代后， 互联网巨头通过把握入口、 掌握流量等方式获取并整合数据， 数据已成为互联网厂商角力的又一战场。", "随着大数据的发展， 大数据所催生的存储需求也在陡然提升， 据统计， 2011 年全球大数据存储市场规模仅为 11 亿美元，而 2016 年全球大数据存储市场规模将达到 64 亿美元，年均复合增长率为 34.1%。", "随着 DT 时代到来， 数据量将呈现爆发式增长， 而数据结构中诸如视频、图片、声音等非结构化数据占比也将显著提升。 根据 IDC 预测数据， 2011-2020 年间，全球数据量将增长 50 倍， 非结构化数据将占数据增量的 90%以上。", "我们认为，在互联网化趋势下，各个行业数据量呈爆炸式增长。另外随着数据大集中、数据挖掘、商业智能、协同作业等大数据处理技术的日趋成熟，数据价值呈指数上升趋势。存储的数据逐渐变成公司越来越重要的无形资产，必然会导致存储行业快速升温。", "我们认为，大数据近几年的发展并没有预期的那么快，主要原因是数据挖掘、数据提取等大数据处理技术没有及时跟上，目前人们普遍都认识到数据的重要性，但是能从大数据中发掘价值的公司相对较少。我们预计未来大数据引出的存储增量主要有三点，一是数据规模扩大引起的扩容需求，二是大数据分析和处理产生的倍增数据存储需求，三是数据价值被发掘后，各大公司会对数据的安全保障投入更大的精力，进而产生更多的容灾需求和备份需求。", " ", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72457"]},
{"module": ["干货教程"], "note": ["\n文 | 王洪磊， 朱晟君\n在上文中，我们概述了传统存储的三种架构以及分布式存储，并对存储市场为什么会迎来快速增长的原因。十三五的信息化战略导航，物联网的快速到来以及大数据时代数据量..."], "title": ["为何说国内企业级存储市场将迎来快速增长？| 下"], "content": ["文 | 王洪磊， 朱晟君", "在上文中，我们概述了传统存储的三种架构以及分布式存储，并对存储市场为什么会迎来快速增长的原因。十三五的信息化战略导航，物联网的快速到来以及大数据时代数据量的疯狂增加。接下来我们将讲述存储产业发展趋势。", "IDC 数据显示，在经过今年的缓慢开端之后，第二季度企业存储市场仍然保持稳定。全闪存阵列销售的持续增长成为市场发展的助推因素之一。闪存介质成本的持续下降、使用实例的增加以及高密度部署需求的增加，推动着市场的快速发展。该季度，全闪存市场总收入近 11 亿美元，同比增长 94.5%。混合闪存阵列细分市场仍然是该市场的主要组成部分，收入 23 亿美元，占有 26.1%的市场份额。", "对于 IT 存储界来说，闪存技术的应用可以算作一场“颠覆”。 2010 年闪存新星Fusion-io 宣布营收额增长 300%，揭开了闪存快速发展的序幕，随后各种工艺的SSD 如雨后春笋般出现，诸多闪存公司竞相并起， 闪存市场发展迅猛。", "我们认为，闪存市场的快速发展，是应用需求驱动的结果。移动设备、云服务、社交网络和大数据分析等都对存储的实时响应能力提出了较高的要求。要满足这些实时应用的需求，后台就需要有高效的计算、网络、存储能力。处理器和内存的性能近些年来基本上符合摩尔定律的预测，增长速度非常快。但是存储的读写", "性能相对于多核处理器、不断增长的网络速度来说，已然成为了系统的短板。无论邮件系统、 ERP 系统、数据库以及实时分析系统，都需要硬盘在最短的时间内将最准确的数据传递给 CPU，用户对这些系统的 I/O 响应要求非常高。传统硬盘读写性能低，依靠传统磁盘阵列的并行读取技术，虽然可以提高硬盘读写性能，但也是杯水车薪，硬盘一直是传统阵列响应速度的瓶颈。高速 SSD 的出现极大地提升了硬盘的 IOPS 和吞吐能力，一定程度上解决了数据存取效率低的问题。", "通常而言， 5U 的全闪存阵列可提供高达 20 万混合读写 IOPS，时延只有 500us，在线重删、压缩技术基本可达成 3～5 倍的数据缩减率。相比传统机械硬盘阵列，提供同样的 IOPS、容量情况下， 5U 的全闪存阵列就可以代替 2 个机柜的传统机械硬盘阵列，同时降低能耗、制冷、空间费用以及 TCO （Total Cost of Ownership）。", "我们认为，随着半导体工业的发展， Flash 集成度愈来愈高， SSD 成本将不断下降。在多通道并行等技术条件下， SSD 盘有着读写速度快、绿色省电和无机械部件可靠性高等优势，因此闪存阵列在企业级市场的高速发展势头必将得以延续。", "另外，我们预计，未来随着闪存价格的下降，或许会出现闪存大规模替换传统磁盘阵列的情况，但是就目前来看，闪存价格依然是普通磁盘阵列价格的 8 倍以上，因此两者会在各自的应用领域发挥自己的优势。", "在引入 SAN 技术之前，存储采购的目标主要是阵列的嵌入式特性/功能和物理硬件属性，如可扩展性和可靠性。一旦 SAN 被广泛地采用，用户面临的就是各种各样的型号和品牌，因此他们只能根据具体的项目需求建设一套又一套的存储设备，这在很大程度上使不同的存储设备在用户的系统中是一个个孤岛。这就造成了不同业务系统的存储资源无法均衡并且维护成本居高不下。 此外，对于用户来说，由于数据迁移的风险和费用很高，一旦购买了一个厂商的存储，后续就只能购买该厂商的服务和扩容，这样就会被一个硬件供应商绑定。用户急需能够对现有存储进行统一整合并且能兼容其他厂商设备的存储管理软件，存储虚拟化应运而生。", "存储虚拟化在一些厂商的概念设臵中， 被当作了软件定义存储的一种。它是指把传统存储设备进行统一管理、池化，然后映射给上层主机使用。这种技术有两个优点，一是很好地解决了传统存储“烟囱式”扩容的弊病，二是解决了被单一存储厂商绑定的问题。", "我们预计，未来三年，数据量的增加将带动虚拟化存储的应用需求，数据中心、异地数据管理等都将应用存储虚拟化技术，因此，存储虚拟化网关的市场规模将呈现高增长。根据 CCID 预测显示， 2017 年市场规模将达到 4.09 亿，年复合增长率超过 25%。", "我们认为，因虚拟化网关本身的价格限制，它的市场规模表明看起来并不太大，但是它对于传统存储厂商的市场格局影响会非常大。首先它降低了存储系统新进厂商的准入门槛和进入可能；其次它将传统 SAN 存储厂商推入一个公平竞争的环境，规避了封闭系统的高门槛，有利于行业的推陈出新。需要特别提到的是，传统 SAN 阵列中低端市场目前属于“红海”，竞争惨烈，利润率逐年走低。", "而作为 SAN 阵列的制高点—高端存储，目前高端存储的玩家较少，基本是国外的存储巨头和国内的华为、浪潮公司，高技术门槛带来了高利润，这也是目前 SAN 存储厂商发力的一个方向。除了能为企业带来较高的利润，高端存储产品历来都是存储厂商创新能力与研发实力的体现。高端存储取得成功，将带动中低端存储的研发实力和品牌提升，对企业做大做强存储业务有巨大帮助。", "目前，云存储、超融合以及基于闪存的存储解决方案等新技术的兴起，对传统存储而言，是一个巨大的挑战。根据 IDC 的全球企业存储系统季度追踪报告称，2016年第一季度企业存储系统工厂收入同比下滑 7%至 82 亿美元。该季度出货容量下滑 4%至 27.8EB。直接出售给超大规模数据中心的原始设计制造商这个类别的收入增幅有所下滑，这部分市场同比减少 39.9%至 6.81 亿美元。", "2016 年第二季度全球企业存储系统工厂收入同比持平，增幅 0.0%为 88 亿美元。该季度总体出货容量同比增长 12.9%达到 34.7EB。直接向超大规模数据中心进行售卖的原始设计制造商的收入增幅有所下滑，这部分市场同比减少 21.5%至 7.947 亿美元。目前存储市场总体规模虽然基本稳定，但是闪存和服务器存储的持续增长是建立在传统存储的份额下滑之上的。", "传统存储最大的弱势在于只能凭借在系统中增加大量磁盘驱动器来实现较高的性能，而这种方式必然带来昂贵的开支和营运费用。存储厂商通过在混合结构中增加闪存，尽力克服磁盘阵列局限性，但是旧有基础架构并不能真正发挥闪存的效率，发挥其最高性能。而且，混合阵列也不能轻易地跨多个系统进行共享，这样就导致效率低下。", "我们认为，存储虚拟化技术的发展为传统存储带来了新的机遇。存储虚拟化的目标是把应用程序存储与物理的数据存储基础设施分离。理论上，这将实现存储资源的“灵活”分配、重新分配或不分配。换言之，存储虚拟化提供一种把存储服务从存储包中分离出来的方法，即使基本的硬件和互连被变更，仍然能提供卷的持续性。软件定义存储允许客户将存储服务集成到服务器的软件层，将软件从原有的存储控制器中抽离出来，使得它们的功能得以进一步的发挥而不仅仅局限在单一的设备中。", "目前来看，存储虚拟化的一种主流方案是在现有的存储层之上增加一个虚拟化层，在虚拟化层连接各存储系统以创建一个整体得资源池。在资源池内部可以实现快照、容灾、数据重删、持续数据保护等多种增值功能。据我们分析，这种方式可以充分利用用户已有的存储设备，实现多厂家设备的异构、数据的迁移、容灾和统一管理，充分实现客户存储硬件的投资价值。将虚拟化与缓存结合起来，这正是获得高 IOPS 性价比的重要途径。虚拟化技术可用于跨多个系统灵活分配存储空间，通过使用缓存系统来发挥闪存的优势。", "我们认为，虚拟化技术的引入给传统存储市场注入了新的活力，主要有以下几点：", "1、可以有效提升传统存储的 IOPS 和扩展能力，这样已购买传统存储的客户可以在其基础上扩容，以满足自己后续的存储需求。", "2、可以利用传统存储进行做容灾，增强数据的安全性，这就带来了新增市场空间。我们预计，随着虚拟化技术能力的提升，运算能力、扩展能力、安全性能得以提升，再加上运维成本的下降，传统存储的成本优势就能得以体现，有望打破近年来市场规模持续下滑的窘境，焕发新的活力。", "在传统存储设备中，存储厂商会自己研制存储硬件，存储厂商的存储硬件主要为了解决三个问题：", "一是提高磁盘的 IO 性能，存储厂商需要自己研制面向 IO 的存储硬件，比如说在一台存储服务器中需要扩展很多的 SATA/SAS 接口、扩展硬件RAID 功能，如果是高性能的存储设备，还需要扩展控制器之间的专用互连网络接口；", "二是应对存储的 IO 密集型读写，低端存储甚至可以采用计算性能比较差的 CPU 进行设计；", "三是提高存储的高可靠性，传统存储往往追求硬件设计的可靠与稳定，通过提高硬件的可靠性来达到存储可靠的目的。在这种传统存储的思路下，存储的设计主要分成存储硬件和存储软件设计两大部分。", "专用存储硬件是各个厂商独立开发的，具有封闭性，因此造成客户一旦购买了某一厂商存储，后续扩容基本上就被该厂商“绑架”，而相应的运维和服务也没有选择权。此外，专属硬件的维护和更换也大量增加了运维成本和故障恢复时间。软件定义存储的理念就是将硬件归一化，而把之前通过专用存储硬件实现的提升IO 性能和可靠性等功能都由软件来实现。", "目前存储各厂商对于软件定义存储的概念并不统一。我们认为，软件定义存储的思想就是要将定制硬件的过程从存储设计中彻底抛弃，存储就是在通用服务器上的一套软件。软件定义存储在本质上来说，会给现有的存储市场带来革命性的改变，这也是市场现有的存储巨头不主动推广该技术的原因。但是，发展趋势不是既得利益者能够改变的，虽然存储厂商没有主动推广，可 EMC、华为、 IBM 等存储巨头以及 Vmware 等软件厂商都看清了这一趋势，并且在积极布局。", "软件定义存储的概念提出较早，但近年来才开始逐步落地， 主要有三方面原因：", "1）通用服务器性能的提升。今天一个普通的服务器就可以安装 1TB 左右的内存；2 路 CPU 基本算是通用服务器的最低配臵；", "2）网络性能的大幅提升。服务器的基础配臵一般都会板载两个 10Gbps 以太网或者 8G/16Gbps FC 卡；", "3）在 IO 处理方面，闪存无论在性能还是容量方面都取得了长足的进步。以前如果我们想要达到 1 万 IOPS 的性能，只能通过并发磁盘的方式，为了能使更多的磁盘并发，需要大量的 SAS/SATA 接口，因此，只能定制主板来满足这种设计。而如今，一块 SSD 盘就可以达成这个目标。", "我们预测，软件定义存储是未来存储行业发展的主要趋势，在该趋势中，存储硬件将归一化，大大降低整体存储解决方案的成本；软件的价值将被凸显，未来在软件方面领先并且具有持续研发能力的厂商将获取产业链的大部分利润。另外，软件定义存储将打破现有存储格局，降低新进厂商的准入门槛，为更多的创业型公司带来机遇，也给整个行业的发展带来新的可能。", "国际标准 SHARE 78 对容灾系统的定义有七个层次：从最简单的仅在本地进行磁带备份，到将备份的磁带存储在异地，再到建立应用系统实时切换的异地备份系统，恢复时间也可以从几天到小时级到分钟级、秒级或零数据丢失等。目前针对这七个层次，都有相应的容灾方案。", "从 SHARE 78 可以看出，灾备系统随着业务的重要程度和对数据丢失数量的容忍度，投资规模差异巨大。我们认为，随着数据资产受到越来越多的重视，政府和企业在容灾上的投资额会逐步加大，灾备系统的大建设时期不会太远。备份系统内部的组件主要有两部分：备份软件和备份介质。就备份软件来看， Gartner 于 6 月 15 日发布了 2015 年企业备份软件和集成设备相关的魔力象限图，与 2014 的备份软件魔力象限图相比变化不大。", "我们认为，当前备份软件和集成设备解决方案基本被外商所垄断，主要原因是备份软件作为整个灾备系统的中枢，其架构和开发难度较大，技术门槛很高。国内目前的灾备解决方案基本上是集成主流厂商的备份软件、加载第三方或者自产存储设备所组成，而整个解决方案的主要利润毫无疑问会被备份软件厂商获取。", "备份介质目前主要有磁带、磁盘和蓝光光盘。我们认为，由于磁带失效率高、数据易丢失、恢复无法保证、维护成本高等种种劣势，出现在备份解决方案中的机会非常小。磁盘阵列是目前在备份系统中应用最为广泛的备份介质，其中以虚拟带库（VTL）最受欢迎。 VTL 使用的是磁盘阵列，对外提供的是磁带库接口，这样就最大程度的得以兼容上层的备份软件，具备磁带库和磁盘阵列两者的优势。", "蓝光存储其实也是 VTL 的一种，只是把 VTL 中的硬盘替换为蓝光光盘，蓝光存储目前还未大规模推向市场。但是由于它具有寿命长（可靠保存时间超过 50 年）、容量大（单张光盘存储容量为 100G、每个光盘盒可装 12 张光盘，共 1.2T 存储容量，每一台光盘库可配备 90 个光盘匣，总存储容量为 108T）、绿色存储（储存条件简单、功耗低、碟片保管时不需要空调）、安全系数高（具有防篡改，防病毒功能，电源切断、地震、洪水时碟片不会损坏）等优势，相信未来必然能在灾备市场上大有作为。", "目前国内灾备市场还未打开，大多数厂商对容灾的认识不足。甚至有些依然采用“手动+移动硬盘”的方式进行数据备份，而很多企业对于数据安全的认识停留在阻击黑客层面。灾备是指通过预先建立备份中心、备份设备，并对数据进行备份，一旦业务系统故障，在企业可以容忍的时间内恢复业务系统的正常运行，将因业务中断导致的损失降低到最小的程度。国外一家权威机构曾做过研究， 1TB工程数据的丢失可能造成高达 50 亿美元的损失。随着系统越来越复杂，数据容灾备份是数据安全的最后保障。", "政府部门以及医疗、金融等行业对灾备的重视程度越来越高，有些还明确地提出了灾备要求，比如保监会要求所有保险机构的系统宕机时间不能超过 8 小时。未来，更多的用户会关注 RTO（恢复时间目标）、 RPO（恢复时间点目标）等与灾备相关的指标，这对于中国灾备市场的发展来说是一个利好因素。", "据 IDC 的估算， 2017 年，灾备市场将达到上千亿元的规模。当前在中国的灾备市场上，国外厂商仍占据主导地位。我们认为，由于灾备涉及到数据安全问题，因此在国内对数据安全性敏感的行业，国产化是未来的趋势。而且随着国内厂商的技术和产品不断走向成熟，以及对国产数据库有更好的支持，中国灾备软件厂商在市场上也拥有了更多的话语权。", "随着互联网与移动互联网的兴起， 海量终端设备带来数据量的大幅增加，对于存储的需求也随之增长。 传统存储的集中式架构在性能和扩展性上存在着天然的缺陷， 难以适应下游客户海量存储的需求场景。在互联网化浪潮推动下， 软件架构的升级与硬件介质更新推动存储行业迈入新的阶段：", "a）虚拟化。随着数据量指数级增长， 互联网化思维要求数据中心能够很好地解决传统存储架构存在的数据孤岛问题，虚拟化应运而生，进而推动传统存储产业发展。", "b）软件定义存储。传统存储架构难以满足互联网业务高并发、 高扩展性的需求，软件定义存储很好地解决了互联网厂商的痛点，未来随着数据量增加、互联网业务并行需求增加，软件定义存储架构占比将持续提升，有望成为存储产业的主流架构。", " 随着数据增加、业务处理量提升，读写频率及读写速度需求均陡然增加，传统磁盘阵列虽能通过并行架构提升读写速率，但仍难以满足移动设备、社交网络、大数据分析等高频高速数据响应需求。而全闪存阵列非常好的契合了 IO 密集型场景，且随着价格逐年走低， 未来全闪存阵列占比有望持续提升。", "我们认为， 在数据大爆炸的时代，数据量的指数级增长必将引爆对存储的需求，且在国产化和去“ IOE”的大背景下，国产厂商将获得历史性发展机遇。 云计算的发展、虚拟化和软件定义存储等新技术的加速发展使国内厂商有望弯道超车，大举占领国内市场，并实现国际化。", "End.", "转载请注明来自36大数据（36dsj.com)：", " » "], "more": ["http://www.36dsj.com/archives/72704"]}
]